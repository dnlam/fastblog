<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Markov Decision Process and Dynamic Programming | Lam Dinh</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Markov Decision Process and Dynamic Programming" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Second in a series on understanding Reinforcement Learning." />
<meta property="og:description" content="Second in a series on understanding Reinforcement Learning." />
<link rel="canonical" href="https://dnlam.github.io/fastblog/2020/06/14/MDP_DP.html" />
<meta property="og:url" content="https://dnlam.github.io/fastblog/2020/06/14/MDP_DP.html" />
<meta property="og:site_name" content="Lam Dinh" />
<meta property="og:image" content="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQChSoDYsSnyFzrYnjJXEA6d5-kRELW-lzYcA&usqp=CAU" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-14T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQChSoDYsSnyFzrYnjJXEA6d5-kRELW-lzYcA&usqp=CAU" />
<meta property="twitter:title" content="Markov Decision Process and Dynamic Programming" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2020-06-14T00:00:00-05:00","datePublished":"2020-06-14T00:00:00-05:00","description":"Second in a series on understanding Reinforcement Learning.","headline":"Markov Decision Process and Dynamic Programming","image":"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQChSoDYsSnyFzrYnjJXEA6d5-kRELW-lzYcA&usqp=CAU","mainEntityOfPage":{"@type":"WebPage","@id":"https://dnlam.github.io/fastblog/2020/06/14/MDP_DP.html"},"url":"https://dnlam.github.io/fastblog/2020/06/14/MDP_DP.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/fastblog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://dnlam.github.io/fastblog/feed.xml" title="Lam Dinh" /><link rel="shortcut icon" type="image/x-icon" href="/fastblog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/fastblog/">Lam Dinh</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/fastblog/about/">About Me</a><a class="page-link" href="/fastblog/search/">Search</a><a class="page-link" href="/fastblog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Markov Decision Process and Dynamic Programming</h1><p class="page-description">Second in a series on understanding Reinforcement Learning.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-06-14T00:00:00-05:00" itemprop="datePublished">
        Jun 14, 2020
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      10 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/dnlam/fastblog/tree/master/_notebooks/2020-06-14-MDP_DP.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/fastblog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/dnlam/fastblog/master?filepath=_notebooks%2F2020-06-14-MDP_DP.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/fastblog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/dnlam/fastblog/blob/master/_notebooks/2020-06-14-MDP_DP.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/fastblog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fdnlam%2Ffastblog%2Fblob%2Fmaster%2F_notebooks%2F2020-06-14-MDP_DP.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/fastblog/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#Preliminaries">Preliminaries </a></li>
<li class="toc-entry toc-h1"><a href="#Markov-Decision-Processes-(MDP)">Markov Decision Processes (MDP) </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Polices-and-Value-functions">Polices and Value functions </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Optimal-Value-Function">Optimal Value Function </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Dynamic-Programming">Dynamic Programming </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Policy-evaluation-(Prediction)">Policy evaluation (Prediction) </a></li>
<li class="toc-entry toc-h2"><a href="#Policy-improvement">Policy improvement </a></li>
<li class="toc-entry toc-h2"><a href="#Value-Iteration">Value Iteration </a></li>
<li class="toc-entry toc-h2"><a href="#Bootstraping-in-Dynamic-Programming">Bootstraping in Dynamic Programming </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-06-14-MDP_DP.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Preliminaries">
<a class="anchor" href="#Preliminaries" aria-hidden="true"><span class="octicon octicon-link"></span></a>Preliminaries<a class="anchor-link" href="#Preliminaries"> </a>
</h1>
<ul>
<li>Markov property: a process is Markov if the future is independent on the past given the present.</li>
<li>State transition
$$
p(s'|s,a)= \sum _r p(s',r|s,a) 
$$</li>
<li>Expected reward
$$
r(s,a) = \mathbb{E} [R|s,a] = \sum _r r \sum _{s'} p(r,s'|s,a) 
$$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Markov-Decision-Processes-(MDP)">
<a class="anchor" href="#Markov-Decision-Processes-(MDP)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Markov Decision Processes (MDP)<a class="anchor-link" href="#Markov-Decision-Processes-(MDP)"> </a>
</h1>
<p>MDPs are a classical formulations of sequential decision making, where each action influences not only immediate rewards, but also subsequent situations, or states and through those future rewards. Thus MDPs involve delayed reward and the need to trade-off immediate reward and delay reward.</p>
<p>In bandit problems, we estimated the value $q_*(a)$ of each action a. In MDP, we estimate the value $q_*(s,a)$ of each action a in each state s, or we estimate the value $v_*(s)$ of each state given optimal action selections. These states dependent quantities are essential to accurately assign credit for long term consequences to individual action selections.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>MDPs are meant to be a straight forward framing of problem of learningfrom interaction to achieve a goal. The learner and decision maker is called the <i>agent. </i> The thing it interacts with, comprising everything outside the agent, is called <i>environment. </i> These interact continually, the agent selecting actions and the environment responding to these actions and presenting new situations to the agent. The environment also gives rise to rewards through choice of actions.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://miro.medium.com/max/1838/1*ywOrdJAHgSL5RP-AuxsfJQ.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A state s has the Markov property when for states $\forall s' \in \mathcal{S} $ and all reward $r \in \mathbb{R}$
$$
p(R_{t+1}=r, S_{t+1}=s' | S_t=s) = p(R_{t+1}=r, S_{t+1}=s' | S_1,...,S_{t-1},S_t=s)
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then, the distribution of the reward and the next state when we condition only the current state is the same as we would conditional all previous states. So, if we know all the previous states, this give us no additional information about what the next state and reward would be. It means that we just need to look at the current state.</p>
<p>$p(r,s'|s,a)$ is the joint probability of a reward r and next state s' if we are at the state s and action a has been taken.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In a finite MDP, the sets of state, actions and rewards ($\mathcal{S}, \mathcal{A},\mathcal{R} $) all have a finite number of element. The funtion <i>p </i> gives the dynamics of MDPs and $\gamma \in [0,1]$ is a discount factor that trades off later rewards. 
Then, a MDP is a tuple of $(\mathcal{S}, \mathcal{A},\mathcal{R}, p, \gamma)$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Acting in a MDP results in returns $G_t$: total discounted reward from time-step t.
$$
G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum _{k=0} ^\infty \gamma^k R_{t+k+1}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In principle, $G_t$ is a random variables that depends on MDP and policy.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Polices-and-Value-functions">
<a class="anchor" href="#Polices-and-Value-functions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Polices and Value functions<a class="anchor-link" href="#Polices-and-Value-functions"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Almost all reinforcement learning algorithms ilvolve estimating value functions - functions fo states (or of state-action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). The rewards that the agent can expect to receive in the future depend on what action it will takes. Accordingly, value function are defined with respect to particular ways of acting, called <i>policy. </i></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Formally, policy maps states to probabilities of selecting each possible action. If the agent follows  policy $\pi$ at time t, then $\pi(a|s)$ is the probability that $A_t=a$ if $S_t=s.$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Recalling that the value function of a state s under policy $\pi$, $v_\pi(s)$ is the expected return when starting in state s and following $\pi$ thereafter. 
$$
v_\pi(s)= \mathbb{E}_\pi [G_t | S_t=s] = \mathbb{E}_\pi [\sum_k \gamma ^k R_{t+k+1} | S_t=s] \\\\
=\mathbb{E}_\pi [ R_{t+1} + \gamma G_{t+1} | S_t=s, \pi] \\\\
= \mathbb{E}_\pi [ R_{t+1} + \gamma v_\pi (S_{t+1}) | S_t=s, A_t \sim \pi(S_t)] \\\\
= \sum _a \pi(a|s) \sum _r \sum _{s'} p(r,s'|s,a)(r+\gamma v_\pi(s'))
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This recursive equation is the <i>Bellman equation </i> for $v_\pi$. It expresses a relatioship between the value of a state and the values of its successor states. It also staes that the value of the start state must equal the discounted value of the expected next state, plus the reward expected along the way.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The same principle is applied for state-action value.
$$
q_\pi(s,a) = \mathbb{E} [G_t | S_t=s, A_t=a,\pi]
$$</p>
<p>It implies that:
$$
q_\pi(s,a) = \mathbb{E}_\pi [ R_{t+1} + \gamma G_{t+1} | S_t=s, A_t=a] \\\\
= \mathbb{E}_\pi [ R_{t+1} + \gamma q_\pi (S_{t+1},A_{t+1}) | S_t=s, A_t=a] \\\\
= \sum _r \sum _{s'} p(r,s'|s,a)(r+\gamma \sum_{a'}\pi(a'|s')q_\pi(s',a'))
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then, the relationship between action value and state-action value can be represented:</p>
$$
v_\pi(s) = \sum _a \pi(a|s) q_\pi (s,a) = \mathbb{E} [q_\pi (s,a) | S_t=s, \pi]
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The value of a state equal to the weighted sum of state-action values.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>IF we represent the Bellman Equation in the matrix form, we can see that it is a linear equation form that can be solved directly.
$$
v= r+ \gamma P^\pi v \\\\
= (I-\gamma P^ \pi)^{-1}r
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So, if we know the transition matrix, discount factor and reward vector, we can just compute the value function as the solution of the Bellman's Equation. However, it is burdensome to make it in reality if we care about a hugh system with millions of states.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By using other iterative method such as Dynamic Programming, Monte Carlo Evaluation and Temporal Difference Learning, we can have another method to solve Bellman Equation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Optimal-Value-Function">
<a class="anchor" href="#Optimal-Value-Function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Optimal Value Function<a class="anchor-link" href="#Optimal-Value-Function"> </a>
</h3>
<p>Solving a reinforcement learning task means finding a policy that achieves a lot of rewards over the long run or optimal policy.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The optimal state value $v_*(s)$ is the maximum value function over all policies
$$
v_*(s) = \max _ \pi v^\pi(s)
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The optimal action value function $q_*(s,a)$ is the maximum action-value function over all policies.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
q_*(s,a) = \max _\pi q^\pi(s,a)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>These optimal values give us what the best possible performance that we can get in a problem. MDP can be considered as solved when we can find these values.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Estimating the value of certain policy either using state value of action value is called Policy evaluation or prediction because  we are are making a prediction what about happens when we follow certain policy.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Estimating their optimal values is called control because it can be used for policy optimization.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Once we can get $v_*$, it is relatively easy to determine optimal policy. For each state s, there will be one or more actions at which the maximum is obtained in the Bellman optimality equation. Any policy that assign non probability only to these actions is an optimal policy. It is related to one step search because the action appears best after a one-step search is the optimal actions. 
The beauty of $v_*$ is that if one uses it to evaluate the short-term consequences of actions, then a greedy policy is actually optimal in the long term sense because $v_*$ already takes into iaccount the rewards of all possible future behaviours.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Having the $q_*$ make choosing the optimal actions easier because the agent does not have to do one-step ahead search, for any state s, it can simply find any action that maximizes $q_*(s,a)$. Hence, at the cost of representing a function of state-action pairs, insteads of the state, the optimal action-value function allows optimal action to be selected without having to know everything about possible successor states and their values (environment's dynamic) .</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are equivalences between state and action values:
$$
v_\pi(s) = \sum _a \pi(a|s) q_\pi(s,a)
$$</p>
$$
v_*(s) = \max _a q_*(s,a)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In order to solve the Bellman Optimality Equation, because it is non linear (contain max operator) so we can not use the same matrix solution as for policy evaluation. So we will use iterative methods to deal with that. 
If we use a model, it is called Dynamic Programming to get into the optimal policy. 
If it is based on sample, we can use Monte Carlo, Q-learning and SARSA.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Dynamic-Programming">
<a class="anchor" href="#Dynamic-Programming" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dynamic Programming<a class="anchor-link" href="#Dynamic-Programming"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><b> By definition, Dynamic Programming (DP) refers to a collection of algorithms that can be used to compute optimal polices given a perfect model of the environment as a MDP. </b></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Policy-evaluation-(Prediction)">
<a class="anchor" href="#Policy-evaluation-(Prediction)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Policy evaluation (Prediction)<a class="anchor-link" href="#Policy-evaluation-(Prediction)"> </a>
</h2>
<p>It refers to how to compute the state-value function $v_\pi$ for a policy $\pi$.
$$
v_\pi(s) = \mathbb{E} [R_{t+1} + \gamma v_ \pi(S_{t+1}) | s, \pi]
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The basic idea is to initialize the first guess to zero for all the possible states and then we are going to iterate this equation, we are going to assign values which is subscripted as k to indicate that it is an estimate at that time. The way we assign is based on the expectation of one-step ahead relying on true model and we are going to boostrap at the value of iteration k at the next state, and we will use that to find new values at k+1.</p>
<p>Here, the bootstraping means we use the estimate at time k to estimate the value at time k+1.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
v_{k+1} (s) = \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) | s, \pi]
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Whenever $v_{k+1}(s) = v_k(s)$, then we must found $v_\pi$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Policy-improvement">
<a class="anchor" href="#Policy-improvement" aria-hidden="true"><span class="octicon octicon-link"></span></a>Policy improvement<a class="anchor-link" href="#Policy-improvement"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By doing policy evaluation, we can evaluate the state value under the policy $\pi$. By doing that, we know how good it is to follow the current policy by choosing a particular action. However, we do not know it is better or worst to change to a new policy or not because another policy can have better value function (better policy).</p>
<p>Let's consider the action value:
$$
q_\pi(s,a) = \mathbb{E} [R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=s, A_t=a] \\\\
= \sum _{s',r} p(s',r|s,a)[r+\gamma v_\pi(s')]
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By picking the greedy action according to action values, we can examine if it is better to change a policy or not.
$$
\pi_{new}(s) = \argmax_{a} q_\pi(s,a) \\\\
= \argmax _a \mathbb{E} [r_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=s, A_t=a] 
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The decide of picking a new policy depends on the greedy action value according to policies. 
In priciple, we can repeat these processes to change the policy to a better one and re-evaluate the value function, perform the policy improvement and so on.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By doing that, we can continually evaluate value function and improve our policy to a better ones until the value of new policy is the same as the old one, then the new policy should be the optimal one.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://miro.medium.com/max/1034/1*ilt4JYda72oe9Eit6KnHwg.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>These processes are called <i> Policy Iteration </i>.  Policy Iteration comprises 2 alternative processes which are policy evaluation and policy improvement.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Value-Iteration">
<a class="anchor" href="#Value-Iteration" aria-hidden="true"><span class="octicon octicon-link"></span></a>Value Iteration<a class="anchor-link" href="#Value-Iteration"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One draw back of the policy iteration is that its policy evaluation requires multiple sweeps through the state set and we do need to compute the state values of the policy we are considering at that time.  Basically, it is the inner loop to calculate the value function of current policy itself and we have the outer loop for the policy imprvoment.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In principle, we can truncate the policy evaluation step into several ways without losing the convergences guarantees of policy iteration. One special case us when policy evaluation is stopped every iteration. This algorithm is called <i>value iteration. </i></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can turn the Bellman optimality equation into an update.
$$
v_{k+1}(s) \leftarrow \max _a \mathbb{E} [R_{t+1} + \gamma v_k(S_{t+1}) | S_t=s, A_t=a]
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is equivalent to policy iteration, with k=1 step of policy evaluation between each two policy improvement steps.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Bootstraping-in-Dynamic-Programming">
<a class="anchor" href="#Bootstraping-in-Dynamic-Programming" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bootstraping in Dynamic Programming<a class="anchor-link" href="#Bootstraping-in-Dynamic-Programming"> </a>
</h2>
<p>Dynamic programming improves the estimates of the value at a state using the estimate of the value function at next states, it is sometimes called learning a guess from a guess and it is a core idea in Reinforcement Learning (bootstraping).</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="dnlam/fastblog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/fastblog/2020/06/14/MDP_DP.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/fastblog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/fastblog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/fastblog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My blog about code and ideas.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/dnlam" target="_blank" title="dnlam"><svg class="svg-icon grey"><use xlink:href="/fastblog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/lam-dinh-34a66a160" target="_blank" title="lam-dinh-34a66a160"><svg class="svg-icon grey"><use xlink:href="/fastblog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>

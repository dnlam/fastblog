<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Model Free Control | Lam Dinh</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Model Free Control" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Fourth in a series on understanding Reinforcement Learning." />
<meta property="og:description" content="Fourth in a series on understanding Reinforcement Learning." />
<link rel="canonical" href="https://dnlam.github.io/fastblog/2020/06/30/Model_Free_Control.html" />
<meta property="og:url" content="https://dnlam.github.io/fastblog/2020/06/30/Model_Free_Control.html" />
<meta property="og:site_name" content="Lam Dinh" />
<meta property="og:image" content="https://www.mdpi.com/symmetry/symmetry-12-01685/article_deploy/html/images/symmetry-12-01685-g002.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-30T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://www.mdpi.com/symmetry/symmetry-12-01685/article_deploy/html/images/symmetry-12-01685-g002.png" />
<meta property="twitter:title" content="Model Free Control" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2020-06-30T00:00:00-05:00","datePublished":"2020-06-30T00:00:00-05:00","description":"Fourth in a series on understanding Reinforcement Learning.","headline":"Model Free Control","image":"https://www.mdpi.com/symmetry/symmetry-12-01685/article_deploy/html/images/symmetry-12-01685-g002.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://dnlam.github.io/fastblog/2020/06/30/Model_Free_Control.html"},"url":"https://dnlam.github.io/fastblog/2020/06/30/Model_Free_Control.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/fastblog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://dnlam.github.io/fastblog/feed.xml" title="Lam Dinh" /><link rel="shortcut icon" type="image/x-icon" href="/fastblog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/fastblog/">Lam Dinh</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/fastblog/about/">About Me</a><a class="page-link" href="/fastblog/search/">Search</a><a class="page-link" href="/fastblog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Model Free Control</h1><p class="page-description">Fourth in a series on understanding Reinforcement Learning.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-06-30T00:00:00-05:00" itemprop="datePublished">
        Jun 30, 2020
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/dnlam/fastblog/tree/master/_notebooks/2020-06-30-Model_Free_Control.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/fastblog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/dnlam/fastblog/master?filepath=_notebooks%2F2020-06-30-Model_Free_Control.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/fastblog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/dnlam/fastblog/blob/master/_notebooks/2020-06-30-Model_Free_Control.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/fastblog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fdnlam%2Ffastblog%2Fblob%2Fmaster%2F_notebooks%2F2020-06-30-Model_Free_Control.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/fastblog/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#Objectives">Objectives </a></li>
<li class="toc-entry toc-h1"><a href="#Monte-Carlo-for-Control">Monte Carlo for Control </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Model-Free-Policy-Iteration-Using-Action-Value-Function">Model Free Policy Iteration Using Action-Value Function </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#TD-for-Control">TD for Control </a>
<ul>
<li class="toc-entry toc-h2"><a href="#SARSA-Algorithm">SARSA Algorithm </a></li>
<li class="toc-entry toc-h2"><a href="#Off-policy-TD-and-Q-learning">Off-policy TD and Q-learning </a></li>
<li class="toc-entry toc-h2"><a href="#On-and-Off-Policy-Learning">On and Off-Policy Learning </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Overestimation-in-Q-learning">Overestimation in Q-learning </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Double-Q-Learning">Double-Q Learning </a></li>
<li class="toc-entry toc-h2"><a href="#Off-Policy-Learning:-Importance-Sampling-Corrections">Off Policy Learning: Importance Sampling Corrections </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-06-30-Model_Free_Control.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Objectives">
<a class="anchor" href="#Objectives" aria-hidden="true"><span class="octicon octicon-link"></span></a>Objectives<a class="anchor-link" href="#Objectives"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Previously, we have learned qbout <a href="https://dnlam.github.io/fastblog/2020/06/21/Model_Free_Prediction_Control.html">Model Free Prediction</a> which enables us to estimate the value function of unknown MDP using sampling (Monte Carlo and Temporal Different Learning Algorithms).</p>
<p>In this notebook, we will talk about model free control which we will try to optimize these value functions.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Monte-Carlo-for-Control">
<a class="anchor" href="#Monte-Carlo-for-Control" aria-hidden="true"><span class="octicon octicon-link"></span></a>Monte Carlo for Control<a class="anchor-link" href="#Monte-Carlo-for-Control"> </a>
</h1>
<h2 id="Model-Free-Policy-Iteration-Using-Action-Value-Function">
<a class="anchor" href="#Model-Free-Policy-Iteration-Using-Action-Value-Function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model Free Policy Iteration Using Action-Value Function<a class="anchor-link" href="#Model-Free-Policy-Iteration-Using-Action-Value-Function"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To improve a policy (policy improvement) as we have noticed in Dynamic Programming, we can make it greedy. However, rhe greedy policy improvement over state value function does require the knowledge of MDP  model.
$$
\pi'(s) = \argmax _ {a} \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t=s, A_t=a]
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Because we are not in model-free for the calculation of the expectation, so it is become curbersome to perform the policy improvement without a model.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Fortunately, instead of estimating state values, we could choose to estimate state action values, then, we can much more easily find the greedy policy by pick the highest valued action in every state.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\pi '(s) = \argmax _a q(s,a)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then, we can apply the Generalised Policy Iteration with Action Value Function. First, starting from a random action value and policy, we can iteratively estimate the action value (e.g using Monte Carlo policy evaluation) and perform policy improvement.</p>
<p>However, if we choose to do greedy policy improvement then we wouldn't explore which means we cant sample all s,a when learning by interacting. And it makes the policy evaluation susceptible to problems. For example, if we have a fully greedy policy and we are trying to evaluate that with MC algorithm. Then, it might not select certain actions in certain states and that means we do not have a reliable estimate of this action value from those actions and the improvement step could be wrong.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To deal with that problem, we can consider $\epsilon$-greedy policy where we allow a small probability of picking any action.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>by doing so, we might have a full model free algorithm that can be used for policy iteration.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="TD-for-Control">
<a class="anchor" href="#TD-for-Control" aria-hidden="true"><span class="octicon octicon-link"></span></a>TD for Control<a class="anchor-link" href="#TD-for-Control"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Basically, one would like to apply the TD learning to the action value functions to have the same convenience of being able to pick a greedy or $\epsilon -$ greedy policy. Plus, we can continue the update every time step because TD can learn from individual transition without completing the full episode.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="SARSA-Algorithm">
<a class="anchor" href="#SARSA-Algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>SARSA Algorithm<a class="anchor-link" href="#SARSA-Algorithm"> </a>
</h2>
<p>One way to do it is to use SARSA Algorithm which is just a TD learning for state-action value.
$$
q_{t+1}(S_t,A_t) = q_t(S_t,A_t) + \alpha _t(R_{t+1} + \gamma q(S_{t+1},A_{t+1}) - q(S_t,A_t) )
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then, we can use this inplace of MC learning to do the policy evaluation step. The policy improvement now is still supportedly considered as $\epsilon$ -greedy improvement.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Off-policy-TD-and-Q-learning">
<a class="anchor" href="#Off-policy-TD-and-Q-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Off-policy TD and Q-learning<a class="anchor-link" href="#Off-policy-TD-and-Q-learning"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In previous cases, we considered MC and SARSA algorithms where we are interleaving the policy with evaluation and improvement. Now, we turn to <code>Off-policy</code> learning which means that learning about a policy different from the on we are following and there is a specific very popular algorithm which is <code>Q-learning</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Q-learning corresponds to sampling of the following value interation $q_{k+1}(s,a) = \mathbb{E}[R_{t+1} + \gamma \max _{a'} q_k(S_{t+1},a') | S_t=s, A_t=a ] $</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here, the interation equation has a maximization over the action in the one step ahead , the one that maximize our action values.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>we can eventually sample this with the following equation:
$$
q_{k+1}(s,a) = q_t(S_t,A_t) + \alpha _t(R_{t+1} + \gamma \max _{a'} q_t(S_{t+1},a') -q_t(S_t,A_t))
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Inside the paranthesis, we bootstrap differently with SARSA. Instead of considering the next action in the next state, we consider the best action that we can possibly take according to our current estimates and then use that for the greedy value of our current value function.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="On-and-Off-Policy-Learning">
<a class="anchor" href="#On-and-Off-Policy-Learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>On and Off-Policy Learning<a class="anchor-link" href="#On-and-Off-Policy-Learning"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>On policy learning is about learning from a behaviour policy from experienced sample from that policy. Here, we always consider there are just one policy (Monte Carlo, SARSA) and that policy would be used to generate the behaviour because we want to take it to evaluate/predict the value functions of that policy. We called that is On-policy learning because we are studying the policy that we are following.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>On the other hand, Off policy learns about a <code>target</code> policy $\pi$ but the experience sample is from a different policy $\mu$. This refers essentially to learning counterfactual about the other things we could do "what if...?"</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In general, in off policy, we  would like to evaluate a target policy $\pi(a|s)$ to compute the estimated  value of that policy $v_\pi(s)$ or $q_\pi(s,a)$ while using behavioud policy $\mu(a|s)$ to generate actions.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In practice, it is important because of several possible reasons:</p>
<ul>
<li>We want to learn from observing database (stored experience) </li>
<li>Reuse experience from old policies (past experience) </li>
<li>Learn about multiple policies while following one policy </li>
<li>Learn about greedy policy while following exploratory policy.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Q learning estimates the value of greedy policy
$$
q_{t+1}(S_t,A_t) = q_t(S_t,A_t) + \alpha _t(R_{t+1} + \gamma \max _{a'} q_t(S_{t+1},a') -q_t(S_t,A_t))
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It imples that we are learning about the action value if we take this action and then in the next step we would be greedy. It is a valid target to update and it would be learning about the greedy policy but we do not need to react all the time according to that policy because it does not explore efficiently.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this case, the action value function directly approximate $q_*$ independantly from the policy being followed.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><i>Theorem: Q-learning control converges to the optimal action-value function, $q \rightarrow q^* $ as long as we take each action in each state infinitely often. </i></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>IT is proved that it works for any policy that eventually selects all actions sufficiently often (requires appropriately decaying step size $\sum _t \alpha _t = \infty$ and $\sum _t \alpha _t ^2 &lt; \infty$)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Overestimation-in-Q-learning">
<a class="anchor" href="#Overestimation-in-Q-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overestimation in Q-learning<a class="anchor-link" href="#Overestimation-in-Q-learning"> </a>
</h3>
<p>Basically, Q-learning can overestimate the action values. Let's see the bootstrap target that is used in Q-learning.
$$
\max _a q_t(S_{t+1},a) = q_t(S_{t+1},\argmax _a q_t(s_{t+1},a))
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The issue of the Q-learning by looking on this equation is that we use the same value function  to both select the action and evaluate the action. However, the action value are approximate. It means that the estimated action value could be wrong and it can comes up with selecting overestimated values rather than underestimated values.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In order to deal with the overestimation in Q-learning, we need to decouple the action values for the selection and evaluation. Therefore, in gives birth to an algorithm which is socalled <code>Double-Q Learning</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Double-Q-Learning">
<a class="anchor" href="#Double-Q-Learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Double-Q Learning<a class="anchor-link" href="#Double-Q-Learning"> </a>
</h2>
<p>Double-Q learning maintain two different state action value functions; $q$ and $q'$.
$$
R_{t+1} + \gamma q'_t (S_{t+1}, \argmax _a q_t(S_{t+1},a)) \quad \quad (1) \\
R_{t+1} + \gamma q_t (S_{t+1}, \argmax _a q'_t(S_{t+1},a)) \quad \quad (2)\\
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The first equation is the standard thing with one step reward and discounted value of the next state. But the value of the next state is defined by picking an action according to our normal action value function $q$ but then evaluating the value of that action with $q'$.</p>
<p>Then, we can use each of these as targets for these two different action value functions. Every timestep, we will pick one of the action value functions ($q$ for (1) and $q'$ for (2)) to update. We never update both at the same time because it will correlate them. Then we can get unbiased estimate of the greedy action according to one of them.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It has been proved that Double Q-Learning also converges to the optimal policy under the same condition as Q-Learning.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Off-Policy-Learning:-Importance-Sampling-Corrections">
<a class="anchor" href="#Off-Policy-Learning:-Importance-Sampling-Corrections" aria-hidden="true"><span class="octicon octicon-link"></span></a>Off Policy Learning: Importance Sampling Corrections<a class="anchor-link" href="#Off-Policy-Learning:-Importance-Sampling-Corrections"> </a>
</h2>
<p>The goal under  some function f with random input X and a distribution $d'$ , we would like to estimate the expectation of f(X) under a different distribution $d$ (target).</p>
<p>The solution is to re-weight the function.
$$
\mathbb{E} _{x \sim d} [f(x)] = \sum d(x) f(x) \\
= \sum d'(x) \frac{d(x)}{d'(x)} f(x) \\
= \mathbb{E} _{x \sim d'} [\frac{d(x)}{d'(x)} f(x)]
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It means that if we sample the final thing, we would actually get un unbiased estimate of the thing that we were looking for on the left hand side.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There is an assumption where the distribution $d'(x)$ should be different to 0, otherwise it intuitively states that of we never actually sample x under $d'$, we cant expect to learn from it. The intuition will be scale up events that are rare under $d'$, but common under $d$ because $d'$ will be small and $d$ will be large and vice versa.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="dnlam/fastblog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/fastblog/2020/06/30/Model_Free_Control.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/fastblog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/fastblog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/fastblog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My blog about code and ideas.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/dnlam" target="_blank" title="dnlam"><svg class="svg-icon grey"><use xlink:href="/fastblog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/lam-dinh-34a66a160" target="_blank" title="lam-dinh-34a66a160"><svg class="svg-icon grey"><use xlink:href="/fastblog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>

<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Exploitation-Exploration Dilemma | Lam Dinh</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Exploitation-Exploration Dilemma" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="First in a series on understanding Reinforcement Learning." />
<meta property="og:description" content="First in a series on understanding Reinforcement Learning." />
<link rel="canonical" href="https://dnlam.github.io/fastblog/2020/06/06/Exploration_Exploitation.html" />
<meta property="og:url" content="https://dnlam.github.io/fastblog/2020/06/06/Exploration_Exploitation.html" />
<meta property="og:site_name" content="Lam Dinh" />
<meta property="og:image" content="https://miro.medium.com/max/1400/1*ywOrdJAHgSL5RP-AuxsfJQ.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-06T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://miro.medium.com/max/1400/1*ywOrdJAHgSL5RP-AuxsfJQ.png" />
<meta property="twitter:title" content="Exploitation-Exploration Dilemma" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2020-06-06T00:00:00-05:00","datePublished":"2020-06-06T00:00:00-05:00","description":"First in a series on understanding Reinforcement Learning.","headline":"Exploitation-Exploration Dilemma","image":"https://miro.medium.com/max/1400/1*ywOrdJAHgSL5RP-AuxsfJQ.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://dnlam.github.io/fastblog/2020/06/06/Exploration_Exploitation.html"},"url":"https://dnlam.github.io/fastblog/2020/06/06/Exploration_Exploitation.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/fastblog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://dnlam.github.io/fastblog/feed.xml" title="Lam Dinh" /><link rel="shortcut icon" type="image/x-icon" href="/fastblog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/fastblog/">Lam Dinh</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/fastblog/about/">About Me</a><a class="page-link" href="/fastblog/search/">Search</a><a class="page-link" href="/fastblog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Exploitation-Exploration Dilemma</h1><p class="page-description">First in a series on understanding Reinforcement Learning.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-06-06T00:00:00-05:00" itemprop="datePublished">
        Jun 6, 2020
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      10 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/dnlam/fastblog/tree/master/_notebooks/2020-06-06-Exploration_Exploitation.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/fastblog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/dnlam/fastblog/master?filepath=_notebooks%2F2020-06-06-Exploration_Exploitation.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/fastblog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/dnlam/fastblog/blob/master/_notebooks/2020-06-06-Exploration_Exploitation.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/fastblog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fdnlam%2Ffastblog%2Fblob%2Fmaster%2F_notebooks%2F2020-06-06-Exploration_Exploitation.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/fastblog/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#Preliminaries">Preliminaries </a></li>
<li class="toc-entry toc-h1"><a href="#Introduction">Introduction </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Multi-Armed-Bandit">Multi-Armed Bandit </a>
<ul>
<li class="toc-entry toc-h3"><a href="#$\epsilon$-greedy-algorithm">$\epsilon$-greedy algorithm </a></li>
<li class="toc-entry toc-h3"><a href="#Lower-Bound">Lower Bound </a></li>
<li class="toc-entry toc-h3"><a href="#Upper-Confidence-Bound">Upper Confidence Bound </a></li>
<li class="toc-entry toc-h3"><a href="#Bayesian-Bandits">Bayesian Bandits </a></li>
<li class="toc-entry toc-h3"><a href="#Bayesian-Bandits-with-Upper-Confidence-Bounds">Bayesian Bandits with Upper Confidence Bounds </a></li>
<li class="toc-entry toc-h3"><a href="#Thompson-Sampling">Thompson Sampling </a></li>
</ul>
</li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-06-06-Exploration_Exploitation.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Preliminaries">
<a class="anchor" href="#Preliminaries" aria-hidden="true"><span class="octicon octicon-link"></span></a>Preliminaries<a class="anchor-link" href="#Preliminaries"> </a>
</h1>
<ul>
<li>A <code>policy</code> defines the agent's behaviours<ul>
<li>Deterministic policy A = $\pi(S)$</li>
<li>Stochastic policy: $\pi(A|S) = p(A|S)$ </li>
</ul>
</li>
<li>
<p>The actual value function is the expected return
$$
v_\pi(s) = \mathbb{E} [G_t | S_t = s, \pi] = \mathbb{E} [R_{t+1} + \gamma R_{t+2 + ...} | S_t = s, \pi] 
=\mathbb{E} [R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t ~= \pi(s)]
$$
where $\gamma$ is a discount factor</p>
</li>
<li>
<p>Optimal value is the highest possible value for any policy</p>
</li>
</ul>
$$
v_*(s) = \max_{a} \mathbb{E} [R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t=a]
$$<ul>
<li>The true action value for action <code>a</code> is the expected reward
$$
q(a) = \mathbb{E}\left\{R_t | A_t=a \right\}
$$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>A simple estimate of the action value is the average of the sampled rewards:
$$
Q_t(a)=\frac{\sum_{n}^{}R_n\mathbb{I}(A_n=a)}{\sum_{n}^{}\mathbb{I}(A_n=a)}
$$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>where $R_n$ is the reward at time n.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The update of the action values at tume step n+1
$$
Q_{n+1} = Q_n + \frac{1}{n} (R_n - Q_n)
$$</li>
</ul>
<p>where $\alpha=\frac{1}{n}$ is a step size.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>
<p>The optimal value is
$$
v_* = \max_{a \in A}  q(a)
$$</p>
</li>
<li>
<p>Regret is the opportunity loss for one step</p>
</li>
</ul>
$$
v_* - q(A_t)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Action Regret $\Delta_a$ for a fiven action is the difference between optimal value and true value of a
$$
\Delta_a = v_* -q(a)
$$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The trade off between exploration and exploitation will be done by minimizing the total regret</li>
</ul>
$$
\begin{split}
L_t &amp;= \sum_{i=1}^t (v_* - q(a_i)) \\
&amp;= \sum _{a \in \mathcal{A}} N_t(a)(v_* - q(a_i)) = \sum _{a \in \mathcal{A}} N_t(a) \Delta _a

\end{split}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>
<p>Categorizing Agents</p>
<ul>
<li>Value Based (Value Function)</li>
<li>Policy Based (Policy)</li>
<li>Actor Critic (Policy and Value Function)</li>
</ul>
</li>
<li>
<p>Prediction and Control</p>
<ul>
<li>Prediction: evaluate the future for a given policy</li>
<li>Control: optimize the future (find the best policy)
$$
\pi_*(s) = \argmax_{\pi} v_\pi (s) 
$$</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction<a class="anchor-link" href="#Introduction"> </a>
</h1>
<p>The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the action taken rather than instructs by giving correct actions. It creates the need for active exploration, for an explicit search of good behaviour.</p>
<p>Evaluative feedback indicates how good the action taken was, but not whether it was the best of worst action possible. On the other hand, instructive feedback indicates the correct action to take  which is the basis of supervised learning.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The well known trade-off between exploitation and exploration is essentially the compromise between maximizing performance (exploitation)  and increasing the knowledge (exploration). It is the typical problem in online decision making because we are actively collecting our information to make the best overall decisions.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Multi-Armed-Bandit">
<a class="anchor" href="#Multi-Armed-Bandit" aria-hidden="true"><span class="octicon octicon-link"></span></a>Multi-Armed Bandit<a class="anchor-link" href="#Multi-Armed-Bandit"> </a>
</h2>
<p>Consider the following learning problem: we are amongs the choice of k different options, after each choice, we receiver a numerical reward which are sampled from a stationary probability distribution that depends on what we selected. The objective is to maximize the expected reward $\sum _i R_i$ over some time period. This is the original form of the k-armed bandit problem.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We do the maximization objective by learning a policy: a distribution on $\mathcal{A}$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When we look into the total regrets, we see the differences between the optimal value and the actual action value which are accumulated as time evolves. The objective is to minimise that regrets because the faster it grows, the worst it is.</p>
<p>In principle, the regret can grow unbounded, so the interesting part is to study how fast it grows. For example, greedy policy has linear regret as it grows in the number of step we have taken.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="$\epsilon$-greedy-algorithm">
<a class="anchor" href="#%24%5Cepsilon%24-greedy-algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>$\epsilon$-greedy algorithm<a class="anchor-link" href="#%24%5Cepsilon%24-greedy-algorithm"> </a>
</h3>
<p>To explore the new information, the most basic strategy is to apply the common solution $\epsilon$-greedy where a randomness is added to the policy for picking a random action uniformly across all of the actions on each time step with a certain probability. It is considered as the most common exploration strategy in current Reinforcement Learning. By doing this, we can avoid the sticking at the sub optimal action forever as the greedy policy does and it continues to explore forever.</p>
$$
\pi_t(a)=\left\{\begin{matrix}
(1-\varepsilon ) + \frac{\varepsilon }{\mathcal{A}} &amp; \text{if $Q_t(a)=\max_{b}Q_t(b)$}  \\
 \frac{\varepsilon }{\mathcal{A}}&amp; \text{otherwise} \\
\end{matrix}\right.
$$<p>Typically, the performance of $\epsilon$-greedy is better than greedy because we do learn event and we select the optimal action with probability 1-$\epsilon$ which is not guaranteed in greedy case. However, the regret still grows linearly so the question is whether ncessary to explore something that are already bad?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Lower-Bound">
<a class="anchor" href="#Lower-Bound" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lower Bound<a class="anchor-link" href="#Lower-Bound"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A decade ago, there was an investigation about the exploration problem to see what is the best possible thing we could hope to get. It is related to the similarities between the optimal action and all the others actions (similar distribution but different means).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Lai and Robbins Theorem: Asymtotic total regret is at least logarithmic in number of steps
$$
\lim_{t \to \infty} L_t \geq \log t \sum ...
$$</p>
<p>It shows out that the total expected regrets that we will incur for any algorithm will grow on the logarithm of time. So, it means that the regrets grows unbounded as expected.</p>
<p>However, the grow speed of $\epsilon$-greedy is much slower than the greedy.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the optimism in the face of uncertainty, practically, if we are more uncertain about the value of an action, maybe it is more important to explore that action but at first we need to be more greedy to try the action with the highest mean.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Upper-Confidence-Bound">
<a class="anchor" href="#Upper-Confidence-Bound" aria-hidden="true"><span class="octicon octicon-link"></span></a>Upper Confidence Bound<a class="anchor-link" href="#Upper-Confidence-Bound"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Exploration is needed because there is always uncertainty about the accuracy of the action-value estimates. So it will be better to select amongs the non-greedy actions according to their potential for actually being optimal , taking into account how close their estimates are maximal and the uncertainties in those estimates.</p>
<p>One effective method to do so is upper confidence bound $U_t(a)$ for each action value $q(a)$. We"ll do such as the true value with a high probability is smaller than the current estimate plus the bonus $q(a)\leq Q_t(a)+U_t(a)$ .</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It signifies that although we do not knwo the true value is but we are pretty certain that it is less than something. We will then design an algorithm that just greedily selects among the actions whose estimates are added a bonus. Normally the bonus will be high if the uncertainties are high.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
A_t = \argmax_{a} [Q_t(a) + c\sqrt{\frac{ln t}{N_t(a)}}]
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Where $N_t(a)$ represents the number of times action $a$ has been selected prior to time t. Then, the uncertainty about the true value will typically decrease as the square root of the number of times you selected an action.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Recall that we want to minimize $\sum_a N_t(a)\Delta_a$, so if the gap $\Delta_a$ is big, we want $N_t(a)$ to be small and vice versa. As $N_t(a)$ grows over the time, so we hope to often select the actions with low gap than the actions with high gap.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Hoeffding's Inequality: Let $X_1, .., X_n$ be i.i.d  random variables in [0,1], and let $\overline{X_t}$ be the mean. Then
$$
p(\mathbb{E}[X] \geq \overline{X_n}+u) \leq \exp{(-2nu^2)}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It means that if we selected a number of points and averaged these, it is an added bonus that we can bound the probability that it is still an underestimate. The bigger u us, the smaller this probability will be.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, let's say that we want to pick certain probability $p$ that the true value exceeds the upper confidence bound.</p>
$$
\exp{(-2N_t(a)U_t(a)^2)} = p
$$<p>Then,</p>
$$
U_t(a) = \sqrt{(\frac{-\log p}{2N_t(a)})} = \sqrt{(\frac{\log t}{2N_t(a)})}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then, it leads to our UCB algorithm:</p>
$$
a_t = \argmax_{a \in \mathcal{A}} Q_t(a) + c\sqrt{\frac{\log t}{N_t(a)})}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Intuitively speaking, if we consider an action that we have not selected in a long long while, it means that the bonus keep growing ultil it is higher than all of the other estimate plus bonuses for all the other actions. At that point, we will select it.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Bayesian-Bandits">
<a class="anchor" href="#Bayesian-Bandits" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bayesian Bandits<a class="anchor-link" href="#Bayesian-Bandits"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Besides value-based algorithm, we can come up with model-based approach which predict the reward from reward model for each action. However, we are based on the model, so we can model the distribution of rewards as well and it gives us Bayesian bandit.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Bayesian bandits model parameterized distributions over rewards, $p(q(a) | \theta_t)$ which is a Bayesian probability. This is interpreted as our belief that $q(a)=x$ $ \forall x \in \mathcal{R}$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We are going to update this using probability of the reward under the model.
$$
p_t(\theta |a) \propto p(R_t | \theta,a)p_{t-1}(\theta|a)
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$\theta$ represents the parameters of our parametric model. Then, it allows us to inject rich prior knowledge $p_0(\theta|a)$, it means that we could update the parameters each time we see our reward from taken action.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here, we are not trying to match the distribution of the reward, we are trying to learn a belief distribtion over where the true value is. Afterwards, we are going to use these distributions to do the exploration.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Bayesian-Bandits-with-Upper-Confidence-Bounds">
<a class="anchor" href="#Bayesian-Bandits-with-Upper-Confidence-Bounds" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bayesian Bandits with Upper Confidence Bounds<a class="anchor-link" href="#Bayesian-Bandits-with-Upper-Confidence-Bounds"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's imagine that we have multiple distributions for different actions. With UCB, we pick the bound which basically defines our confidence interval. Now, we could look at the distribution and define our confidence interval as might be the standand deviation of our distribution.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Probability matching selects action a according to probability that a is the optimal action. 
$$
\pi_t(a) = p(q(a)=\max_{b}q(b) | H_{t-1})
$$</p>
<p>It means that we are defining a policy and making this policy equal to the probability that the true value is the maxmimum true value under the history. So we can calculate the probability of an action is optimal one under the distribution (belief distribution) that we updated so far.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Probability matching is optimistic in the face of uncertainty because if we have a large uncertainty about where our action will be, then the probability of it being optimal also goes up.</p>
<p>So actions with larger probabilities are either high-valued actions or we have a lot of uncertainty about them.</p>
<p>However, it can be difficult to compute $\pi(a)$ analytically from posterior.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Thompson-Sampling">
<a class="anchor" href="#Thompson-Sampling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Thompson Sampling<a class="anchor-link" href="#Thompson-Sampling"> </a>
</h3>
<p>Thompson Sampling is an algorithm to solve bandits and it is a Baysian approach and related to probability matching. The idea is to keep track of the posterior distributions so we are going to update these via Bayes's rule, and then, we sample each from belief distributions an actual action value.  ($Q_t(a) \sim p_t(q(a)) $)</p>
<p>We have the belief distribution at time step t about where we believe the mean value for that action to be, then we are going to sample the distribution which gives us an action value and we keep doing that for each of these actions.</p>
<p>Then, we are going to pick the greedy action according to the sample action values. 
$$
A_t = \argmax_{a\in \mathcal{A}} Q_t(a)
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It turns out that Thompson Sampling will select actions according to the same probability as probability matching would do.</p>
$$
\pi_t(a) = \mathbb{E}[I(Q_t(a)=\max_{b}Q_t(b))] = p(q(a)=\max_{b}q(b)))
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The event of action a is picked means that action a had the highest sampled action value, but the expectation of this instance is equal to the probability of that happening. So it turns our that if we first sample the action values and then we pick greedily, this is the same as trying to compute these whole probabilities and selecting our action according to that.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So in some sense, TS is simply a technique that allow us to go from Bayesian distributions to a policy. Interestingly, TS achieves Lai and Robbins lower bounds on regret, there for it is optimal in the same way UCB is and it has logarithmic regrets.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="dnlam/fastblog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/fastblog/2020/06/06/Exploration_Exploitation.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/fastblog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/fastblog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/fastblog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My blog about code and ideas.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/dnlam" target="_blank" title="dnlam"><svg class="svg-icon grey"><use xlink:href="/fastblog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/lam-dinh-34a66a160" target="_blank" title="lam-dinh-34a66a160"><svg class="svg-icon grey"><use xlink:href="/fastblog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>

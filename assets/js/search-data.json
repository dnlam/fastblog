{
  
    
        "post0": {
            "title": "Steps On GANs",
            "content": ". Image credit: Dev Nag . This post is not necessarily a crash course on GANs. It is at least a record of me giving myself a crash course on GANs. Adding to this as I go. . Intro/Motivation . I’ve been wanting to grasp the seeming-magic of Generative Adversarial Networks (GANs) since I started seeing handbags turned into shoes and brunettes turned to blondes… …and seeing Magic Pony’s image super-resolution results and hearing that Yann Lecun had called GANs the most important innovation in machine learning in recent years. . Finally, seeing Google’s Cat-Pig Sketch-Drawing Math… . …broke me, and so…I need to ‘get’ this. . I’ve noticed that, although people use GANs with great success for images, not many have tried using them for audio yet (Note: see SEGAN paper, below). Maybe with already-successful generative audio systems like WaveNet, SampleRNN (listen to those piano sounds!!) and TacoTron there’s less of a push for trying GANs. Or maybe GANs just suck for audio. Guess I’ll find out… . Steps I Took . Day 1: . Gathered list of some prominent papers (below). | Watched video of Ian Goodfellow’s Berkeley lecture (notes below). | Started reading the EBGAN paper (notes below)… | …but soon switched to BEGAN paper – because wow! Look at these generated images: | Googled for Keras-based BEGAN implementations and other code repositories (below)…Noticed SEGAN… | …Kept reading BEGAN, making notes as I went (below). | Finished paper, started looking through BEGAN codes from GitHub (below) &amp; began trying them out… a. Cloned @mokemokechicken’s Keras repo, grabbed suggested LFW database, converted images via script, ran training… Takes 140 seconds per epoch on Titan X Pascal. . Main part of code is in models.py | . b. Cloned @carpedm’s Tensorflow repo, looked through it, got CelebA dataset, started running code. . | Leaving codes to train overnight. Next time, I’d like to try to better understand the use of an autoencoder as the discriminator. | Day 2: . My office is hot. Two Titan X GPUs pulling ~230W for 10 hours straight has put the cards up towards annoyingly high temperatures, as in ~ 85 Celsius! My previous nightly runs wouldn’t even go above 60 C. But the results – espically from the straight-Tensorflow code trained on the CelebA dataset – are as incredible as advertised! (Not that I understand them yet. LOL.) The Keras version, despite claiming to be a BEGAN implementation, seems to suffer from “mode collapse,” i.e. that too many very similar images get generated. | Fished around a little more on the web for audio GAN applications. Found an RNN-GAN application to MIDI, and found actual audio examples of what not to do: don’t try to just produce spectrograms with DCGAN and convert them to audio. The latter authors seem to have decided to switch to a SampleRNN approach. Perhaps it would be wise to heed their example? ;-) | Since EBGAN implemented autoencoders as discriminators before BEGAN did, I went back to read that part of the EBGAN paper. Indeed, section “2.3 - Using AutoEncoders” (page 4). (see notes below) | Ok, I basically get the autoencoder-discriminator thing now. :-) | Day 3: . “Life” intervened. :-/ Hope to pick this up later. . Papers . Haven’t read hardly any of these yet, just gathering them here for reference: . Original GAN Paper: ” Generative Adversarial Networks” by GoodFellow (2014) | DCGAN: “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks” by Radford, Metz &amp; Chintala (2015) | “Image-to-Image Translation with Conditional Adversarial Networks” by Isola et al (2016) | “Improved Techniques for Training GANs” by Salimans et al (2016). | DiscoGAN: “Learning to Discover Cross-Domain Relations with Generative Adversarial Networks” by Kim et al. (2017) | EBGAN: “Energy-based Generative Adversarial Network by Zhao, Matheiu &amp; Lecun (2016/2017). Remarks/Notes: | “This variant [EBGAN] converges more stably [than previous GANs] and is both easy to train and robust to hyper-parameter variations” (quoting from BEGAN paper, below). | If it’s energy-based, does that mean we get a Lagrangian, and Euler-Lagrange Equations, and Lagrange Multipliers? And thus can physics students (&amp; professors!) grasp these networks in a straightforward way? Should perhaps take a look at Lecun’s Tutorial on Energy-Based Learning. | “The energy is the resconstruction error [of the autoencoder]” (Section 1.3, bullet points) | | . Image credit: Roozbeh Farhoodi + EBGAN authors . “…256×256 pixel resolution, without a multi-scale approach.” (ibid) | Section 2.3 covers on the use of the autoencoder as a discriminator. Wow, truly, the discriminator’s “energy”/ “loss” criterion is literally just the reconstruction error of the autoencoder. How does that get you a discriminator?? | It gets you a discriminator because the outputs of the generator are likely to have high energies whereas the real data (supposedly) will produce low energies: “We argue that the energy function (the discriminator) in the EBGAN framework is also seen as being regularized by having a generator producing the contrastive samples, to which the discrim- inator ought to give high reconstruction energies” (bottom of page 4). | . | “Wasserstein GAN (WGAN) by Arjovsky, Chintala, &amp; Bottou (2017) . | “BEGAN: Boundary Equilibrium Generative Adversarial Networks” by Berthelot, Schumm &amp; Metz (April 2017). . Remarks/Notes: | “Our model is easier to train and simpler than other GANs architectures: no batch normalization, no dropout, no transpose convolutions and no exponential growth for convolution filters.” (end of section 3.5, page 5) | This is probably not the kind of paper that anyone off the street can just pick up &amp; read. There will be math. | Uses an autoencoder for the discriminator. | I notice that Table 1, page 8 shows “DFM” (from “Improving Generative Adversarial Networks with Denoising Feature Matching” by Warde-Farley &amp; Bengio, 2017) as scoring higher than BEGAN. | page 2: “Given two normal distributions…with covariances C1,C2C_1, C_2C1​,C2​,…”: see “Multivariate Normal Distribution”. | Section 3.3, Equilibrium: The “E[ ] mathbb{E}[ ]E[ ]” notation – as in E[L(x)] mathbb{E} left[ mathcal{L}(x) right]E[L(x)] – means “expected value.” See https://en.wikipedia.org/wiki/Expected_value | Introduces the diversity ratio: γ=E[L(G(z))]E[L(x)] gamma= frac{ mathbb{E} left[ mathcal{L}(G(z)) right]}{ mathbb{E} left[ mathcal{L}(x) right]}γ=E[L(x)]E[L(G(z))]​. “Lower values of γ gammaγ lead to lower image diversity because the discriminator focuses more heavily on auto-encoding real images.” | “3.5 Model architecture”: Did not actually get the bit about the autoencoder as the discriminator: “How does an autoencoder output a 1 or a zero?” | Ok, done. Will come back later if needed; maybe looking at code will make things clearer… | . | “SEGAN: Speech Enhancement Generative Adversarial Network” by Pascual, Bonafonte &amp; Serra (April 2017). Actual audio GAN! They only used it to remove noise. | . Videos . Ian Goodfellow (original GAN author), Guest lecture on GANs for UC Berkeley CS295 (Oct 2016). 1 hour 27 minutes. NOTE: actually starts at 4:33. Watch at 1.25 speed. Remarks/Notes: | This is on a fairly “high” level, which may be too much for some viewers; if hearing the words “probability distribution” over &amp; over again makes you tune out, and e.g. if you don’t know what a Jacobian is, then you may not want to watch this. | His “Taxonomy of Generative Models” is GREAT! | The discriminator is just an ordinary classifier. | So, the generator’s cost function can be just the negative of the discriminator’s cost function, (i.e. it tries to “mess up” the discriminator), however that can saturate (i.e. produce small gradients) so instead they try to “maximize the probability that the discriminator will make a mistake” (44:12). | “KL divergence” is a measure of the ‘difference’ between two PD’s. | “Logit” is the inverse of the sigmoid/logistical function. (logit&lt;–&gt;sigmoid :: tan&lt;–&gt;arctan) | Jensen-Shannon divergence is a measure of the ‘similarity’ between two PD’s. Jensen-Shannon produces better results for GANs than KL/maximum likelihood. | . | . Web Posts/Tutorials . “Machine Learning is Fun Part 7: Abusing Generative Adversarial Networks to Make 8-bit Pixel Art” by Adam Geitgey, skip down to “How DCGANs Work” (2017) | Post on BEGAN: https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/ | An introduction to Generative Adversarial Networks (with code in TensorFlow) | “Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch)” by Dev Nag (2017) | “Stability of Generative Adversarial Networks” by Nicholas Guttenberg (2016) | “End to End Neural Art with Generative Models” by Bing Xu (2016) . | Kording Lab GAN Tutorial by Roozbeh Farhoodi :-). Nicely done, has code too. . | . Code . Keras: . ‘Basic’ GAN with MNIST example: https://www.kdnuggets.com/2016/07/mnist-generative-adversarial-model-keras.html | GAN, BiGAN &amp; Adversarial AutoEncoder: https://github.com/bstriner/keras-adversarial | Kording Lab’s GAN tutorial, Jupyter Notebook https://github.com/KordingLab/lab_teaching_2016/blob/master/session_4/Generative%20Adversarial%20Networks.ipynb. (Code is short and understandable.) | Keras BEGAN: https://github.com/mokemokechicken/keras_BEGAN: Only works on 64x64 images; BEGAN paper shows some 128x128 | https://github.com/pbontrager/BEGAN-keras: No documentation, and I don’t see how it could run. I notice local variables being referenced in models.py as if they’re global. | . | Keras DCGAN (MNIST): https://github.com/jacobgil/keras-dcgan | Auxiliary Classifier GAN: https://github.com/lukedeo/keras-acgan | . Tensorflow: . BEGAN-Tensorflow: https://github.com/carpedm20/BEGAN-tensorflow | EBGAN.Tensorflow: https://github.com/shekkizh/EBGAN.tensorflow | SEGAN: https://github.com/santi-pdp/segan | DCGAN-Tensorflow: https://github.com/carpedm20/DCGAN-tensorflow | . PyTorch: . Tutorial &amp; simple implementation: https://github.com/devnag/pytorch-generative-adversarial-networks | . Datasets . CelebA: https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html | MNIST: https://yann.lecun.com/exdb/mnist/ | Speech enhancement: https://datashare.is.ed.ac.uk/handle/10283/1942 | “Labelled Faces in the Wild” https://vis-www.cs.umass.edu/lfw/ | . More References (Lists) . “Delving deep into Generative Adversarial Networks (GANs): A curated list of state-of-the-art publications and resources about Generative Adversarial Networks (GANs) and their applications.” | .",
            "url": "https://dnlam.github.io/fastblog/2022/03/15/GAN.html",
            "relUrl": "/2022/03/15/GAN.html",
            "date": " • Mar 15, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "The Beauty of Neural Network",
            "content": "The Sample Problem . Consider a system that tries to map groups of 3 inputs to some corresponding output which is a single number. In the following picture, we&#39;ll show each set of 3 inputs as a row of a matrix $X$, and each output as the corresponding row of $Y$: . $$ overbrace{ left[ { begin{array}{ccc} 0 &amp; 0 &amp; 1 0 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 end{array} } right] }^{X} rightarrow overbrace{ left[ { begin{array}{c} 0 0 1 1 end{array} } right] }^Y. $$Even though this system has an exact solution (namely, $Y$ equals the first column of $X$), usually we&#39;ll need to be satisfied with a system that maps our inputs $X$ to some approximate &quot;prediction&quot; $ tilde{Y}$, which we hope to bring closer to the &quot;target&quot; $Y$ by means of successive improvements. . The way we&#39;ll get our prediction $ tilde{Y}$ is by means of a weighted sum of each set of 3 inputs, and some nonlinear function $f$ which we call the &quot;activation function&quot; (or just &quot;activation&quot;). Pictorially, the process looks like the following, for each row $i$ of $X$ and $Y$, (where the columns of $X$ are shown arranged vertically instead of horizonally): . . In terms of matrix multiplication, since X is a 4x3 matrix, and Y is a 4x1 matrix, that implies that our weights should be a 3x1 matrix consisting of (unknown) values $w_0$, $w_1$ and $w_2$. The calculation can be written as: . $$ f left( overbrace{ left[ { begin{array}{ccc} 0 &amp; 0 &amp; 1 0 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 end{array} } right] }^ text{X} overbrace{ left[ { begin{array}{c} w_0 w_1 w_2 end{array} } right] }^{w} right) = overbrace{ left[ { begin{array}{c} 0 0 1 1 end{array} } right] }^{ tilde{Y}} $$Our nonlinear activation function $f$ is taken to operate on each row element one at a time, and we&#39;ll let $f_i$ denote the $i$th row of this completed activation, i.e.: . $$ f_i = f left( sum_j X_{ij}w_j right) = tilde{Y}_i . $$The particular activation function we will use is the &quot;sigmoid&quot;, . $$ f(x) = {1 over{1+e^{-x}}}, $$-- click here to see a plot of this function -- which has the derivative . $$ {df over dx} = {e^{-x} over(1 + e^{-x})^2} $$which can be shown (Hint: exercise for &quot;mathy&quot; students!) to simplify to $$ {df over dx}= f(1-f). $$ . The overall problem then amounts to finding the values of the &quot;weights&quot; $w_0, w_1,$ and $w_2$ so that the $ tilde{Y}$ we calculate is as close to the target $Y$ as possible. . To do this, we will seek to minimize a loss function defined as a sum across all data points we have, i.e. all 4 rows. The loss function $L$ we will choose is the mean square error loss, or MSE, we will use a &#39;better&#39; loss function for this problem): . $$ L = {1 over N} sum_{i=0}^{N-1} left[ tilde{Y}_i - Y_i right]^2, $$or in terms of the activation function $$ L = {1 over N} sum_{i=0}^{N-1} left[ f_i - Y_i right]^2. $$ . Each of the weights $w_j$ ($j=0..2$) will start with random values, and then be updated via gradient descent, i.e. . $$ w_j^{new} = w_j^{old} - alpha{ partial L over partial w_j} $$where $ alpha$ is the learning rate, chosen to be some small parameter. For the MSE loss shown above, the partial derivatives with respect to each of the weights is . $$ { partial L over partial w_j} = {2 over N} sum_{i=0}^{N-1} left[ tilde{Y}_i - Y_i right]{ partial f_i over partial w_j} = {2 over N} sum_{i=0}^{N-1} left[ tilde{Y}_i - Y_i right]f_i(1-f_i)X_{ij}. $$Absorbing the factor of 2/N into our choice of $ alpha$, and writing the summation as a dot product, and noting that $f_i = tilde{Y}_i$, we can write the update for all the weights together as . $$ w = w - alpha X^T cdot left( [ tilde{Y}-Y]* tilde{Y}*(1- tilde{Y}) right) $$where the $ cdot$ denotes a matrix-matrix product (i.e. a dot product for successive rows of $X^T$) and $*$ denotes elementwise multiplication. . To clarify the above expression in terms of matrix dimensions, we can see that $w$, a 3x1 matrix, can be made by multipyting $X^T$ (a 3 x4 matrix) with the term in parentheses, i.e. the product of elementwise terms involving $ tilde{Y}$, which is a 4x1 matrix. In other words, a 3x4 matrix, times a 4x1 matrix, yields a 3x1 matrix. . Actual Code . The full code for all of this is then... . # https://iamtrask.github.io/2015/07/12/basic-python-network/ import numpy as np # sigmoid activation def sigmoid(x,deriv=False): if(deriv==True): return x*(1-x) return 1/(1+np.exp(-x)) # input dataset X = np.array([ [0,0,1], [0,1,1], [1,0,1], [1,1,1] ]) # target output dataset Y = np.array([[0,0,1,1]]).T # seed random numbers to make calculation # deterministic (just a good practice) np.random.seed(1) # initialize weights randomly with mean 0 w = 2*np.random.random((3,1)) - 1 alpha = 1.0 # learning rate loss_history = [] # keep a record of how the loss proceeded, blank for now for iter in range(1000): # forward propagation Y_pred = sigmoid(np.dot(X,w)) # prediction, i.e. tilde{Y} # how much did we miss? diff = Y_pred - Y loss_history.append((diff**2).mean()) # add to the history of the loss # update weights w -= alpha * np.dot( X.T, diff*sigmoid(Y_pred, deriv=True)) print(&quot;Output After Training:&quot;) print(&quot;Y_pred = (should be two 0&#39;s followed by two 1&#39;s) n&quot;,Y_pred) print(&quot;weights = n&quot;,w) . Output After Training: Y_pred = (should be two 0&#39;s followed by two 1&#39;s) [[0.03178421] [0.02576499] [0.97906682] [0.97414645]] weights = [[ 7.26283009] [-0.21614618] [-3.41703015]] . Note that, because of our nonlinear activation, we don&#39;t get the solution $w_0=1, w_1=0, w_2=0$. . Plotting the loss vs. iteration number, we see... . %matplotlib inline import matplotlib.pyplot as plt plt.loglog(loss_history) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.show() . Change the activation function . Another popular choice of activation function is the rectified linear unit or ReLU. The function ReLU(x) is zero for x &lt;= 0, and equal to x (i.e. a straight line at 45 degrees for) x &gt;0. It can be written as max(x,0) or x * (x&gt;0), and its derivative is 1 for positive x, and zero otherwise. . Click here to see a graph of ReLU . Modifying our earlier code to use ReLU activation instead of sigmoid looks like this: . def relu(x,deriv=False): # relu activation if(deriv==True): return 1*(x&gt;0) return x*(x&gt;0) # seed random numbers to make calculation # deterministic (just a good practice) np.random.seed(1) # initialize weights randomly (but only &gt;0 because ReLU clips otherwise) w = np.random.random((3,1)) alpha = 0.3 # learning rate new_loss_history = [] # keep a record of how the error proceeded for iter in range(1000): # forward propagation Y_pred = relu(np.dot(X,w)) # how much did we miss? diff = Y_pred - Y new_loss_history.append((diff**2).mean()) # add to the record of the loss # update weights w -= alpha * np.dot( X.T, diff*relu(Y_pred, deriv=True)) print(&quot;Output After Training:&quot;) print(&quot;Y_pred = (should be two 0&#39;s followed by two 1&#39;s) n&quot;,Y_pred) print(&quot;weights = n&quot;,w) . Output After Training: Y_pred = (should be two 0&#39;s followed by two 1&#39;s) [[-0.] [-0.] [ 1.] [ 1.]] weights = [[ 1.01784368e+00] [ 8.53961786e-17] [-1.78436793e-02]] . print( w[2] - (1-w[0]) ) . [-3.46944695e-17] . Plot old results with new results: . %matplotlib inline import matplotlib.pyplot as plt plt.loglog(loss_history,label=&quot;sigmoid&quot;) plt.loglog(new_loss_history,label=&quot;relu&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.legend() plt.show() . Looks like ReLU may be a better choice than sigmoid for this problem! . Exercise: Read a 7-segment display . A 7-segment display is used for displaying numerical digits 0 through 9, usually by lighting up LEDs or parts of a liquid crystal display (LCD). The segments are labelled $a$ through $g$ according to the following diagram: . . Diagram of the network . The 7 inputs &quot;a&quot; through &quot;g&quot; will be mapped to 10 outputs for the individual digits, and each output can range from 0 (&quot;false&quot; or &quot;no&quot;) to 1 (&quot;true&quot; or &quot;yes&quot;) for that digit. The input and outputs will be connected by a matrix of weights. Pictorially, this looks like the following (Not shown: activation function $f$): . . ...where again, this network operates on a single data point at a time, datapoints which are rows of X and Y. What is shown in the above diagram are the columns of $X$ and $Y$ for a single row (/ single data point). . Create the dataset . Let the input X be the segments $a$ through $g$ are the columns of the input $X$, and are either 1 for on or 0 for off. Let the columns of the target $Y$ be the digits 0-9 themselves arranged in a &quot;one hot&quot; encoding scheme, as follows: . Digit | One-Hot Encoding for $Y$ | . 0 | 1,0,0,0,0,0,0,0,0,0 | . 1 | 0,1,0,0,0,0,0,0,0,0 | . 2 | 0,0,1,0,0,0,0,0,0,0 | . ... | ... | . 9 | 0,0,0,0,0,0,0,0,0,1 | . The values in the columns for $Y$ are essentially true/false &quot;bits&quot; for each digit, answering the question &quot;Is this digit the appropriate output?&quot; with a &quot;yes&quot;(=1) or &quot;no&quot; (=0) response. . The input $X$ will be a 10x7 matrix, and the target $Y$ will be a 10x10 matrix. Each row of $X$ will be the segments to produce the digit for that row. For example, the zeroth row of $X$ should show segments on which make an image of the digit zero, namely segments a, b, c, d, e, and f but not g, so that the zeroth row of X should be [1,1,1,1,1,1,0]. . Define numpy arrays for both $X$ and $Y$ (Hint: for $Y$, check out np.eye()): . # for the 7-segment display. The following is just a &quot;stub&quot; to get you started. X = np.array([ [1,1,1,1,1,1,0], [], [], [] ]) Y = np.array([ [1,0,0,0,0,0,0,0,0,0], [], [] ]) . Initialize the weights . Previously the dimensions of the weight matrix $w$ were 3x1 because we were mapping each row of 3 elements in $X$ to each row of 1 element of $Y$. For this new problem, each row of $X$ has 7 elements, and we want to map those to the 10 elements in each 1-hot-encoded row of $Y$, so what should the dimensions of the weights matrix $w$ be? . Write some numpy code to randomly initialize the weights matrix: . np.random.seed(1) # initial RNG so everybody gets similar results w = np.random.random(( , )) # Students, fill in the array dimensions here . File &#34;&lt;ipython-input-7-20f53a51cded&gt;&#34;, line 1 w = np.random.random(( , )) ^ SyntaxError: invalid syntax . Train the network . Having created an $X$ and its matching $Y$, and initalized the weights $w$ randomly, train a neural network such as the ones above to learn to map a row of X to a row of Y, i.e. train it to recognize digits on 7-segment displays. Do this below. . # Use sigmoid activation, and 1000 iterations, and learning rate of 0.9 # Question: What happens if you use ReLU instead? Try it later. Is ReLU always the best choice? # And then print out your Y_pred &amp; weights matrix, and limit it to 3 significant digits print(&quot;Output After Training:&quot;) np.set_printoptions(formatter={&#39;float&#39;: lambda x: &quot;{0:0.3f}&quot;.format(x)}) # 3 sig figs print(&quot;Y_pred= n&quot;,Y_pred) print(&quot;weights = n&quot;,repr(w)) # the repr() makes it so it can be copied &amp; pasted back into Python code . Final Check: Keras version . Keras is a neural network library that lets us write NN applications very compactly. Try running the following using the X and Y from your 7-segment dataset: . import keras from keras.models import Sequential from keras.layers import Dense, Activation model = Sequential([ Dense(10, input_shape=(7,)), Activation(&#39;sigmoid&#39;) ]) model.compile(optimizer=&#39;adam&#39;, # We&#39;ll talk about optimizer choices and loss choices later loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.fit(X, Y, epochs=200, batch_size=1) print(&quot; nY_tilde = n&quot;, model.predict(X) ) . Follow-up: Remarks . Re-stating what we just did . The original problem (posed at the top of this notebook) involves mapping some points from a 3-dimensional space into points in a 1-dimensional space, i.e. to points on the number line. The mapping is done by the combination of a weighted sum (a linear operation) and a nonlinear &quot;activation function&quot; applied to that sum. The use of an activation function like a sigmoid was originally intended to serve as an analogy of activation of biological neurons. Nonlinear activation functions are source of the &quot;power&quot; of neural networks (essentially we approximate some other function by means of a sum of basis functions in some function space, but don&#39;t worry about that if you&#39;re not math-inclined). The algorithm &#39;learns&#39; to approximate this operation via supervised learning and gradient descent according to some loss function. We used the mean squared error (MSE) for our loss, but lots and lots of different loss functions could be used, a few of which we&#39;ll look at another time. . Question for reflection: Unlike fitting a line $y = mx+b$, the weighted sum in our models in this notebook had no constant &quot;bias&quot; term like $b$. How might we include such a term? . One thing we glossed over: &quot;batch size&quot; . Question: Should we apply the gradient descent &quot;update&quot; to the weights each time we process a single row of $X$ &amp; $Y$, or should we compute the combined loss of all the rows together at the same time, and then do the update? This is essentially asking the same question as &quot;When fitting a line $mx+b$ to a bunch of data points, should we use all the points together to update $m$ and $b,$ or should we do this one point at a time -- compute the gradients of the loss at one point, update the weights, compute gradients at another point, etc.?&quot; . The number of points you use is called the batch size and it is what&#39;s known as a &quot;hyperparameter&quot; -- it is not part of the model per se, but it is a(n important) choice you make when training the model. The batch size affects the learning as follows: Averaging the gradints for many data points (i..e. a large batch size) will produce a smoother loss function and will also usually make the code execute more quickly through the dataset, but updating the weights for every point will cause the algorithm to learn with fewer iterations. . One quick way to observe this is to go up to the Keras code above and change batch_size from 1 to 10, and re-execute the cell. How is the accuracy after 200 iteractions, compared to when batch_size=1? . Terminology: Technically, it&#39;s called &quot;batch training&quot; when you sum the gradients for all the data points before updating the weights, whereas using fewer points is &quot;minibatch training&quot;, and updating for each point (i.e. each row, for us) is Stochastic Gradient Descent* (SGD -- more on these terms here). In practice, there is a tradeoff between smaller vs. larger (mini)batches, which has been the subject of intense scrutiny by researchers over the years. We will have more to say on this later. . For discussion later: In our presentation above, were we using batch training, minibatch training or SGD? . . . *Note: many people will regard SGD as an optimization algorithm per se, and refer to doing SGD even for (mini)batch sizes larger than 1. . Optional: If you want to go really crazy . How about training on this dataset: $$ overbrace{ left[ { begin{array}{cc} 0 &amp; 0 0 &amp; 1 1 &amp; 0 1 &amp; 1 end{array} } right] }^{X} rightarrow overbrace{ left[ { begin{array}{c} 0 1 1 0 end{array} } right] }^Y. $$ Good luck! ;-) (Hint 1: This problem features prominently in the history of Neural Networks, involving Marvin Minsky and &quot;AI Winter.&quot; Hint 2: This whole lesson could instead be entitled &quot;My First Artificial Neuron.&quot;) . Additional Optional Exercise: Binary Math vs. One-Hot Encoding . For the 7-segment display, we used a one-hot encoding for our output, namely a set of true/false &quot;bits&quot; for each digit. One may wonder how effective this ouput-encoding method is, compared to a different bit-setting encoding method, namely binary representations. . Construct the target output matrix $Y$ for binary representations of the numbers 0 through 9. Your target matrix should have 10 rows and 4 columns (i.e, output bits for 1s, 2s, 4s, and 8s). | Using this $Y$ array, train the network as before, and plot the loss as a function of iteration. | Question: Which method works &#39;better&#39;? One-hot encoding or binary encoding? .",
            "url": "https://dnlam.github.io/fastblog/2022/03/14/_The_Beauty_Of_Neural_Network.html",
            "relUrl": "/2022/03/14/_The_Beauty_Of_Neural_Network.html",
            "date": " • Mar 14, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Assignment 2 - Implement your agent",
            "content": "Packages . numpy : Fundamental package for scientific computing with Python. | matplotlib : Library for plotting graphs in Python. | RL-Glue, BaseEnvironment, BaseAgent : Library and abstract classes to inherit from for reinforcement learning experiments. | LunarLanderEnvironment : An RLGlue environment that wraps a LundarLander environment implementation from OpenAI Gym. | collections.deque: a double-ended queue implementation. We use deque to implement the experience replay buffer. | copy.deepcopy: As objects are not passed by value in python, we often need to make copies of mutable objects. copy.deepcopy allows us to make a new object with the same contents as another object. (Take a look at this link if you are interested to learn more: https://robertheaton.com/2014/02/09/pythons-pass-by-object-reference-as-explained-by-philip-k-dick/) | tqdm : A package to display progress bar when running experiments | os: Package used to interface with the operating system. Here we use it for creating a results folder when it does not exist. | shutil: Package used to operate on files and folders. Here we use it for creating a zip file of the results folder. | plot_script: Used for plotting learning curves using matplotlib. | . # Import necessary libraries # DO NOT IMPORT OTHER LIBRARIES - This will break the autograder. import numpy as np import matplotlib.pyplot as plt %matplotlib inline from rl_glue import RLGlue from environment import BaseEnvironment from lunar_lander import LunarLanderEnvironment from agent import BaseAgent from collections import deque from copy import deepcopy from tqdm import tqdm import os import shutil from plot_script import plot_result . Section 1: Action-Value Network . This section includes the function approximator that we use in our agent, a neural network. In Course 3 Assignment 2, we used a neural network as the function approximator for a policy evaluation problem. In this assignment, we will use a neural network for approximating the action-value function in a control problem. The main difference between approximating a state-value function and an action-value function using a neural network is that in the former the output layer only includes one unit whereas in the latter the output layer includes as many units as the number of actions. . In the cell below, you will specify the architecture of the action-value neural network. More specifically, you will specify self.layer_sizes in the __init__() function. . We have already provided get_action_values() and get_TD_update() methods. The former computes the action-value function by doing a forward pass and the latter computes the gradient of the action-value function with respect to the weights times the TD error. These get_action_values() and get_TD_update() methods are similar to the get_value() and get_gradient() methods that you implemented in Course 3 Assignment 2. The main difference is that in this notebook, they are designed to be applied to batches of states instead of one state. You will later use these functions for implementing the agent. . # Graded Cell # -- # Work Required: Yes. Fill in the code for layer_sizes in __init__ (~1 Line). # Also go through the rest of the code to ensure your understanding is correct. class ActionValueNetwork: # Work Required: Yes. Fill in the layer_sizes member variable (~1 Line). def __init__(self, network_config): self.state_dim = network_config.get(&quot;state_dim&quot;) self.num_hidden_units = network_config.get(&quot;num_hidden_units&quot;) self.num_actions = network_config.get(&quot;num_actions&quot;) self.rand_generator = np.random.RandomState(network_config.get(&quot;seed&quot;)) # Specify self.layer_sizes which shows the number of nodes in each layer # your code here self.layer_sizes = [self.state_dim,self.num_hidden_units, self.num_actions] # Initialize the weights of the neural network # self.weights is an array of dictionaries with each dictionary corresponding to # the weights from one layer to the next. Each dictionary includes W and b self.weights = [dict() for i in range(0, len(self.layer_sizes) - 1)] for i in range(0, len(self.layer_sizes) - 1): self.weights[i][&#39;W&#39;] = self.init_saxe(self.layer_sizes[i], self.layer_sizes[i + 1]) self.weights[i][&#39;b&#39;] = np.zeros((1, self.layer_sizes[i + 1])) # Work Required: No. def get_action_values(self, s): &quot;&quot;&quot; Args: s (Numpy array): The state. Returns: The action-values (Numpy array) calculated using the network&#39;s weights. &quot;&quot;&quot; W0, b0 = self.weights[0][&#39;W&#39;], self.weights[0][&#39;b&#39;] psi = np.dot(s, W0) + b0 x = np.maximum(psi, 0) W1, b1 = self.weights[1][&#39;W&#39;], self.weights[1][&#39;b&#39;] q_vals = np.dot(x, W1) + b1 return q_vals # Work Required: No. def get_TD_update(self, s, delta_mat): &quot;&quot;&quot; Args: s (Numpy array): The state. delta_mat (Numpy array): A 2D array of shape (batch_size, num_actions). Each row of delta_mat correspond to one state in the batch. Each row has only one non-zero element which is the TD-error corresponding to the action taken. Returns: The TD update (Array of dictionaries with gradient times TD errors) for the network&#39;s weights &quot;&quot;&quot; W0, b0 = self.weights[0][&#39;W&#39;], self.weights[0][&#39;b&#39;] W1, b1 = self.weights[1][&#39;W&#39;], self.weights[1][&#39;b&#39;] psi = np.dot(s, W0) + b0 x = np.maximum(psi, 0) dx = (psi &gt; 0).astype(float) # td_update has the same structure as self.weights, that is an array of dictionaries. # td_update[0][&quot;W&quot;], td_update[0][&quot;b&quot;], td_update[1][&quot;W&quot;], and td_update[1][&quot;b&quot;] have the same shape as # self.weights[0][&quot;W&quot;], self.weights[0][&quot;b&quot;], self.weights[1][&quot;W&quot;], and self.weights[1][&quot;b&quot;] respectively td_update = [dict() for i in range(len(self.weights))] v = delta_mat td_update[1][&#39;W&#39;] = np.dot(x.T, v) * 1. / s.shape[0] td_update[1][&#39;b&#39;] = np.sum(v, axis=0, keepdims=True) * 1. / s.shape[0] v = np.dot(v, W1.T) * dx td_update[0][&#39;W&#39;] = np.dot(s.T, v) * 1. / s.shape[0] td_update[0][&#39;b&#39;] = np.sum(v, axis=0, keepdims=True) * 1. / s.shape[0] return td_update # Work Required: No. You may wish to read the relevant paper for more information on this weight initialization # (Exact solutions to the nonlinear dynamics of learning in deep linear neural networks by Saxe, A et al., 2013) def init_saxe(self, rows, cols): &quot;&quot;&quot; Args: rows (int): number of input units for layer. cols (int): number of output units for layer. Returns: NumPy Array consisting of weights for the layer based on the initialization in Saxe et al. &quot;&quot;&quot; tensor = self.rand_generator.normal(0, 1, (rows, cols)) if rows &lt; cols: tensor = tensor.T tensor, r = np.linalg.qr(tensor) d = np.diag(r, 0) ph = np.sign(d) tensor *= ph if rows &lt; cols: tensor = tensor.T return tensor # Work Required: No. def get_weights(self): &quot;&quot;&quot; Returns: A copy of the current weights of this network. &quot;&quot;&quot; return deepcopy(self.weights) # Work Required: No. def set_weights(self, weights): &quot;&quot;&quot; Args: weights (list of dictionaries): Consists of weights that this network will set as its own weights. &quot;&quot;&quot; self.weights = deepcopy(weights) . Run the cell below to test your implementation of the __init__() function for ActionValueNetwork: . # Debugging Cell # -- # Feel free to make any changes to this cell to debug your code network_config = { &quot;state_dim&quot;: 5, &quot;num_hidden_units&quot;: 20, &quot;num_actions&quot;: 3 } test_network = ActionValueNetwork(network_config) print(&quot;layer_sizes:&quot;, test_network.layer_sizes) assert(np.allclose(test_network.layer_sizes, np.array([5, 20, 3]))) . layer_sizes: [5, 20, 3] . # Tested Cell # -- # The contents of the cell will be tested by the autograder. # If they do not pass here, they will not pass there. rand_generator = np.random.RandomState(0) for _ in range(1000): network_config = { &quot;state_dim&quot;: rand_generator.randint(2, 10), &quot;num_hidden_units&quot;: rand_generator.randint(2, 1024), &quot;num_actions&quot;: rand_generator.randint(2, 10) } test_network = ActionValueNetwork(network_config) assert(np.allclose(test_network.layer_sizes, np.array([network_config[&quot;state_dim&quot;], network_config[&quot;num_hidden_units&quot;], network_config[&quot;num_actions&quot;]]))) . Expected output: (assuming no changes to the debugging cell) . layer_sizes: [ 5 20 3] . Section 2: Adam Optimizer . In this assignment, you will use the Adam algorithm for updating the weights of your action-value network. As you may remember from Course 3 Assignment 2, the Adam algorithm is a more advanced variant of stochastic gradient descent (SGD). The Adam algorithm improves the SGD update with two concepts: adaptive vector stepsizes and momentum. It keeps running estimates of the mean and second moment of the updates, denoted by $ mathbf{m}$ and $ mathbf{v}$ respectively: $$ mathbf{m_t} = beta_m mathbf{m_{t-1}} + (1 - beta_m)g_t mathbf{v_t} = beta_v mathbf{v_{t-1}} + (1 - beta_v)g^2_t $$ . Here, $ beta_m$ and $ beta_v$ are fixed parameters controlling the linear combinations above and $g_t$ is the update at time $t$ (generally the gradients, but here the TD error times the gradients). . Given that $ mathbf{m}$ and $ mathbf{v}$ are initialized to zero, they are biased toward zero. To get unbiased estimates of the mean and second moment, Adam defines $ mathbf{ hat{m}}$ and $ mathbf{ hat{v}}$ as: $$ mathbf{ hat{m}_t} = frac{ mathbf{m_t}}{1 - beta_m^t} mathbf{ hat{v}_t} = frac{ mathbf{v_t}}{1 - beta_v^t} $$ . The weights are then updated as follows: $$ mathbf{w_t} = mathbf{w_{t-1}} + frac{ alpha}{ sqrt{ mathbf{ hat{v}_t}}+ epsilon} mathbf{ hat{m}_t} $$ . Here, $ alpha$ is the step size parameter and $ epsilon$ is another small parameter to keep the denominator from being zero. . In the cell below, you will implement the __init__() and update_weights() methods for the Adam algorithm. In __init__(), you will initialize self.m and self.v. In update_weights(), you will compute new weights given the input weights and an update $g$ (here td_errors_times_gradients) according to the equations above. . class Adam(): # Work Required: Yes. Fill in the initialization for self.m and self.v (~4 Lines). def __init__(self, layer_sizes, optimizer_info): self.layer_sizes = layer_sizes # Specify Adam algorithm&#39;s hyper parameters self.step_size = optimizer_info.get(&quot;step_size&quot;) self.beta_m = optimizer_info.get(&quot;beta_m&quot;) self.beta_v = optimizer_info.get(&quot;beta_v&quot;) self.epsilon = optimizer_info.get(&quot;epsilon&quot;) # Initialize Adam algorithm&#39;s m and v self.m = [dict() for i in range(1, len(self.layer_sizes))] self.v = [dict() for i in range(1, len(self.layer_sizes))] for i in range(0, len(self.layer_sizes) - 1): # Hint: The initialization for m and v should look very much like the initializations of the weights # except for the fact that initialization here is to zeroes (see description above.) # Replace the None in each following line self.m[i][&quot;W&quot;] = np.zeros((self.layer_sizes[i],self.layer_sizes[i+1])) self.m[i][&quot;b&quot;] = np.zeros((1,self.layer_sizes[i+1])) self.v[i][&quot;W&quot;] = np.zeros((self.layer_sizes[i],self.layer_sizes[i+1])) self.v[i][&quot;b&quot;] = np.zeros((1,self.layer_sizes[i+1])) # your code here # Notice that to calculate m_hat and v_hat, we use powers of beta_m and beta_v to # the time step t. We can calculate these powers using an incremental product. At initialization then, # beta_m_product and beta_v_product should be ...? (Note that timesteps start at 1 and if we were to # start from 0, the denominator would be 0.) self.beta_m_product = self.beta_m self.beta_v_product = self.beta_v # Work Required: Yes. Fill in the weight updates (~5-7 lines). def update_weights(self, weights, td_errors_times_gradients): &quot;&quot;&quot; Args: weights (Array of dictionaries): The weights of the neural network. td_errors_times_gradients (Array of dictionaries): The gradient of the action-values with respect to the network&#39;s weights times the TD-error Returns: The updated weights (Array of dictionaries). &quot;&quot;&quot; for i in range(len(weights)): for param in weights[i].keys(): # Hint: Follow the equations above. First, you should update m and v and then compute # m_hat and v_hat. Finally, compute how much the weights should be incremented by. # self.m[i][param] = None # self.v[i][param] = None # m_hat = None # v_hat = None self.m[i][param] = self.beta_m * self.m[i][param] + (1-self.beta_m)*td_errors_times_gradients[i][param] self.v[i][param] = self.beta_v * self.v[i][param] + (1-self.beta_v)*(td_errors_times_gradients[i][param] * td_errors_times_gradients[i][param]) m_hat = self.m[i][param]/(1 - self.beta_m_product) v_hat = self.v[i][param]/(1 - self.beta_v_product) weight_update = (self.step_size * m_hat) / (np.sqrt(v_hat) + self.epsilon) # your code here weights[i][param] = weights[i][param] + weight_update # Notice that to calculate m_hat and v_hat, we use powers of beta_m and beta_v to ### update self.beta_m_product and self.beta_v_product self.beta_m_product *= self.beta_m self.beta_v_product *= self.beta_v return weights . Run the following code to test your implementation of the __init__() function: . # Debugging Cell # -- # Feel free to make any changes to this cell to debug your code network_config = {&quot;state_dim&quot;: 5, &quot;num_hidden_units&quot;: 2, &quot;num_actions&quot;: 3 } optimizer_info = {&quot;step_size&quot;: 0.1, &quot;beta_m&quot;: 0.99, &quot;beta_v&quot;: 0.999, &quot;epsilon&quot;: 0.0001 } network = ActionValueNetwork(network_config) test_adam = Adam(network.layer_sizes, optimizer_info) print(&quot;m[0][ &quot;W &quot;] shape: {}&quot;.format(test_adam.m[0][&quot;W&quot;].shape)) print(&quot;m[0][ &quot;b &quot;] shape: {}&quot;.format(test_adam.m[0][&quot;b&quot;].shape)) print(&quot;m[1][ &quot;W &quot;] shape: {}&quot;.format(test_adam.m[1][&quot;W&quot;].shape)) print(&quot;m[1][ &quot;b &quot;] shape: {}&quot;.format(test_adam.m[1][&quot;b&quot;].shape), &quot; n&quot;) assert(np.allclose(test_adam.m[0][&quot;W&quot;].shape, np.array([5, 2]))) assert(np.allclose(test_adam.m[0][&quot;b&quot;].shape, np.array([1, 2]))) assert(np.allclose(test_adam.m[1][&quot;W&quot;].shape, np.array([2, 3]))) assert(np.allclose(test_adam.m[1][&quot;b&quot;].shape, np.array([1, 3]))) print(&quot;v[0][ &quot;W &quot;] shape: {}&quot;.format(test_adam.v[0][&quot;W&quot;].shape)) print(&quot;v[0][ &quot;b &quot;] shape: {}&quot;.format(test_adam.v[0][&quot;b&quot;].shape)) print(&quot;v[1][ &quot;W &quot;] shape: {}&quot;.format(test_adam.v[1][&quot;W&quot;].shape)) print(&quot;v[1][ &quot;b &quot;] shape: {}&quot;.format(test_adam.v[1][&quot;b&quot;].shape), &quot; n&quot;) assert(np.allclose(test_adam.v[0][&quot;W&quot;].shape, np.array([5, 2]))) assert(np.allclose(test_adam.v[0][&quot;b&quot;].shape, np.array([1, 2]))) assert(np.allclose(test_adam.v[1][&quot;W&quot;].shape, np.array([2, 3]))) assert(np.allclose(test_adam.v[1][&quot;b&quot;].shape, np.array([1, 3]))) assert(np.all(test_adam.m[0][&quot;W&quot;]==0)) assert(np.all(test_adam.m[0][&quot;b&quot;]==0)) assert(np.all(test_adam.m[1][&quot;W&quot;]==0)) assert(np.all(test_adam.m[1][&quot;b&quot;]==0)) assert(np.all(test_adam.v[0][&quot;W&quot;]==0)) assert(np.all(test_adam.v[0][&quot;b&quot;]==0)) assert(np.all(test_adam.v[1][&quot;W&quot;]==0)) assert(np.all(test_adam.v[1][&quot;b&quot;]==0)) . m[0][&#34;W&#34;] shape: (5, 2) m[0][&#34;b&#34;] shape: (1, 2) m[1][&#34;W&#34;] shape: (2, 3) m[1][&#34;b&#34;] shape: (1, 3) v[0][&#34;W&#34;] shape: (5, 2) v[0][&#34;b&#34;] shape: (1, 2) v[1][&#34;W&#34;] shape: (2, 3) v[1][&#34;b&#34;] shape: (1, 3) . # Tested Cell # -- # The contents of the cell will be tested by the autograder. # If they do not pass here, they will not pass there. # import our implementation of Adam # while you can go look at this for the answer, try to solve the programming challenge yourself first from tests import TrueAdam rand_generator = np.random.RandomState(0) for _ in range(1000): network_config = { &quot;state_dim&quot;: rand_generator.randint(2, 10), &quot;num_hidden_units&quot;: rand_generator.randint(2, 1024), &quot;num_actions&quot;: rand_generator.randint(2, 10) } optimizer_info = {&quot;step_size&quot;: rand_generator.choice(np.geomspace(0.1, 1e-5, num=5)), &quot;beta_m&quot;: rand_generator.choice([0.9, 0.99, 0.999, 0.9999, 0.99999]), &quot;beta_v&quot;: rand_generator.choice([0.9, 0.99, 0.999, 0.9999, 0.99999]), &quot;epsilon&quot;: rand_generator.choice(np.geomspace(0.1, 1e-5, num=5)) } test_network = ActionValueNetwork(network_config) test_adam = Adam(test_network.layer_sizes, optimizer_info) true_adam = TrueAdam(test_network.layer_sizes, optimizer_info) assert(np.allclose(test_adam.m[0][&quot;W&quot;].shape, true_adam.m[0][&quot;W&quot;].shape)) assert(np.allclose(test_adam.m[0][&quot;b&quot;].shape, true_adam.m[0][&quot;b&quot;].shape)) assert(np.allclose(test_adam.m[1][&quot;W&quot;].shape, true_adam.m[1][&quot;W&quot;].shape)) assert(np.allclose(test_adam.m[1][&quot;b&quot;].shape, true_adam.m[1][&quot;b&quot;].shape)) assert(np.allclose(test_adam.v[0][&quot;W&quot;].shape, true_adam.v[0][&quot;W&quot;].shape)) assert(np.allclose(test_adam.v[0][&quot;b&quot;].shape, true_adam.v[0][&quot;b&quot;].shape)) assert(np.allclose(test_adam.v[1][&quot;W&quot;].shape, true_adam.v[1][&quot;W&quot;].shape)) assert(np.allclose(test_adam.v[1][&quot;b&quot;].shape, true_adam.v[1][&quot;b&quot;].shape)) assert(np.all(test_adam.m[0][&quot;W&quot;]==0)) assert(np.all(test_adam.m[0][&quot;b&quot;]==0)) assert(np.all(test_adam.m[1][&quot;W&quot;]==0)) assert(np.all(test_adam.m[1][&quot;b&quot;]==0)) assert(np.all(test_adam.v[0][&quot;W&quot;]==0)) assert(np.all(test_adam.v[0][&quot;b&quot;]==0)) assert(np.all(test_adam.v[1][&quot;W&quot;]==0)) assert(np.all(test_adam.v[1][&quot;b&quot;]==0)) assert(test_adam.beta_m_product == optimizer_info[&quot;beta_m&quot;]) assert(test_adam.beta_v_product == optimizer_info[&quot;beta_v&quot;]) . Expected output: . m[0][&quot;W&quot;] shape: (5, 2) m[0][&quot;b&quot;] shape: (1, 2) m[1][&quot;W&quot;] shape: (2, 3) m[1][&quot;b&quot;] shape: (1, 3) v[0][&quot;W&quot;] shape: (5, 2) v[0][&quot;b&quot;] shape: (1, 2) v[1][&quot;W&quot;] shape: (2, 3) v[1][&quot;b&quot;] shape: (1, 3) . Section 3: Experience Replay Buffers . In Course 3, you implemented agents that update value functions once for each sample. We can use a more efficient approach for updating value functions. You have seen an example of an efficient approach in Course 2 when implementing Dyna. The idea behind Dyna is to learn a model using sampled experience, obtain simulated experience from the model, and improve the value function using the simulated experience. . Experience replay is a simple method that can get some of the advantages of Dyna by saving a buffer of experience and using the data stored in the buffer as a model. This view of prior data as a model works because the data represents actual transitions from the underlying MDP. Furthermore, as a side note, this kind of model that is not learned and simply a collection of experience can be called non-parametric as it can be ever-growing as opposed to a parametric model where the transitions are learned to be represented with a fixed set of parameters or weights. . We have provided the implementation of the experience replay buffer in the cell below. ReplayBuffer includes two main functions: append() and sample(). append() adds an experience transition to the buffer as an array that includes the state, action, reward, terminal flag (indicating termination of the episode), and next_state. sample() gets a batch of experiences from the buffer with size minibatch_size. . You will use the append() and sample() functions when implementing the agent. . # Discussion Cell # class ReplayBuffer: def __init__(self, size, minibatch_size, seed): &quot;&quot;&quot; Args: size (integer): The size of the replay buffer. minibatch_size (integer): The sample size. seed (integer): The seed for the random number generator. &quot;&quot;&quot; self.buffer = [] self.minibatch_size = minibatch_size self.rand_generator = np.random.RandomState(seed) self.max_size = size def append(self, state, action, reward, terminal, next_state): &quot;&quot;&quot; Args: state (Numpy array): The state. action (integer): The action. reward (float): The reward. terminal (integer): 1 if the next state is a terminal state and 0 otherwise. next_state (Numpy array): The next state. &quot;&quot;&quot; if len(self.buffer) == self.max_size: del self.buffer[0] self.buffer.append([state, action, reward, terminal, next_state]) def sample(self): &quot;&quot;&quot; Returns: A list of transition tuples including state, action, reward, terinal, and next_state &quot;&quot;&quot; idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size) return [self.buffer[idx] for idx in idxs] def size(self): return len(self.buffer) . Section 4: Softmax Policy . In this assignment, you will use a softmax policy. One advantage of a softmax policy is that it explores according to the action-values, meaning that an action with a moderate value has a higher chance of getting selected compared to an action with a lower value. Contrast this with an $ epsilon$-greedy policy which does not consider the individual action values when choosing an exploratory action in a state and instead chooses randomly when doing so. . The probability of selecting each action according to the softmax policy is shown below: $$Pr{(A_t=a | S_t=s)} hspace{0.1cm} dot{=} hspace{0.1cm} frac{e^{Q(s, a)/ tau}}{ sum_{b in A}e^{Q(s, b)/ tau}}$$ where $ tau$ is the temperature parameter which controls how much the agent focuses on the highest valued actions. The smaller the temperature, the more the agent selects the greedy action. Conversely, when the temperature is high, the agent selects among actions more uniformly random. . Given that a softmax policy exponentiates action values, if those values are large, exponentiating them could get very large. To implement the softmax policy in a numerically stable way, we often subtract the maximum action-value from the action-values. If we do so, the probability of selecting each action looks as follows: . $$Pr{(A_t=a | S_t=s)} hspace{0.1cm} dot{=} hspace{0.1cm} frac{e^{Q(s, a)/ tau - max_{c}Q(s, c)/ tau}}{ sum_{b in A}e^{Q(s, b)/ tau - max_{c}Q(s, c)/ tau}}$$ . In the cell below, you will implement the softmax() function. In order to do so, you could break the above computation into smaller steps: . compute the preference, $H(a)$, for taking each action by dividing the action-values by the temperature parameter $ tau$, | subtract the maximum preference across the actions from the preferences to avoid overflow, and, | compute the probability of taking each action. | . # Graded Cell # -- def softmax(action_values, tau=1.0): &quot;&quot;&quot; Args: action_values (Numpy array): A 2D array of shape (batch_size, num_actions). The action-values computed by an action-value network. tau (float): The temperature parameter scalar. Returns: A 2D array of shape (batch_size, num_actions). Where each column is a probability distribution over the actions representing the policy. &quot;&quot;&quot; # Compute the preferences by dividing the action-values by the temperature parameter tau preferences = action_values / tau # Compute the maximum preference across the actions max_preference = np.max(preferences,axis = 1) # your code here # Reshape max_preference array which has shape [Batch,] to [Batch, 1]. This allows NumPy broadcasting # when subtracting the maximum preference from the preference of each action. reshaped_max_preference = max_preference.reshape((-1, 1)) # Compute the numerator, i.e., the exponential of the preference - the max preference. exp_preferences = np.exp(preferences - reshaped_max_preference) # Compute the denominator, i.e., the sum over the numerator along the actions axis. sum_of_exp_preferences = np.sum(exp_preferences,axis = 1) # your code here # Reshape sum_of_exp_preferences array which has shape [Batch,] to [Batch, 1] to allow for NumPy broadcasting # when dividing the numerator by the denominator. reshaped_sum_of_exp_preferences = sum_of_exp_preferences.reshape((-1, 1)) # Compute the action probabilities according to the equation in the previous cell. action_probs = exp_preferences / reshaped_sum_of_exp_preferences # your code here # squeeze() removes any singleton dimensions. It is used here because this function is used in the # agent policy when selecting an action (for which the batch dimension is 1.) As np.random.choice is used in # the agent policy and it expects 1D arrays, we need to remove this singleton batch dimension. action_probs = action_probs.squeeze() return action_probs . Run the cell below to test your implementation of the softmax() function: . # Debugging Cell # -- # Feel free to make any changes to this cell to debug your code rand_generator = np.random.RandomState(0) action_values = rand_generator.normal(0, 1, (2, 4)) tau = 0.5 action_probs = softmax(action_values, tau) print(&quot;action_probs&quot;, action_probs) assert(np.allclose(action_probs, np.array([ [0.25849645, 0.01689625, 0.05374514, 0.67086216], [0.84699852, 0.00286345, 0.13520063, 0.01493741] ]))) print(&quot;Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)&quot;) . action_probs [[0.25849645 0.01689625 0.05374514 0.67086216] [0.84699852 0.00286345 0.13520063 0.01493741]] Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.) . # Tested Cell # -- # The contents of the cell will be tested by the autograder. # If they do not pass here, they will not pass there. from tests import __true__softmax rand_generator = np.random.RandomState(0) for _ in range(1000): action_values = rand_generator.normal(0, 1, (rand_generator.randint(1, 5), 4)) tau = rand_generator.rand() assert(np.allclose(softmax(action_values, tau), __true__softmax(action_values, tau))) . Expected output: . action_probs [[0.25849645 0.01689625 0.05374514 0.67086216] [0.84699852 0.00286345 0.13520063 0.01493741]] . Section 5: Putting the pieces together . In this section, you will combine components from the previous sections to write up an RL-Glue Agent. The main component that you will implement is the action-value network updates with experience sampled from the experience replay buffer. . At time $t$, we have an action-value function represented as a neural network, say $Q_t$. We want to update our action-value function and get a new one we can use at the next timestep. We will get this $Q_{t+1}$ using multiple replay steps that each result in an intermediate action-value function $Q_{t+1}^{i}$ where $i$ indexes which replay step we are at. . In each replay step, we sample a batch of experiences from the replay buffer and compute a minibatch Expected-SARSA update. Across these N replay steps, we will use the current &quot;un-updated&quot; action-value network at time $t$, $Q_t$, for computing the action-values of the next-states. This contrasts using the most recent action-values from the last replay step $Q_{t+1}^{i}$. We make this choice to have targets that are stable across replay steps. Here is the pseudocode for performing the updates: . $$ begin{align} &amp; Q_t leftarrow text{action-value network at timestep t (current action-value network)} &amp; text{Initialize } Q_{t+1}^1 leftarrow Q_t &amp; text{For } i text{ in } [1, ..., N] text{ (i.e. N} text{ replay steps)}: &amp; hspace{1cm} s, a, r, t, s&#39; leftarrow text{Sample batch of experiences from experience replay buffer} &amp; hspace{1cm} text{Do Expected Sarsa update with } Q_t: Q_{t+1}^{i+1}(s, a) leftarrow Q_{t+1}^{i}(s, a) + alpha cdot left[r + gamma left( sum_{b} pi(b | s&#39;) Q_t(s&#39;, b) right) - Q_{t+1}^{i}(s, a) right] &amp; hspace{1.5cm} text{ making sure to add the } gamma left( sum_{b} pi(b | s&#39;) Q_t(s&#39;, b) right) text{ for non-terminal transitions only.} &amp; text{After N replay steps, we set } Q_{t+1}^{N} text{ as } Q_{t+1} text{ and have a new } Q_{t+1} text{for time step } t + 1 text{ that we will fix in the next set of updates. } end{align} $$As you can see in the pseudocode, after sampling a batch of experiences, we do many computations. The basic idea however is that we are looking to compute a form of a TD error. In order to so, we can take the following steps: . compute the action-values for the next states using the action-value network $Q_{t}$, | compute the policy $ pi(b | s&#39;)$ induced by the action-values $Q_{t}$ (using the softmax function you implemented before), | compute the Expected sarsa targets $r + gamma left( sum_{b} pi(b | s&#39;) Q_t(s&#39;, b) right)$, | compute the action-values for the current states using the latest $Q_{t + 1}$, and, | compute the TD-errors with the Expected Sarsa targets. | . For the third step above, you can start by computing $ pi(b | s&#39;) Q_t(s&#39;, b)$ followed by summation to get $ hat{v}_ pi(s&#39;) = left( sum_{b} pi(b | s&#39;) Q_t(s&#39;, b) right)$. $ hat{v}_ pi(s&#39;)$ is an estimate of the value of the next state. Note for terminal next states, $ hat{v}_ pi(s&#39;) = 0$. Finally, we add the rewards to the discount times $ hat{v}_ pi(s&#39;)$. . You will implement these steps in the get_td_error() function below which given a batch of experiences (including states, next_states, actions, rewards, terminals), fixed action-value network (current_q), and action-value network (network), computes the TD error in the form of a 1D array of size batch_size. . def get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau): &quot;&quot;&quot; Args: states (Numpy array): The batch of states with the shape (batch_size, state_dim). next_states (Numpy array): The batch of next states with the shape (batch_size, state_dim). actions (Numpy array): The batch of actions with the shape (batch_size,). rewards (Numpy array): The batch of rewards with the shape (batch_size,). discount (float): The discount factor. terminals (Numpy array): The batch of terminals with the shape (batch_size,). network (ActionValueNetwork): The latest state of the network that is getting replay updates. current_q (ActionValueNetwork): The fixed network used for computing the targets, and particularly, the action-values at the next-states. Returns: The TD errors (Numpy array) for actions taken, of shape (batch_size,) &quot;&quot;&quot; # Note: Here network is the latest state of the network that is getting replay updates. In other words, # the network represents Q_{t+1}^{i} whereas current_q represents Q_t, the fixed network used for computing the # targets, and particularly, the action-values at the next-states. # Compute action values at next states using current_q network # Note that q_next_mat is a 2D array of shape (batch_size, num_actions) ### START CODE HERE (~1 Line) q_next_mat = current_q.get_action_values(next_states) ### END CODE HERE # your code here # Compute policy at next state by passing the action-values in q_next_mat to softmax() # Note that probs_mat is a 2D array of shape (batch_size, num_actions) ### START CODE HERE (~1 Line) probs_mat = softmax(q_next_mat,tau) ### END CODE HERE # your code here # Compute the estimate of the next state value, v_next_vec. # Hint: sum the action-values for the next_states weighted by the policy, probs_mat. Then, multiply by # (1 - terminals) to make sure v_next_vec is zero for terminal next states. # Note that v_next_vec is a 1D array of shape (batch_size,) ### START CODE HERE (~3 Lines) v_next_vec = np.sum(q_next_mat * probs_mat , axis = 1) * (1-terminals) ### END CODE HERE # your code here # Compute Expected Sarsa target # Note that target_vec is a 1D array of shape (batch_size,) ### START CODE HERE (~1 Line) target_vec = rewards + discount * v_next_vec ### END CODE HERE # your code here # Compute action values at the current states for all actions using network # Note that q_mat is a 2D array of shape (batch_size, num_actions) ### START CODE HERE (~1 Line) q_mat = network.get_action_values(states) ### END CODE HERE # your code here # Batch Indices is an array from 0 to the batch size - 1. batch_indices = np.arange(q_mat.shape[0]) # Compute q_vec by selecting q(s, a) from q_mat for taken actions # Use batch_indices as the index for the first dimension of q_mat # Note that q_vec is a 1D array of shape (batch_size) ### START CODE HERE (~1 Line) q_vec = q_mat[batch_indices,actions] ### END CODE HERE # your code here # Compute TD errors for actions taken # Note that delta_vec is a 1D array of shape (batch_size) ### START CODE HERE (~1 Line) delta_vec = target_vec - q_vec ### END CODE HERE # your code here return delta_vec . Run the following code to test your implementation of the get_td_error() function: . # Debugging Cell # -- # Feel free to make any changes to this cell to debug your code data = np.load(&quot;asserts/get_td_error_1.npz&quot;, allow_pickle=True) states = data[&quot;states&quot;] next_states = data[&quot;next_states&quot;] actions = data[&quot;actions&quot;] rewards = data[&quot;rewards&quot;] discount = data[&quot;discount&quot;] terminals = data[&quot;terminals&quot;] tau = 0.001 network_config = {&quot;state_dim&quot;: 8, &quot;num_hidden_units&quot;: 512, &quot;num_actions&quot;: 4 } network = ActionValueNetwork(network_config) network.set_weights(data[&quot;network_weights&quot;]) current_q = ActionValueNetwork(network_config) current_q.set_weights(data[&quot;current_q_weights&quot;]) delta_vec = get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau) answer_delta_vec = data[&quot;delta_vec&quot;] assert(np.allclose(delta_vec, answer_delta_vec)) print(&quot;Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)&quot;) . Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.) . # Tested Cell # -- # The contents of the cell will be tested by the autograder. # If they do not pass here, they will not pass there. data = np.load(&quot;asserts/get_td_error_1.npz&quot;, allow_pickle=True) states = data[&quot;states&quot;] next_states = data[&quot;next_states&quot;] actions = data[&quot;actions&quot;] rewards = data[&quot;rewards&quot;] discount = data[&quot;discount&quot;] terminals = data[&quot;terminals&quot;] tau = 0.001 network_config = {&quot;state_dim&quot;: 8, &quot;num_hidden_units&quot;: 512, &quot;num_actions&quot;: 4 } network = ActionValueNetwork(network_config) network.set_weights(data[&quot;network_weights&quot;]) current_q = ActionValueNetwork(network_config) current_q.set_weights(data[&quot;current_q_weights&quot;]) delta_vec = get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau) answer_delta_vec = data[&quot;delta_vec&quot;] assert(np.allclose(delta_vec, answer_delta_vec)) . Now that you implemented the get_td_error() function, you can use it to implement the optimize_network() function. In this function, you will: . get the TD-errors vector from get_td_error(), | make the TD-errors into a matrix using zeroes for actions not taken in the transitions, | pass the TD-errors matrix to the get_TD_update() function of network to calculate the gradients times TD errors, and, | perform an ADAM optimizer step. | . # Graded Cell # -- ### Work Required: Yes. Fill in code in optimize_network (~2 Lines). def optimize_network(experiences, discount, optimizer, network, current_q, tau): &quot;&quot;&quot; Args: experiences (Numpy array): The batch of experiences including the states, actions, rewards, terminals, and next_states. discount (float): The discount factor. network (ActionValueNetwork): The latest state of the network that is getting replay updates. current_q (ActionValueNetwork): The fixed network used for computing the targets, and particularly, the action-values at the next-states. &quot;&quot;&quot; # Get states, action, rewards, terminals, and next_states from experiences states, actions, rewards, terminals, next_states = map(list, zip(*experiences)) states = np.concatenate(states) next_states = np.concatenate(next_states) rewards = np.array(rewards) terminals = np.array(terminals) batch_size = states.shape[0] # Compute TD error using the get_td_error function # Note that q_vec is a 1D array of shape (batch_size) delta_vec = get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau) # Batch Indices is an array from 0 to the batch_size - 1. batch_indices = np.arange(batch_size) # Make a td error matrix of shape (batch_size, num_actions) # delta_mat has non-zero value only for actions taken delta_mat = np.zeros((batch_size, network.num_actions)) delta_mat[batch_indices, actions] = delta_vec # Pass delta_mat to compute the TD errors times the gradients of the network&#39;s weights from back-propagation ### START CODE HERE td_update = network.get_TD_update(states,delta_mat) ### END CODE HERE # your code here # Pass network.get_weights and the td_update to the optimizer to get updated weights ### START CODE HERE weights = optimizer.update_weights(network.get_weights(), td_update) ### END CODE HERE # your code here network.set_weights(weights) . Run the following code to test your implementation of the optimize_network() function: . # Tested Cell # -- # The contents of the cell will be tested by the autograder. # If they do not pass here, they will not pass there. input_data = np.load(&quot;asserts/optimize_network_input_1.npz&quot;, allow_pickle=True) experiences = list(input_data[&quot;experiences&quot;]) discount = input_data[&quot;discount&quot;] tau = 0.001 network_config = {&quot;state_dim&quot;: 8, &quot;num_hidden_units&quot;: 512, &quot;num_actions&quot;: 4 } network = ActionValueNetwork(network_config) network.set_weights(input_data[&quot;network_weights&quot;]) current_q = ActionValueNetwork(network_config) current_q.set_weights(input_data[&quot;current_q_weights&quot;]) optimizer_config = {&#39;step_size&#39;: 3e-5, &#39;beta_m&#39;: 0.9, &#39;beta_v&#39;: 0.999, &#39;epsilon&#39;: 1e-8 } optimizer = Adam(network.layer_sizes, optimizer_config) optimizer.m = input_data[&quot;optimizer_m&quot;] optimizer.v = input_data[&quot;optimizer_v&quot;] optimizer.beta_m_product = input_data[&quot;optimizer_beta_m_product&quot;] optimizer.beta_v_product = input_data[&quot;optimizer_beta_v_product&quot;] optimize_network(experiences, discount, optimizer, network, current_q, tau) updated_weights = network.get_weights() output_data = np.load(&quot;asserts/optimize_network_output_1.npz&quot;, allow_pickle=True) answer_updated_weights = output_data[&quot;updated_weights&quot;] assert(np.allclose(updated_weights[0][&quot;W&quot;], answer_updated_weights[0][&quot;W&quot;])) assert(np.allclose(updated_weights[0][&quot;b&quot;], answer_updated_weights[0][&quot;b&quot;])) assert(np.allclose(updated_weights[1][&quot;W&quot;], answer_updated_weights[1][&quot;W&quot;])) assert(np.allclose(updated_weights[1][&quot;b&quot;], answer_updated_weights[1][&quot;b&quot;])) . Now that you implemented the optimize_network() function, you can implement the agent. In the cell below, you will fill the agent_step() and agent_end() functions. You should: . select an action (only in agent_step()), | add transitions (consisting of the state, action, reward, terminal, and next state) to the replay buffer, and, | update the weights of the neural network by doing multiple replay steps and calling the optimize_network() function that you implemented above. | . # Graded Cell # -- ### Work Required: Yes. Fill in code in agent_step and agent_end (~7 Lines). class Agent(BaseAgent): def __init__(self): self.name = &quot;expected_sarsa_agent&quot; # Work Required: No. def agent_init(self, agent_config): &quot;&quot;&quot;Setup for the agent called when the experiment first starts. Set parameters needed to setup the agent. Assume agent_config dict contains: { network_config: dictionary, optimizer_config: dictionary, replay_buffer_size: integer, minibatch_sz: integer, num_replay_updates_per_step: float discount_factor: float, } &quot;&quot;&quot; self.replay_buffer = ReplayBuffer(agent_config[&#39;replay_buffer_size&#39;], agent_config[&#39;minibatch_sz&#39;], agent_config.get(&quot;seed&quot;)) self.network = ActionValueNetwork(agent_config[&#39;network_config&#39;]) self.optimizer = Adam(self.network.layer_sizes, agent_config[&quot;optimizer_config&quot;]) self.num_actions = agent_config[&#39;network_config&#39;][&#39;num_actions&#39;] self.num_replay = agent_config[&#39;num_replay_updates_per_step&#39;] self.discount = agent_config[&#39;gamma&#39;] self.tau = agent_config[&#39;tau&#39;] self.rand_generator = np.random.RandomState(agent_config.get(&quot;seed&quot;)) self.last_state = None self.last_action = None self.sum_rewards = 0 self.episode_steps = 0 # Work Required: No. def policy(self, state): &quot;&quot;&quot; Args: state (Numpy array): the state. Returns: the action. &quot;&quot;&quot; action_values = self.network.get_action_values(state) probs_batch = softmax(action_values, self.tau) action = self.rand_generator.choice(self.num_actions, p=probs_batch.squeeze()) return action # Work Required: No. def agent_start(self, state): &quot;&quot;&quot;The first method called when the experiment starts, called after the environment starts. Args: state (Numpy array): the state from the environment&#39;s evn_start function. Returns: The first action the agent takes. &quot;&quot;&quot; self.sum_rewards = 0 self.episode_steps = 0 self.last_state = np.array([state]) self.last_action = self.policy(self.last_state) return self.last_action # Work Required: Yes. Fill in the action selection, replay-buffer update, # weights update using optimize_network, and updating last_state and last_action (~5 lines). def agent_step(self, reward, state): &quot;&quot;&quot;A step taken by the agent. Args: reward (float): the reward received for taking the last action taken state (Numpy array): the state from the environment&#39;s step based, where the agent ended up after the last step Returns: The action the agent is taking. &quot;&quot;&quot; self.sum_rewards += reward self.episode_steps += 1 # Make state an array of shape (1, state_dim) to add a batch dimension and # to later match the get_action_values() and get_TD_update() functions state = np.array([state]) # Select action # your code here action = self.policy(state) # Append new experience to replay buffer # Note: look at the replay_buffer append function for the order of arguments # your code here self.replay_buffer.append(self.last_state,self.last_action,reward,0,state) # Perform replay steps: if self.replay_buffer.size() &gt; self.replay_buffer.minibatch_size: current_q = deepcopy(self.network) for _ in range(self.num_replay): # Get sample experiences from the replay buffer experiences = self.replay_buffer.sample() # Call optimize_network to update the weights of the network (~1 Line) # your code here optimize_network(experiences,self.discount,self.optimizer,self.network,current_q,self.tau) # Update the last state and last action. ### START CODE HERE (~2 Lines) self.last_state = state self.last_action = action ### END CODE HERE # your code here return action # Work Required: Yes. Fill in the replay-buffer update and # update of the weights using optimize_network (~2 lines). def agent_end(self, reward): &quot;&quot;&quot;Run when the agent terminates. Args: reward (float): the reward the agent received for entering the terminal state. &quot;&quot;&quot; self.sum_rewards += reward self.episode_steps += 1 # Set terminal state to an array of zeros state = np.zeros_like(self.last_state) # Append new experience to replay buffer # Note: look at the replay_buffer append function for the order of arguments # your code here self.replay_buffer.append(self.last_state,self.last_action,reward,1,state) # Perform replay steps: if self.replay_buffer.size() &gt; self.replay_buffer.minibatch_size: current_q = deepcopy(self.network) for _ in range(self.num_replay): # Get sample experiences from the replay buffer experiences = self.replay_buffer.sample() # Call optimize_network to update the weights of the network # your code here optimize_network(experiences,self.discount,self.optimizer,self.network,current_q,self.tau) def agent_message(self, message): if message == &quot;get_sum_reward&quot;: return self.sum_rewards else: raise Exception(&quot;Unrecognized Message!&quot;) . Run the following code to test your implementation of the agent_step() function: . # Tested Cell # -- # The contents of the cell will be tested by the autograder. # If they do not pass here, they will not pass there. agent_info = { &#39;network_config&#39;: { &#39;state_dim&#39;: 8, &#39;num_hidden_units&#39;: 256, &#39;num_hidden_layers&#39;: 1, &#39;num_actions&#39;: 4 }, &#39;optimizer_config&#39;: { &#39;step_size&#39;: 3e-5, &#39;beta_m&#39;: 0.9, &#39;beta_v&#39;: 0.999, &#39;epsilon&#39;: 1e-8 }, &#39;replay_buffer_size&#39;: 32, &#39;minibatch_sz&#39;: 32, &#39;num_replay_updates_per_step&#39;: 4, &#39;gamma&#39;: 0.99, &#39;tau&#39;: 1000.0, &#39;seed&#39;: 0} # Initialize agent agent = Agent() agent.agent_init(agent_info) # load agent network, optimizer, replay_buffer from the agent_input_1.npz file input_data = np.load(&quot;asserts/agent_input_1.npz&quot;, allow_pickle=True) agent.network.set_weights(input_data[&quot;network_weights&quot;]) agent.optimizer.m = input_data[&quot;optimizer_m&quot;] agent.optimizer.v = input_data[&quot;optimizer_v&quot;] agent.optimizer.beta_m_product = input_data[&quot;optimizer_beta_m_product&quot;] agent.optimizer.beta_v_product = input_data[&quot;optimizer_beta_v_product&quot;] agent.replay_buffer.rand_generator.seed(int(input_data[&quot;replay_buffer_seed&quot;])) for experience in input_data[&quot;replay_buffer&quot;]: agent.replay_buffer.buffer.append(experience) # Perform agent_step multiple times last_state_array = input_data[&quot;last_state_array&quot;] last_action_array = input_data[&quot;last_action_array&quot;] state_array = input_data[&quot;state_array&quot;] reward_array = input_data[&quot;reward_array&quot;] for i in range(5): agent.last_state = last_state_array[i] agent.last_action = last_action_array[i] state = state_array[i] reward = reward_array[i] agent.agent_step(reward, state) # Load expected values for last_state, last_action, weights, and replay_buffer output_data = np.load(&quot;asserts/agent_step_output_{}.npz&quot;.format(i), allow_pickle=True) answer_last_state = output_data[&quot;last_state&quot;] answer_last_action = output_data[&quot;last_action&quot;] answer_updated_weights = output_data[&quot;updated_weights&quot;] answer_replay_buffer = output_data[&quot;replay_buffer&quot;] # Asserts for last_state and last_action assert(np.allclose(answer_last_state, agent.last_state)) assert(np.allclose(answer_last_action, agent.last_action)) # Asserts for replay_buffer for i in range(answer_replay_buffer.shape[0]): for j in range(answer_replay_buffer.shape[1]): assert(np.allclose(np.asarray(agent.replay_buffer.buffer)[i, j], answer_replay_buffer[i, j])) # Asserts for network.weights assert(np.allclose(agent.network.weights[0][&quot;W&quot;], answer_updated_weights[0][&quot;W&quot;])) assert(np.allclose(agent.network.weights[0][&quot;b&quot;], answer_updated_weights[0][&quot;b&quot;])) assert(np.allclose(agent.network.weights[1][&quot;W&quot;], answer_updated_weights[1][&quot;W&quot;])) assert(np.allclose(agent.network.weights[1][&quot;b&quot;], answer_updated_weights[1][&quot;b&quot;])) . Run the following code to test your implementation of the agent_end() function: . # Tested Cell # -- # The contents of the cell will be tested by the autograder. # If they do not pass here, they will not pass there. agent_info = { &#39;network_config&#39;: { &#39;state_dim&#39;: 8, &#39;num_hidden_units&#39;: 256, &#39;num_hidden_layers&#39;: 1, &#39;num_actions&#39;: 4 }, &#39;optimizer_config&#39;: { &#39;step_size&#39;: 3e-5, &#39;beta_m&#39;: 0.9, &#39;beta_v&#39;: 0.999, &#39;epsilon&#39;: 1e-8 }, &#39;replay_buffer_size&#39;: 32, &#39;minibatch_sz&#39;: 32, &#39;num_replay_updates_per_step&#39;: 4, &#39;gamma&#39;: 0.99, &#39;tau&#39;: 1000, &#39;seed&#39;: 0 } # Initialize agent agent = Agent() agent.agent_init(agent_info) # load agent network, optimizer, replay_buffer from the agent_input_1.npz file input_data = np.load(&quot;asserts/agent_input_1.npz&quot;, allow_pickle=True) agent.network.set_weights(input_data[&quot;network_weights&quot;]) agent.optimizer.m = input_data[&quot;optimizer_m&quot;] agent.optimizer.v = input_data[&quot;optimizer_v&quot;] agent.optimizer.beta_m_product = input_data[&quot;optimizer_beta_m_product&quot;] agent.optimizer.beta_v_product = input_data[&quot;optimizer_beta_v_product&quot;] agent.replay_buffer.rand_generator.seed(int(input_data[&quot;replay_buffer_seed&quot;])) for experience in input_data[&quot;replay_buffer&quot;]: agent.replay_buffer.buffer.append(experience) # Perform agent_step multiple times last_state_array = input_data[&quot;last_state_array&quot;] last_action_array = input_data[&quot;last_action_array&quot;] state_array = input_data[&quot;state_array&quot;] reward_array = input_data[&quot;reward_array&quot;] for i in range(5): agent.last_state = last_state_array[i] agent.last_action = last_action_array[i] reward = reward_array[i] agent.agent_end(reward) # Load expected values for last_state, last_action, weights, and replay_buffer output_data = np.load(&quot;asserts/agent_end_output_{}.npz&quot;.format(i), allow_pickle=True) answer_updated_weights = output_data[&quot;updated_weights&quot;] answer_replay_buffer = output_data[&quot;replay_buffer&quot;] # Asserts for replay_buffer for i in range(answer_replay_buffer.shape[0]): for j in range(answer_replay_buffer.shape[1]): assert(np.allclose(np.asarray(agent.replay_buffer.buffer)[i, j], answer_replay_buffer[i, j])) # Asserts for network.weights assert(np.allclose(agent.network.weights[0][&quot;W&quot;], answer_updated_weights[0][&quot;W&quot;])) assert(np.allclose(agent.network.weights[0][&quot;b&quot;], answer_updated_weights[0][&quot;b&quot;])) assert(np.allclose(agent.network.weights[1][&quot;W&quot;], answer_updated_weights[1][&quot;W&quot;])) assert(np.allclose(agent.network.weights[1][&quot;b&quot;], answer_updated_weights[1][&quot;b&quot;])) . Section 6: Run Experiment . Now that you implemented the agent, we can use it to run an experiment on the Lunar Lander problem. We will plot the learning curve of the agent to visualize learning progress. To plot the learning curve, we use the sum of rewards in an episode as the performance measure. We have provided for you the experiment/plot code in the cell below which you can go ahead and run. Note that running the cell below has taken approximately 10 minutes in prior testing. . # Discussion Cell # def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters): rl_glue = RLGlue(environment, agent) # save sum of reward at the end of each episode agent_sum_reward = np.zeros((experiment_parameters[&quot;num_runs&quot;], experiment_parameters[&quot;num_episodes&quot;])) env_info = {} agent_info = agent_parameters # one agent setting for run in range(1, experiment_parameters[&quot;num_runs&quot;]+1): agent_info[&quot;seed&quot;] = run agent_info[&quot;network_config&quot;][&quot;seed&quot;] = run env_info[&quot;seed&quot;] = run rl_glue.rl_init(agent_info, env_info) for episode in tqdm(range(1, experiment_parameters[&quot;num_episodes&quot;]+1)): # run episode rl_glue.rl_episode(experiment_parameters[&quot;timeout&quot;]) episode_reward = rl_glue.rl_agent_message(&quot;get_sum_reward&quot;) agent_sum_reward[run - 1, episode - 1] = episode_reward save_name = &quot;{}&quot;.format(rl_glue.agent.name) if not os.path.exists(&#39;results&#39;): os.makedirs(&#39;results&#39;) np.save(&quot;results/sum_reward_{}&quot;.format(save_name), agent_sum_reward) shutil.make_archive(&#39;results&#39;, &#39;zip&#39;, &#39;results&#39;) # Run Experiment # Experiment parameters experiment_parameters = { &quot;num_runs&quot; : 1, &quot;num_episodes&quot; : 300, # OpenAI Gym environments allow for a timestep limit timeout, causing episodes to end after # some number of timesteps. Here we use the default of 500. &quot;timeout&quot; : 500 } # Environment parameters environment_parameters = {} current_env = LunarLanderEnvironment # Agent parameters agent_parameters = { &#39;network_config&#39;: { &#39;state_dim&#39;: 8, &#39;num_hidden_units&#39;: 256, &#39;num_actions&#39;: 4 }, &#39;optimizer_config&#39;: { &#39;step_size&#39;: 1e-3, &#39;beta_m&#39;: 0.9, &#39;beta_v&#39;: 0.999, &#39;epsilon&#39;: 1e-8 }, &#39;replay_buffer_size&#39;: 50000, &#39;minibatch_sz&#39;: 8, &#39;num_replay_updates_per_step&#39;: 4, &#39;gamma&#39;: 0.99, &#39;tau&#39;: 0.001 } current_agent = Agent # run experiment run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters) . 100%|██████████| 300/300 [07:05&lt;00:00, 1.42s/it] . Run the cell below to see the comparison between the agent that you implemented and a random agent for the one run and 300 episodes. Note that the plot_result() function smoothes the learning curve by applying a sliding window on the performance measure. . plot_result([&quot;expected_sarsa_agent&quot;, &quot;random_agent&quot;]) . In the following cell you can visualize the performance of the agent with a correct implementation. As you can see, the agent initially crashes quite quickly (Episode 0). Then, the agent learns to avoid crashing by expending fuel and staying far above the ground. Finally however, it learns to land smoothly within the landing zone demarcated by the two flags (Episode 275). . %%HTML &lt;div align=&quot;middle&quot;&gt; &lt;video width=&quot;80%&quot; controls&gt; &lt;source src=&quot;ImplementYourAgent.mp4&quot; type=&quot;video/mp4&quot;&gt; &lt;/video&gt;&lt;/div&gt; . In the learning curve above, you can see that sum of reward over episode has quite a high-variance at the beginning. However, the performance seems to be improving. The experiment that you ran was for 300 episodes and 1 run. To understand how the agent performs in the long run, we provide below the learning curve for the agent trained for 3000 episodes with performance averaged over 30 runs. You can see that the agent learns a reasonably good policy within 3000 episodes, gaining sum of reward bigger than 200. Note that because of the high-variance in the agent performance, we also smoothed the learning curve. . Wrapping up! . You have successfully implemented Course 4 Programming Assignment 2. . You have implemented an Expected Sarsa agent with a neural network and the Adam optimizer and used it for solving the Lunar Lander problem! You implemented different components of the agent including: . a neural network for function approximation, | the Adam algorithm for optimizing the weights of the neural network, | a Softmax policy, | the replay steps for updating the action-value function using the experiences sampled from a replay buffer | . You tested the agent for a single parameter setting. In the next assignment, you will perform a parameter study on the step-size parameter to gain insight about the effect of step-size on the performance of your agent. .",
            "url": "https://dnlam.github.io/fastblog/2022/03/14/_Lunar_Landing_Expected_SARSA.html",
            "relUrl": "/2022/03/14/_Lunar_Landing_Expected_SARSA.html",
            "date": " • Mar 14, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://dnlam.github.io/fastblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there, I am Lam DINH (Ngoc-Lam DINH), I graduated from Ha Noi University of Science and Technology (Vietnam) in 2016 with a degree in Electronics and Telecommunications Engineering. My training mainly focused on several areas: Digital Signal Processing, Wireless Communication and Embedded Programming. Then, I continued my study with a special interest in Signal Theory, Wireless Telecommunications and Optical Networks at Universidad Politecnica de Valencia (Spain) in 2017. . My Master’s degree is jointly awarded by the École Normale Supérieure Paris Saclay (France) and the Universidad Complutense de Madrid (Spain) in 2019. The courses are related to the application of molecular photonics to telecommunications and biosensors. . From 2019, I am conducting a PhD thesis at the Commissariat à l’Énergie Atomique (CEA) in Grenoble. My research addresses ultra-reliable and low-latency communications (URLLC) in 5G systems and beyond. . More Information . Personal Web Page .",
          "url": "https://dnlam.github.io/fastblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://dnlam.github.io/fastblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
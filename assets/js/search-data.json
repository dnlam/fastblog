{
  
    
        "post0": {
            "title": "Image Classification with FastAI",
            "content": "Objectives . So far, from Part 1, we understood how to create and deploy a model. I practice, to make your model really works, there are a lots of details that we have to check including: . different types of layers | regularization methods | optimizers | how to put layers into architectures. | labelling techniques and much more | . In this post, we will enlighten them on! . Dogs, Cats and Pet Breeds . from fastai.vision.all import * path=untar_data(URLs.PETS) path.ls() (path/&quot;images&quot;).ls() . (#7393) [Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/Siamese_87.jpg&#39;),Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/chihuahua_126.jpg&#39;),Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/german_shorthaired_97.jpg&#39;),Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/Bombay_157.jpg&#39;),Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/Bengal_12.jpg&#39;),Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/japanese_chin_116.jpg&#39;),Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/havanese_109.jpg&#39;),Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/scottish_terrier_122.jpg&#39;),Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_146.jpg&#39;),Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/boxer_176.jpg&#39;)...] . In order to extract information from strings of dataset, we can use regular expression (regex). A regular expression (link) is a special string, written in regular expression language and specifies a general rule for deciding whether another string passes a test. As the example given below, we will take a file name from scratch and then we use regex to grap all the parts of regular expression that have parentheses around them. . fname=(path/&quot;images&quot;).ls()[1] re.findall(r&#39;(.+)_ d+.jpg$&#39;,fname.name) . [&#39;chihuahua&#39;] . In the next part, we will give an example of using regex to label the whole dataset by RegexLabeller. get_y will take RegexLabellerfunction and changes it to a function which will be passed the &#39;name&#39; attribute. . Then, 2 last lines resize and aug_transforms() do image augmentation. . pets = DataBlock(blocks=(ImageBlock,CategoryBlock), # independant and dependant variable get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;),&#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224,min_scale=0.75)) dls=pets.dataloaders(path/&quot;images&quot;) . /home/tmpext4/nd258645/conda-env/lib/python3.8/site-packages/torch/_tensor.py:1023: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release. torch.linalg.solve has its arguments reversed and does not return the LU factorization. To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack. X = torch.solve(B, A).solution should be replaced with X = torch.linalg.solve(A, B) (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:760.) ret = func(*args, **kwargs) . Presizing . Presizing will grap a square randomly in the original picture. Then the second step of aug_transform will grap a random warped crop (possibly rotated) and will turn that into a square. . Because these steps will change the images (lower quality) since it requires an interpolation after each step, so FastAI (Resize()) will coordinate the image transformation in a non lossy way. And only once at the end, we will do the interpolation. . . learn=cnn_learner(dls,resnet34,metrics=error_rate) learn.fine_tune(2) . /home/tmpext4/nd258645/conda-env/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . epoch train_loss valid_loss error_rate time . 0 | 1.483577 | 0.336734 | 0.116373 | 01:17 | . epoch train_loss valid_loss error_rate time . 0 | 0.523443 | 0.271582 | 0.094723 | 01:12 | . 1 | 0.313463 | 0.219852 | 0.077131 | 01:15 | . Cross-entropy loss . Cross-entropy loss is really much the same as MNIST loss we have defined before but it provides at least 2 benefits: . It works even when our dependant variable has more than 2 categories | Faster and more reliable training. | . The purpose of the Cross-Entropy loss is to take the output probabilities and measure the distance from the truth table. By means of model training, we will minimize the Cross-Entropy loss. . View the activation function and labels . Let&#39;s look at the mini-batch (64) of our training model . x,y=dls.one_batch() y . [&#39;Abyssinian&#39;, &#39;Bengal&#39;, &#39;Birman&#39;, &#39;Bombay&#39;, &#39;British_Shorthair&#39;, &#39;Egyptian_Mau&#39;, &#39;Maine_Coon&#39;, &#39;Persian&#39;, &#39;Ragdoll&#39;, &#39;Russian_Blue&#39;, &#39;Siamese&#39;, &#39;Sphynx&#39;, &#39;american_bulldog&#39;, &#39;american_pit_bull_terrier&#39;, &#39;basset_hound&#39;, &#39;beagle&#39;, &#39;boxer&#39;, &#39;chihuahua&#39;, &#39;english_cocker_spaniel&#39;, &#39;english_setter&#39;, &#39;german_shorthaired&#39;, &#39;great_pyrenees&#39;, &#39;havanese&#39;, &#39;japanese_chin&#39;, &#39;keeshond&#39;, &#39;leonberger&#39;, &#39;miniature_pinscher&#39;, &#39;newfoundland&#39;, &#39;pomeranian&#39;, &#39;pug&#39;, &#39;saint_bernard&#39;, &#39;samoyed&#39;, &#39;scottish_terrier&#39;, &#39;shiba_inu&#39;, &#39;staffordshire_bull_terrier&#39;, &#39;wheaten_terrier&#39;, &#39;yorkshire_terrier&#39;] . dls.vocab . [&#39;Abyssinian&#39;, &#39;Bengal&#39;, &#39;Birman&#39;, &#39;Bombay&#39;, &#39;British_Shorthair&#39;, &#39;Egyptian_Mau&#39;, &#39;Maine_Coon&#39;, &#39;Persian&#39;, &#39;Ragdoll&#39;, &#39;Russian_Blue&#39;, &#39;Siamese&#39;, &#39;Sphynx&#39;, &#39;american_bulldog&#39;, &#39;american_pit_bull_terrier&#39;, &#39;basset_hound&#39;, &#39;beagle&#39;, &#39;boxer&#39;, &#39;chihuahua&#39;, &#39;english_cocker_spaniel&#39;, &#39;english_setter&#39;, &#39;german_shorthaired&#39;, &#39;great_pyrenees&#39;, &#39;havanese&#39;, &#39;japanese_chin&#39;, &#39;keeshond&#39;, &#39;leonberger&#39;, &#39;miniature_pinscher&#39;, &#39;newfoundland&#39;, &#39;pomeranian&#39;, &#39;pug&#39;, &#39;saint_bernard&#39;, &#39;samoyed&#39;, &#39;scottish_terrier&#39;, &#39;shiba_inu&#39;, &#39;staffordshire_bull_terrier&#39;, &#39;wheaten_terrier&#39;, &#39;yorkshire_terrier&#39;] . Then, we can show the predictions (the activation results of final layer of our neural network) of one mini-batch . preds,_ = learn.get_preds(dl=[(x,y)]) preds[0] . tensor([1.1638e-05, 2.7098e-07, 7.4142e-05, 7.0873e-07, 4.7944e-07, 3.0185e-08, 1.1611e-05, 6.6417e-06, 1.0301e-05, 4.4232e-08, 1.4760e-07, 8.9016e-08, 5.7057e-09, 7.4013e-08, 5.1872e-07, 3.6632e-06, 4.5897e-07, 7.8231e-06, 5.5353e-07, 2.9222e-08, 2.3144e-08, 1.1901e-07, 1.5109e-06, 4.9122e-06, 4.9140e-05, 1.6393e-06, 4.1687e-07, 1.9792e-07, 9.9980e-01, 1.4814e-07, 4.8275e-08, 8.0303e-07, 3.9702e-07, 1.0734e-05, 6.0649e-08, 8.1256e-07, 1.1867e-06]) . The results return 37 probabilities between 0 and 1, which add up to 1 in total. To transform the activation of our model into predictions like this, we used soft-max activation function . Soft-max . Soft-max activation function is an extension of Sigmoid function to handle more than 2 categories. So we can obtain multi activations for multi label categories. The output of each final layer shows the likelyhood of the input item being a particular category. . As indicated in the example below, the unnormalized outputs of the neural network will be converted into probability by using Softmax fucntion. It measures how likely in terms of probability an input item belongs to a particular category. . . def softmax(x): return exp(x) / exp(x).sum(dim=1,keepdim=True) . When we apply Sigmoid activation function for each individual final layer, we can not guarantee that the sum of those outputs will be added up to 1. That&#39;s the reason for why we apply Softmax where it calculates the exponential of each outcome to the sum of exponential of all possible outcomes. . acts=torch.randn((6,2))*2 sm_acts=torch.softmax(acts,dim=1) sm_acts . tensor([[0.0734, 0.9266], [0.2011, 0.7989], [0.8459, 0.1541], [0.9867, 0.0133], [0.9817, 0.0183], [0.0025, 0.9975]]) . Because the exponential function grows very fast, so softmax activation function really want to pick one class among the others, so it will be ideal for training a classifier when we have various categories. . Entropy . The concept of entropy was proposed by Shannon in the field of information theory. By definition, Entropy of a random variable X measures the level of uncertainty ingerent in the variables possible outcomes. . For p(x) - probability distribution and a random variable X, entropy H(x) is defined as follows: . $$ H(X)= left { begin{matrix} - int_{x} p(x)* log{p(x)} ,&amp; text{if X is continuous} - sum_{x} p(x)* log{p(x)} , &amp; text{if X is discrete} end{matrix} right. $$The negative sign is used to deal with the logarithm of a value in range between 0 and 1. Thus, the greater value of entropy H(x) (events have comparable probabilities), the greater of uncertainty for probability distribution and vice versa. . In the context of Machine Learning, the comparison between predicted distribution and true distribution provides us the information about the differences between those. The larger gap between those distributions, the more uncertainty of our model will be. That is what the cross-entropy loss determine: . $$ L_{CE} = - sum _{i=1} ^{n} t_i * log {p_i}, quad text{for n classes} $$where $t_i$ is the truth label and $p_i$ is the Softmax probability of the $i^{th}$ class. . In Pytorch, cross-entropy loss are available in either class instance or function instance. By default, Pytorch loss function takes the mean of the loss of all categories, so we can use reduction=&#39;none&#39; to explicitly show the individual loss. . targ = tensor([0,1,0,0,1,1]) loss_func = nn.CrossEntropyLoss() loss_func(acts,targ) . tensor(1.1703) . targ = tensor([0,1,0,0,1,1]) F.cross_entropy(acts,targ) . tensor(1.1703) . nn.CrossEntropyLoss(reduction=&#39;none&#39;)(acts,targ) . tensor([2.6122e+00, 2.2455e-01, 1.6730e-01, 1.3394e-02, 4.0018e+00, 2.5001e-03]) . Model interpretation . In order for human to understand the optimization process, we can use confusion matrix to see where our model is doing well and where its doing badly . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12),dpi=60) . To have a cleaner view of what is going on, we can choose to show the cells of confusion matrix with the most incorrect predictions. . interp.most_confused(min_val=5) . [(&#39;american_pit_bull_terrier&#39;, &#39;staffordshire_bull_terrier&#39;, 10), (&#39;Egyptian_Mau&#39;, &#39;Bengal&#39;, 7)] . Improving our model . In this part, we will study a range of techniques to improve the training of our model and make it better. It includes: . Learning rate finder | Transfer Learning process | . The learning rate finder . One of the most critical part in training a model os to find a good learning rate. If the predefined learning rate is too small, it will takes many epochs to train our model and it might result in time consuming and overfitting exposure. IF it is set too high, the detrimental effects of error rate increasing might be seen. . learn = cnn_learner(dls,resnet34,metrics=error_rate) learn.fine_tune(1,base_lr=0.2) . epoch train_loss valid_loss error_rate time . 0 | 9.363783 | 43.051037 | 0.903924 | 01:27 | . epoch train_loss valid_loss error_rate time . 0 | 7.759617 | 29.350748 | 0.971583 | 01:25 | . In 2015, resercher Leslie Smith came up with a brilliant idea of learning rate finder link. His idea starts with a very small learning rate and examine the loss over one mini-batch. Then, he increase learning rate by a certain percentage and keep doing that until the loss get worse. That the point we know that we have over-reacted and we should choose a learning rate lower than this point. . lr_min,lr_steep = learn.lr_find() . ValueError Traceback (most recent call last) Input In [39], in &lt;cell line: 1&gt;() -&gt; 1 lr_min,lr_steep = learn.lr_find() ValueError: not enough values to unpack (expected 2, got 1) . learn = cnn_learner(dls,resnet34,metrics=error_rate) learn.fine_tune(2,base_lr=0.008) . epoch train_loss valid_loss error_rate time . 0 | 1.014067 | 0.404690 | 0.130582 | 01:29 | . epoch train_loss valid_loss error_rate time . 0 | 0.865397 | 0.785443 | 0.220568 | 01:28 | . 1 | 0.480247 | 0.299356 | 0.093369 | 01:24 | . Unfreezing and transfer learning . Transfer learning will take a set of parameters that have been previously trained and throw away the last layer of pretrained model. Then we replace by a layer with random weights and we train that. . The next task is to fine tune the newly added weight to align with our new objective. fine_tun is a method we called to operate that. It does 2 things: . train the randomly added layers for one epoch with all other layer frozen | unfreeze all of the layers and train them all for the number of epoch requested. | . learn.fine_tune?? . There are several parameters in fine_tune we shoud notice: . self.freeze(): make only the last layer&#39;s weight get step and freeze the other layers. | self.fit_one_cycle() : update the newly added weights in one cycle, it trains model without using fine_tune. In summary, it starts training at a lower learning rate, gradually increase ot for the first section of training and then gradually decrease it again for the last section of training. | . learn = cnn_learner(dls,resnet34,metrics=error_rate) learn.fit_one_cycle(3,3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.131244 | 0.339623 | 0.106225 | 01:30 | . 1 | 0.533571 | 0.272031 | 0.083897 | 01:26 | . 2 | 0.332284 | 0.235257 | 0.075101 | 01:26 | . learn.unfreeze() learn.lr_find() . SuggestedLRs(valley=6.918309736647643e-06) . learn.fit_one_cycle(6,lr_max=1e-5) . epoch train_loss valid_loss error_rate time . 0 | 0.247704 | 0.222702 | 0.071719 | 01:26 | . 1 | 0.245497 | 0.211186 | 0.069689 | 01:26 | . 2 | 0.211546 | 0.210558 | 0.067659 | 01:30 | . 3 | 0.198770 | 0.204503 | 0.063599 | 01:29 | . 4 | 0.190857 | 0.202116 | 0.066306 | 01:27 | . 5 | 0.177307 | 0.202761 | 0.067659 | 01:23 | . Intuitively speaking, in transfer learning, the deepest layers (beginning layers) of our pretrained model might not need as high a learning rate as the last ones, so we should probably use different learning rate approach for different layers. That what FastAI called discriminative learning rate . . Discriminative Learning rate . The idea behind this is simple: we apply lower learning rate for the early layers of the neural network and higher learning rate for the later ones. . In FastAI, we will pass a Python slice object anywhere that a learning rate is expected. The first value of slice is the learning rate of the starting layer and the last value of the slice is the final layer. The layers in between share the multiplicatively equidistant learning rate throughout the slice. . learn = cnn_learner(dls,resnet34,metrics=error_rate) learn.fit_one_cycle(3,3e-3) learn.unfreeze() learn.fit_one_cycle(12,lr_max=slice(1e-6,1e-4)) . epoch train_loss valid_loss error_rate time . 0 | 1.105149 | 0.319647 | 0.104195 | 01:32 | . 1 | 0.509816 | 0.279788 | 0.093369 | 01:28 | . 2 | 0.319550 | 0.241098 | 0.087280 | 01:32 | . epoch train_loss valid_loss error_rate time . 0 | 0.257214 | 0.241996 | 0.083221 | 01:32 | . 1 | 0.256226 | 0.228803 | 0.078484 | 01:31 | . 2 | 0.240209 | 0.225714 | 0.077131 | 01:33 | . 3 | 0.202059 | 0.217118 | 0.071042 | 01:25 | . 4 | 0.179049 | 0.209164 | 0.070365 | 01:25 | . 5 | 0.177175 | 0.212497 | 0.065629 | 01:28 | . 6 | 0.165256 | 0.202155 | 0.067659 | 01:31 | . 7 | 0.150717 | 0.210655 | 0.070365 | 01:33 | . 8 | 0.147985 | 0.206712 | 0.063599 | 01:34 | . 9 | 0.132928 | 0.202588 | 0.066306 | 01:33 | . 10 | 0.136363 | 0.203536 | 0.066306 | 01:24 | . 11 | 0.122196 | 0.202970 | 0.064276 | 01:25 | . Deeper Architecture . It literally increase the numbers of layers of our architecture (more activation functions, linear model). For instance, resnet architecture comes with 18,34,50,101,152 layer variants, pretrained with ImageNet. A larger version of the resnet returns a better training loss, but it can suffer more from overfitting. . In additions, the bigger model and larger batch-size is, the memory requirement for GPU is higher. . Another downside of training data with deeper architecture is time consuming, as it will take more time to train a larger model.µ . Take home messages . So, in this part, we have learned some important practical tips: . Preparing data for modelling (presizing) | Fitting the model (learning rate finder, unfreezing, fine_tune, discriminative learning rate, epochs, deeper architecture) | Entropy loss discusison | .",
            "url": "https://dnlam.github.io/fastblog/2022/03/20/image_classification_fastai.html",
            "relUrl": "/2022/03/20/image_classification_fastai.html",
            "date": " • Mar 20, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Introduction about FastAI",
            "content": "1. What is FastAI? . FasiAI is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and low-level components that can be mixed and matched to build new approaches. . FastAI libraries include: . a dispatch system for Python along with a semantic type hierarchy for tensors | a GPU-optimized computer vision library | an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4-5 lines of code. | a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training | a new data block API | . The design of FastAI follows layered structure where we want the clarity and development speed of Keras and the customizability of PyTorch, which is not possible to be achieved both for the other frameworks. . FastAI was co-founded by Jeremy Howard, who is a data scientist, researcher, developer, educator, and entrepreneur, and Rachel Thomas, who is a professor of practice at Queensland University of Technology. . 1.1. FastAI by example . Let&#39;s go deeper to their FastAI codes to see how it works. Here is an example of how to fine-tune an ImageNet model on the Oxford IIT Pets dataset and achieve close to state-of-the-art accuracy within a couple of minutes of training on a single GPU. . from fastai.vision.all import * path = untar_data(URLs.PETS)/&#39;images&#39; . search_images_bing . &lt;function fastbook.search_images_bing(key, term, min_sz=128, max_images=150)&gt; . def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func(path, get_image_files(path), valid_pct= 0.2, seed= 42, label_func= is_cat, item_tfms= Resize(224)) learn = cnn_learner(dls, resnet34, metrics= error_rate) learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.176250 | 0.017708 | 0.007442 | 01:00 | . epoch train_loss valid_loss error_rate time . 0 | 0.062521 | 0.018719 | 0.005413 | 01:01 | . Each line of given code does one important task: . The second line (path = untar_data(URLs.PETS)/&#39;images&#39;) downloads a standard dataset from the fast.ai datasets collection (if not previously downloaded) to a configurable location (~/.fastai/data by default), extracts it (if not previously extracted), and returns a pathlib.Path object with the extracted location. | Then, dls=ImageDataLoaders.from_name_func(...) sets up the DataLoaders object and represents a combination of training and validation data. | . After defining DataLoaders object, we can easily look at the data with a single line of code. . dls.show_batch() . Let&#39;s analyze the parameters inside the ImageDataLoader: . valid_pct is the percentage of validation set compared to training set to avoid over-fitting. By defaults, valid_pct=0.2. As being quoted by Jeremy &quot;Overfitting is the single most important and challenging issue. It is easy to create a model that does a great job at making predictions on the exact data it has been trained on, but it is much harder to make accurate predictions on data the model has never seen before.&quot; | . Afterwards, we created a Learner, which provides an abstraction combining an optimizer, a model, and the data to train. THis line of code learn = cnn_learner(dls, resnet34, metrics= error_rate) will download an ImageNet-pretrained model, if not already available, remove the classification head of the model, and set appropriate defaults for the optimizer, weight decay, learning rate and so forth. . Basically, a Learner contains our data (i.e dls), architecture (i.e. resnet34) which is a mathematical function that we are optimizing and a metrics (i.e, error_rate). a Learner will figure out which are the best parameters for the architecture to match the label in the dataset. . When we are talking about the metrics, which is a function that measures the quality of the model’s predictions using the validation set, it should be noted that the metrics is not necessarily the same as loss. The loss measures how parameters changing influences the results of performances (better or worse). . To fits the model, the next line of code learn.fine_tune(1) tells us how to do. Model fitting is simply looking at how many times to look at each image (epochs). Instead of using fit, we use fine_tune method because we started with a pre-trained model and we don&#39; want to throw away all the capabilities that it already has. By performing fine_tune, the parameters of a pretrained model are updated by training for additional epochs using a different task to that used for pretraining. . In sums, fine_tune is used for transfer learning, in which we used a pretrained model for a task different to what is was originally trained for. . 2. End-to-End Digit Classifier with FastAI . 2.1. From Data to DataLoaders . To train our model with images, the first thing we should consider is the sizes of image inputs, because we don&#39;t feed the model one image at a time but a several of them (mini-batch ). To group them in a big array (usually called a tensor )) that is going to go through our model, they all need to be of the same size. In FastAI, the size modification of each single image, category is done via Item transform), for example Resize() function. . The, a mini-batch of items will be ready to be fed to GPU via DataLoaders Class. By default, fastai will give us 64 items at a time, all stacked up into a single tensor. . Instead of Resize, RandomResizeCrop is also super popupar since it change how we look at the same image differently on each epoch and it is a simple technique to avoid overfitting. . Data Augmentation . Data augmentation refers to creating random variations of our input data, such that they appear different but do not change the meaning of the data. One of the best way to do data augmentation is to use &lt;/i&gt; aug_transform()&lt;/i&gt;. It will return a list of different augmentation (e.g: contrast, bright , rotation etc). . It should be noted that the data augmentation is applied into a batch of equally sized items. So, we can apply these augmentations to an entire batch of them using GPU. . 2. Training a Digit Classifier . In this part, we will enlighten the role of . Firstly, we will download the well known MNIST dataset using fastAI . path = untar_data(URLs.MNIST_SAMPLE) . . 100.14% [3219456/3214948 00:00&lt;00:00] Then, we will look at the train folder which contains image digits of &#39;3&#39; and &#39;7&#39;. . threes=(path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens=(path/&#39;train&#39;/&#39;7&#39;).ls().sorted() threes . (#6131) [Path(&#39;train/3/10.png&#39;),Path(&#39;train/3/10000.png&#39;),Path(&#39;train/3/10011.png&#39;),Path(&#39;train/3/10031.png&#39;),Path(&#39;train/3/10034.png&#39;),Path(&#39;train/3/10042.png&#39;),Path(&#39;train/3/10052.png&#39;),Path(&#39;train/3/1007.png&#39;),Path(&#39;train/3/10074.png&#39;),Path(&#39;train/3/10091.png&#39;)...] . Let&#39;s look at one particular handwriting image in the &#39;7&#39; folder . im7_path = sevens[1] im7=Image.open(im7_path) im7 . To transform an image into a numeric value, we can use array method which is a part of Numpy array. For instance, to show a few numbers from the image: . array(im7)[7:16,8:16] . array([[ 0, 0, 15, 157, 254, 197, 0, 0], [ 0, 9, 220, 254, 254, 230, 104, 0], [ 0, 169, 254, 254, 231, 126, 40, 11], [183, 251, 254, 226, 81, 70, 180, 229], [254, 254, 255, 254, 254, 254, 255, 254], [254, 254, 254, 254, 253, 250, 212, 169], [254, 181, 77, 77, 48, 0, 0, 0], [195, 29, 0, 0, 0, 0, 0, 75], [ 0, 0, 0, 0, 0, 0, 59, 217]], dtype=uint8) . tensor(im7)[7:16,8:16] . tensor([[ 0, 0, 15, 157, 254, 197, 0, 0], [ 0, 9, 220, 254, 254, 230, 104, 0], [ 0, 169, 254, 254, 231, 126, 40, 11], [183, 251, 254, 226, 81, 70, 180, 229], [254, 254, 255, 254, 254, 254, 255, 254], [254, 254, 254, 254, 253, 250, 212, 169], [254, 181, 77, 77, 48, 0, 0, 0], [195, 29, 0, 0, 0, 0, 0, 75], [ 0, 0, 0, 0, 0, 0, 59, 217]], dtype=torch.uint8) . The beauty of using pytorch tensor over Numpy array is that the calculation of Pytorch tensor can be done in GPU. . Evenmore, we can use Panda to represent numeric values of an image because it has a very convenient thing which is so-called background_gradient that turn the background into gradient . im7_t = tensor(im7) df=pd.DataFrame(im7_t[7:20,7:20]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . &nbsp; 0 1 2 3 4 5 6 7 8 9 10 11 12 . 0 0 | 0 | 0 | 15 | 157 | 254 | 197 | 0 | 0 | 0 | 0 | 0 | 18 | . 1 0 | 0 | 9 | 220 | 254 | 254 | 230 | 104 | 0 | 0 | 0 | 65 | 216 | . 2 0 | 0 | 169 | 254 | 254 | 231 | 126 | 40 | 11 | 70 | 180 | 254 | 254 | . 3 40 | 183 | 251 | 254 | 226 | 81 | 70 | 180 | 229 | 254 | 254 | 254 | 254 | . 4 208 | 254 | 254 | 255 | 254 | 254 | 254 | 255 | 254 | 254 | 254 | 254 | 254 | . 5 254 | 254 | 254 | 254 | 254 | 253 | 250 | 212 | 169 | 125 | 167 | 254 | 254 | . 6 254 | 254 | 181 | 77 | 77 | 48 | 0 | 0 | 0 | 128 | 254 | 254 | 253 | . 7 157 | 195 | 29 | 0 | 0 | 0 | 0 | 0 | 75 | 248 | 254 | 254 | 139 | . 8 0 | 0 | 0 | 0 | 0 | 0 | 0 | 59 | 217 | 254 | 254 | 170 | 15 | . 9 0 | 0 | 0 | 0 | 0 | 0 | 15 | 217 | 254 | 254 | 214 | 15 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 5 | 113 | 254 | 254 | 238 | 55 | 0 | 0 | . 11 0 | 0 | 0 | 0 | 2 | 89 | 254 | 254 | 239 | 81 | 0 | 0 | 0 | . 12 0 | 0 | 0 | 0 | 141 | 254 | 254 | 240 | 106 | 0 | 0 | 0 | 0 | . As we can see, the background white pixels are stored as the number zero, black is 255, and shaded grey are something in between. In MNIST dataset, an entire image contains 28 pixels across and 28 pixels down, for a total of 768 pixels. . In the next mission, we will be going to create a model which help us to recognize &#39;3&#39; and &#39;7&#39; . First, we will create a list of all sevens and threes images by using tensor. . seven_tensors = [tensor(Image.open(o)) for o in sevens] three_tensors = [tensor(Image.open(o)) for o in threes] len(three_tensors),len(seven_tensors) . (6131, 6265) . show_image(three_tensors[2]) three_tensors[2].shape . torch.Size([28, 28]) . Then, we use Machine Learning approach that is described by Dr.Samuel to solve the differentiation problem: . &lt;/i&gt; &quot;Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would “learn” from its experience.&quot; &lt;/i&gt; . So, lets think about a function with parameter. Instead of finding an ideal image and compare every single image with an ideal image, we come up with a weight for each pixel of the image. The accumulated information of the weighted pixels gives us valuable information to differentiate between images. . To be more specific, we will build a Machine Learning classifier according to the following steps: . Initialize the weights. | For each image, use these weights to predict whether it appears to be a 3 or a 7. | Based on these predictions, calculate how good the model is (its loss). | Calculate the gradient, which measures for each weight how changing that weight would change the loss. | Step (that is, change) all the weights based on that calculation. | Go back to step 2 and repeat the process. | Iterate until you decide to stop the training process (for instance, because the model is good enough or you don’t want to wait any longer). | . . Calculate Gradients . Pytorch has a built-in engine that helps us to calculate gradient effeciently and simply. In order to do this, we start with a tensor and it comes up with a special method requires_grad()_. The purpose of using this method is when we perform any calculation on a tensor, it will remember that calculation it does so that we can take the derivatives later. Then we will call a special method backward which refers to the back propagation and do the derivative for us. Afterwards, we can view the gradients by checking the grad attribute of our tensor. . def f(x): return x**2 xt=tensor(3.).requires_grad_() xt yt=f(xt) yt yt.backward() xt.grad . tensor(6.) . Stepping with a learning rate . Deciding to change our parameter based on the value of the gradients is an important part. Gradient tells us the slop of a function, but does not tell us exactly how far to adjust our parameters. That&#39;ss where the learning rate appears. . w -= gradient(w) * lr . The update of the parameters will be inversed to the gradient and is multiplied by a learning rate. If we take a learning rate is too small, it will be needing more time for our algorithm to converge. If we take a large learning rate, it can results in the loss getting even worse. So picking up a good learning rate is really important. . Getting back to the MNIST, we need gradient to improve our model using SGD. In order to calculate gradient, we need some loss function that represent us how good our model is. . # stack the tensor together stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes = torch.stack(three_tensors).float()/255 # create the items and labels train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28) train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1) train_x.shape,train_y.shape # create dataset dset = list(zip(train_x,train_y)) . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape,valid_7_tens.shape valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) . Step 1: Initialize parameter . def init_params(size,std=1.0): return (torch.randn(size)*std).requires_grad_() . weights=init_params((28*28,1)) bias = init_params(1) . We can now calculate a prediction for one image . (train_x[0]*weights.T).sum() + bias . tensor([-3.8565], grad_fn=&lt;AddBackward0&gt;) . By utilising the power of a GPU, we can predict a set of images by using matrix multiplication to loop between the pixels of an image and between images . def linear(xb): return xb@weights + bias preds = linear(train_x) preds . tensor([[-3.8565], [-5.0067], [-9.2098], ..., [-8.2678], [ 0.1213], [ 1.4608]], grad_fn=&lt;AddBackward0&gt;) . To check out the accuracy, to decide if an output represents a 3 or 7, we can simply apply binary method for that. if the output is greater than 0, it represent a 3 and vice versa. . corrects = (preds&gt;0.0).float() == train_y corrects . tensor([[False], [False], [False], ..., [ True], [False], [False]]) . Notice that we can not apply the accuracy for the loss function here because a small changes of paramters does not lead to the significant changes of the results, so we need to build a new loss function to estimate the prediction. The following function will give a first try of measuring the distance between predictions and targets: . def mnist_loss(predictions,targets): return torch.where(targets==1,1-predictions,predictions).mean() . One problem with mnist_loss is that it assumes the predictions are always between 0 and 1. Then, we need to ensure that it is always the case. That&#39;s the place for the activation function - Sigmoid function . Sigmoid . def sigmoid(x): return 1/(1+torch.exp(-x)) . plot_function(torch.sigmoid, title=&#39;Sigmoid&#39;,min=-4,max=4) . /home/tmpext4/nd258645/conda-env/lib/python3.8/site-packages/fastbook/__init__.py:74: UserWarning: Not providing a value for linspace&#39;s steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/RangeFactories.cpp:25.) x = torch.linspace(min,max) . Then, let&#39;s update mnist_loss with sigmoid function: . def mnist_loss(predictions,targets): predictions=predictions.sigmoid() return torch.where(targets==1,1-predictions,predictions).mean() . SGD and mini-batches . In order to change or update the weight, we will walk through the step method (optimisation step). To take an optimiser step we need to calculate the loss over one or more data items. If we perform the optimisation step for every single item, it would take a very long time. On the other hand, if we perform the step for the whole data set at once, it could return an unstable gradient. . So, we will take a compromise between the two: we calculate the average loss for a few data items at a time(mini-batches). The number of data items in a batch is call batch size . A larger batch size means you will get more accurate and stable estimation of our dataset&#39;s gradienton the loss function, but it will take longer and you will get less mini-batches per epoch. Then, choosing a good batch size is one of the decision that is to help deep learning to train our model quickly and accurately. . In Pytorch and fastAI, there is a class that will do the shuffling and minibatch collation for us, called DataLoader . coll = range(15) dl = DataLoader(coll,batch_size=5,shuffle=True) list(dl) . [tensor([ 3, 12, 8, 10, 2]), tensor([ 9, 4, 7, 14, 5]), tensor([ 1, 13, 0, 6, 11])] . dl = DataLoader(dset,batch_size=256) valid_dl = DataLoader(valid_dset,batch_size=256) # def calc_grad(xb,yb,model): preds=model(xb) loss=mnist_loss(preds,yb) loss.backward() batch=train_x[:4] calc_grad(batch,train_y[:4],linear) weights.grad.mean(),bias.grad . (tensor(-0.0029), tensor([-0.0204])) . calc_grad(batch,train_y[:4],linear) weights.grad.mean(),bias.grad . (tensor(-0.0039), tensor([-0.0272])) . When we will perform the calc_grad twice, although we have not changed anything related to the weights, but the gradient results return different values!!! The reason for that is that loss.backward actually adds the gradients of loss into any gradients that are currently stored. So we have to set the current gradients to zero first. . weights.grad.zero_() bias.grad.zero_(); . Then, we will update the weights and bias based on the gradient and learning rate. . def train_epoch(model,lr,params): for xb,yb in dl: calc_grad(xb,yb,model) for p in params: p.data -= p.grad*lr p.grad.zero_() . (preds&gt;0.0).float() == train_y . tensor([[False], [False], [False], ..., [ True], [False], [False]]) . Then, we calculate the accuracy . def batch_accuracy(xb,yb): preds = xb.sigmoid() corrects=(preds&gt;0.5)==yb return corrects.float().mean() . batch_accuracy(linear(batch),train_y[:4]) . tensor(0.) . def validate_epoch(model): accs = [batch_accuracy(model(xb),yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(),4) . validate_epoch(linear) . 0.4085 . Let&#39;s train in one epoch . lr = 1. params=weights,bias train_epoch(linear,lr,params) validate_epoch(linear) . 0.5849 . for i in range(20): train_epoch(linear,lr,params) print(validate_epoch(linear),end=&#39; &#39;) . 0.973 0.973 0.9735 0.9745 0.9749 0.9754 0.9759 0.9759 0.9759 0.9764 0.9774 0.9779 0.9779 0.9784 0.9784 0.9784 0.9789 0.9794 0.9794 0.9794 . So, we have succesfully built a SGD optimizer of a simple linear function anf get the accuracy upto 97.94% . Creating an optimizer . In order to automate the initialization of an optimizer, Pytorch provides some useful functions to replace our linear() function with Pytorch&#39;s nn.Linear module. . nn.Linear does the same thing as our init_params and Linear together. It contains both weights and bias in a single class. . linear_model = nn.Linear(28*28,1) w,b = linear_model.parameters() w.shape,b.shape . (torch.Size([1, 784]), torch.Size([1])) . The, we can use this information to create an optimizer: . class BasicOptim: def __init__(self,params,lr): self.params, self.lr= list(params),lr def step(self,*args,**kwargs): for p in self.params: p.data-= p.grad.data*self.lr def zero_grad(self,*args,**kwargs): for p in self.params: p.grad=None . opt = BasicOptim(linear_model.parameters(),lr) . Then, the new training loop should be: . def train_epoch(model): for xb,yb in dl: calc_grad(xb,yb,model) opt.step() opt.zero_grad() def train_model(model,epochs): for i in range(epochs): train_epoch(model) print(validate_epoch(model), end=&#39; &#39;) . train_model(linear_model,30) . 0.4932 0.8076 0.8554 0.917 0.935 0.9487 0.9575 0.9633 0.9653 0.9677 0.9697 0.9716 0.9736 0.9751 0.976 0.9765 0.9775 0.9775 0.9785 0.9785 0.979 0.979 0.979 0.979 0.9795 0.9795 0.9799 0.9804 0.9809 0.9814 . In FastAI, it provides us an API of SGD class which does the same thing as BasicOptim . linear_model = nn.Linear(28*28,1) opt = SGD(linear_model.parameters(),lr) train_model(linear_model,30) . 0.4932 0.789 0.853 0.9155 0.935 0.9492 0.9555 0.9638 0.9658 0.9672 0.9697 0.9716 0.9731 0.9751 0.9755 0.977 0.9775 0.978 0.978 0.9785 0.979 0.979 0.979 0.9795 0.9795 0.9795 0.9799 0.9804 0.9804 0.9814 . FastAI also provides Learner.fit, which we can use instead of train_model. To create a Learner we first need to create DataLoader, by passing in our training and validation DataLoaders. . dls=DataLoaders(dl,valid_dl) . learn=Learner(dls,nn.Linear(28*28,1),opt_func=SGD,loss_func=mnist_loss,metrics=batch_accuracy) . learn.fit(10,lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.009775 | 0.017821 | 0.985280 | 00:00 | . 1 | 0.009761 | 0.017807 | 0.985280 | 00:00 | . 2 | 0.009744 | 0.017793 | 0.985280 | 00:00 | . 3 | 0.009726 | 0.017780 | 0.985280 | 00:00 | . 4 | 0.009707 | 0.017767 | 0.985280 | 00:00 | . 5 | 0.009688 | 0.017754 | 0.985280 | 00:00 | . 6 | 0.009669 | 0.017742 | 0.985280 | 00:00 | . 7 | 0.009650 | 0.017730 | 0.985280 | 00:00 | . 8 | 0.009631 | 0.017717 | 0.985280 | 00:00 | . 9 | 0.009613 | 0.017706 | 0.985280 | 00:00 | . Adding a non-linearity . So far, we studied a general procedure for optimising the parameters of a function by using a simple linear classifier. To make it a bit more sophisticated, we need to add a non-linearity between two linear classifiers, and this gives us a neural network . def simple_net(xb): s1=xb@w1+b1 res=s1.max(tensor(0.0)) s2=res@w2+b2 return s2 . The little s1.max(tensor(0.0)) is called rectified linear unit (RELU). In Pytorch, it is also available as F.relu . plot_function(F.relu) . The addition of nonlinear function, we can provide what Universal Approximation Theorem says, which can represent any arbitrary function. . We can replace the initialization of the basic neural network by taking advantage of Pytorch: . simple_net = nn.Sequential( nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1) ) . nn.Sequential is a module that calls each of the listed layers or function in turn. . learn=Learner(dls,simple_net,opt_func=SGD,loss_func=mnist_loss,metrics=batch_accuracy) #hide_output learn.fit(40,0.1) . plt.plot(L(learn.recorder.values).itemgot(2)) . [&lt;matplotlib.lines.Line2D at 0x2b1b144271c0&gt;] . For deeper models, we may need to use a lower learning rate and a few more epochs. In practice, we can freely to set many layers as well as the nonlinearity between layers. However, the deeper the model gets, the harder it is to optimize the parameters in practice. So why we would use a deeper model? The reason is the performance. With the deeper model, it turns out that we can use smaller matrices , with more layers, and get better results that we would get with larger matrices and few layers. . Here is what happens when we train an 18-layer model . dls=ImageDataLoaders.from_folder(path) learn=cnn_learner(dls,resnet18,pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(1,0.1) . /home/tmpext4/nd258645/conda-env/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . epoch train_loss valid_loss accuracy time . 0 | 0.156772 | 0.021748 | 0.995584 | 00:32 | .",
            "url": "https://dnlam.github.io/fastblog/2022/03/17/intro_fastai.html",
            "relUrl": "/2022/03/17/intro_fastai.html",
            "date": " • Mar 17, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "The Beauty of Neural Network",
            "content": "The Sample Problem . Consider a system that tries to map groups of 3 inputs to some corresponding output which is a single number. In the following picture, we&#39;ll show each set of 3 inputs as a row of a matrix $X$, and each output as the corresponding row of $Y$: . $$ overbrace{ left[ { begin{array}{ccc} 0 &amp; 0 &amp; 1 0 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 end{array} } right] }^{X} rightarrow overbrace{ left[ { begin{array}{c} 0 0 1 1 end{array} } right] }^Y. $$Even though this system has an exact solution (namely, $Y$ equals the first column of $X$), usually we&#39;ll need to be satisfied with a system that maps our inputs $X$ to some approximate &quot;prediction&quot; $ tilde{Y}$, which we hope to bring closer to the &quot;target&quot; $Y$ by means of successive improvements. . The way we&#39;ll get our prediction $ tilde{Y}$ is by means of a weighted sum of each set of 3 inputs, and some nonlinear function $f$ which we call the &quot;activation function&quot; (or just &quot;activation&quot;). Pictorially, the process looks like the following, for each row $i$ of $X$ and $Y$, (where the columns of $X$ are shown arranged vertically instead of horizonally): . . In terms of matrix multiplication, since X is a 4x3 matrix, and Y is a 4x1 matrix, that implies that our weights should be a 3x1 matrix consisting of (unknown) values $w_0$, $w_1$ and $w_2$. The calculation can be written as: . $$ f left( overbrace{ left[ { begin{array}{ccc} 0 &amp; 0 &amp; 1 0 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 end{array} } right] }^ text{X} overbrace{ left[ { begin{array}{c} w_0 w_1 w_2 end{array} } right] }^{w} right) = overbrace{ left[ { begin{array}{c} 0 0 1 1 end{array} } right] }^{ tilde{Y}} $$Our nonlinear activation function $f$ is taken to operate on each row element one at a time, and we&#39;ll let $f_i$ denote the $i$th row of this completed activation, i.e.: . $$ f_i = f left( sum_j X_{ij}w_j right) = tilde{Y}_i . $$The particular activation function we will use is the &quot;sigmoid&quot;, . $$ f(x) = {1 over{1+e^{-x}}}, $$-- click here to see a plot of this function -- which has the derivative . $$ {df over dx} = {e^{-x} over(1 + e^{-x})^2} $$which can be shown (Hint: exercise for &quot;mathy&quot; students!) to simplify to $$ {df over dx}= f(1-f). $$ . The overall problem then amounts to finding the values of the &quot;weights&quot; $w_0, w_1,$ and $w_2$ so that the $ tilde{Y}$ we calculate is as close to the target $Y$ as possible. . To do this, we will seek to minimize a loss function defined as a sum across all data points we have, i.e. all 4 rows. The loss function $L$ we will choose is the mean square error loss, or MSE, we will use a &#39;better&#39; loss function for this problem): . $$ L = {1 over N} sum_{i=0}^{N-1} left[ tilde{Y}_i - Y_i right]^2, $$or in terms of the activation function $$ L = {1 over N} sum_{i=0}^{N-1} left[ f_i - Y_i right]^2. $$ . Each of the weights $w_j$ ($j=0..2$) will start with random values, and then be updated via gradient descent, i.e. . $$ w_j^{new} = w_j^{old} - alpha{ partial L over partial w_j} $$where $ alpha$ is the learning rate, chosen to be some small parameter. For the MSE loss shown above, the partial derivatives with respect to each of the weights is . $$ { partial L over partial w_j} = {2 over N} sum_{i=0}^{N-1} left[ tilde{Y}_i - Y_i right]{ partial f_i over partial w_j} = {2 over N} sum_{i=0}^{N-1} left[ tilde{Y}_i - Y_i right]f_i(1-f_i)X_{ij}. $$Absorbing the factor of 2/N into our choice of $ alpha$, and writing the summation as a dot product, and noting that $f_i = tilde{Y}_i$, we can write the update for all the weights together as . $$ w = w - alpha X^T cdot left( [ tilde{Y}-Y]* tilde{Y}*(1- tilde{Y}) right) $$where the $ cdot$ denotes a matrix-matrix product (i.e. a dot product for successive rows of $X^T$) and $*$ denotes elementwise multiplication. . To clarify the above expression in terms of matrix dimensions, we can see that $w$, a 3x1 matrix, can be made by multipyting $X^T$ (a 3 x4 matrix) with the term in parentheses, i.e. the product of elementwise terms involving $ tilde{Y}$, which is a 4x1 matrix. In other words, a 3x4 matrix, times a 4x1 matrix, yields a 3x1 matrix. . Actual Code . The full code for all of this is then... . # https://iamtrask.github.io/2015/07/12/basic-python-network/ import numpy as np # sigmoid activation def sigmoid(x,deriv=False): if(deriv==True): return x*(1-x) return 1/(1+np.exp(-x)) # input dataset X = np.array([ [0,0,1], [0,1,1], [1,0,1], [1,1,1] ]) # target output dataset Y = np.array([[0,0,1,1]]).T # seed random numbers to make calculation # deterministic (just a good practice) np.random.seed(1) # initialize weights randomly with mean 0 w = 2*np.random.random((3,1)) - 1 alpha = 1.0 # learning rate loss_history = [] # keep a record of how the loss proceeded, blank for now for iter in range(1000): # forward propagation Y_pred = sigmoid(np.dot(X,w)) # prediction, i.e. tilde{Y} # how much did we miss? diff = Y_pred - Y loss_history.append((diff**2).mean()) # add to the history of the loss # update weights w -= alpha * np.dot( X.T, diff*sigmoid(Y_pred, deriv=True)) print(&quot;Output After Training:&quot;) print(&quot;Y_pred = (should be two 0&#39;s followed by two 1&#39;s) n&quot;,Y_pred) print(&quot;weights = n&quot;,w) . Output After Training: Y_pred = (should be two 0&#39;s followed by two 1&#39;s) [[0.03178421] [0.02576499] [0.97906682] [0.97414645]] weights = [[ 7.26283009] [-0.21614618] [-3.41703015]] . Note that, because of our nonlinear activation, we don&#39;t get the solution $w_0=1, w_1=0, w_2=0$. . Plotting the loss vs. iteration number, we see... . %matplotlib inline import matplotlib.pyplot as plt plt.loglog(loss_history) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.show() . Change the activation function . Another popular choice of activation function is the rectified linear unit or ReLU. The function ReLU(x) is zero for x &lt;= 0, and equal to x (i.e. a straight line at 45 degrees for) x &gt;0. It can be written as max(x,0) or x * (x&gt;0), and its derivative is 1 for positive x, and zero otherwise. . Click here to see a graph of ReLU . Modifying our earlier code to use ReLU activation instead of sigmoid looks like this: . def relu(x,deriv=False): # relu activation if(deriv==True): return 1*(x&gt;0) return x*(x&gt;0) # seed random numbers to make calculation # deterministic (just a good practice) np.random.seed(1) # initialize weights randomly (but only &gt;0 because ReLU clips otherwise) w = np.random.random((3,1)) alpha = 0.3 # learning rate new_loss_history = [] # keep a record of how the error proceeded for iter in range(1000): # forward propagation Y_pred = relu(np.dot(X,w)) # how much did we miss? diff = Y_pred - Y new_loss_history.append((diff**2).mean()) # add to the record of the loss # update weights w -= alpha * np.dot( X.T, diff*relu(Y_pred, deriv=True)) print(&quot;Output After Training:&quot;) print(&quot;Y_pred = (should be two 0&#39;s followed by two 1&#39;s) n&quot;,Y_pred) print(&quot;weights = n&quot;,w) . Output After Training: Y_pred = (should be two 0&#39;s followed by two 1&#39;s) [[-0.] [-0.] [ 1.] [ 1.]] weights = [[ 1.01784368e+00] [ 8.53961786e-17] [-1.78436793e-02]] . print( w[2] - (1-w[0]) ) . [-3.46944695e-17] . Plot old results with new results: . %matplotlib inline import matplotlib.pyplot as plt plt.loglog(loss_history,label=&quot;sigmoid&quot;) plt.loglog(new_loss_history,label=&quot;relu&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.legend() plt.show() . Looks like ReLU may be a better choice than sigmoid for this problem! . Exercise: Read a 7-segment display . A 7-segment display is used for displaying numerical digits 0 through 9, usually by lighting up LEDs or parts of a liquid crystal display (LCD). The segments are labelled $a$ through $g$ according to the following diagram: . . Diagram of the network . The 7 inputs &quot;a&quot; through &quot;g&quot; will be mapped to 10 outputs for the individual digits, and each output can range from 0 (&quot;false&quot; or &quot;no&quot;) to 1 (&quot;true&quot; or &quot;yes&quot;) for that digit. The input and outputs will be connected by a matrix of weights. Pictorially, this looks like the following (Not shown: activation function $f$): . . ...where again, this network operates on a single data point at a time, datapoints which are rows of X and Y. What is shown in the above diagram are the columns of $X$ and $Y$ for a single row (/ single data point). . Create the dataset . Let the input X be the segments $a$ through $g$ are the columns of the input $X$, and are either 1 for on or 0 for off. Let the columns of the target $Y$ be the digits 0-9 themselves arranged in a &quot;one hot&quot; encoding scheme, as follows: . Digit | One-Hot Encoding for $Y$ | . 0 | 1,0,0,0,0,0,0,0,0,0 | . 1 | 0,1,0,0,0,0,0,0,0,0 | . 2 | 0,0,1,0,0,0,0,0,0,0 | . ... | ... | . 9 | 0,0,0,0,0,0,0,0,0,1 | . The values in the columns for $Y$ are essentially true/false &quot;bits&quot; for each digit, answering the question &quot;Is this digit the appropriate output?&quot; with a &quot;yes&quot;(=1) or &quot;no&quot; (=0) response. . The input $X$ will be a 10x7 matrix, and the target $Y$ will be a 10x10 matrix. Each row of $X$ will be the segments to produce the digit for that row. For example, the zeroth row of $X$ should show segments on which make an image of the digit zero, namely segments a, b, c, d, e, and f but not g, so that the zeroth row of X should be [1,1,1,1,1,1,0]. . Define numpy arrays for both $X$ and $Y$ (Hint: for $Y$, check out np.eye()): . # for the 7-segment display. The following is just a &quot;stub&quot; to get you started. X = np.array([ [1,1,1,1,1,1,0], [], [], [] ]) Y = np.array([ [1,0,0,0,0,0,0,0,0,0], [], [] ]) . Initialize the weights . Previously the dimensions of the weight matrix $w$ were 3x1 because we were mapping each row of 3 elements in $X$ to each row of 1 element of $Y$. For this new problem, each row of $X$ has 7 elements, and we want to map those to the 10 elements in each 1-hot-encoded row of $Y$, so what should the dimensions of the weights matrix $w$ be? . Write some numpy code to randomly initialize the weights matrix: . np.random.seed(1) # initial RNG so everybody gets similar results w = np.random.random(( , )) # Students, fill in the array dimensions here . File &#34;&lt;ipython-input-7-20f53a51cded&gt;&#34;, line 1 w = np.random.random(( , )) ^ SyntaxError: invalid syntax . Train the network . Having created an $X$ and its matching $Y$, and initalized the weights $w$ randomly, train a neural network such as the ones above to learn to map a row of X to a row of Y, i.e. train it to recognize digits on 7-segment displays. Do this below. . # Use sigmoid activation, and 1000 iterations, and learning rate of 0.9 # Question: What happens if you use ReLU instead? Try it later. Is ReLU always the best choice? # And then print out your Y_pred &amp; weights matrix, and limit it to 3 significant digits print(&quot;Output After Training:&quot;) np.set_printoptions(formatter={&#39;float&#39;: lambda x: &quot;{0:0.3f}&quot;.format(x)}) # 3 sig figs print(&quot;Y_pred= n&quot;,Y_pred) print(&quot;weights = n&quot;,repr(w)) # the repr() makes it so it can be copied &amp; pasted back into Python code . Final Check: Keras version . Keras is a neural network library that lets us write NN applications very compactly. Try running the following using the X and Y from your 7-segment dataset: . import keras from keras.models import Sequential from keras.layers import Dense, Activation model = Sequential([ Dense(10, input_shape=(7,)), Activation(&#39;sigmoid&#39;) ]) model.compile(optimizer=&#39;adam&#39;, # We&#39;ll talk about optimizer choices and loss choices later loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.fit(X, Y, epochs=200, batch_size=1) print(&quot; nY_tilde = n&quot;, model.predict(X) ) . Follow-up: Remarks . Re-stating what we just did . The original problem (posed at the top of this notebook) involves mapping some points from a 3-dimensional space into points in a 1-dimensional space, i.e. to points on the number line. The mapping is done by the combination of a weighted sum (a linear operation) and a nonlinear &quot;activation function&quot; applied to that sum. The use of an activation function like a sigmoid was originally intended to serve as an analogy of activation of biological neurons. Nonlinear activation functions are source of the &quot;power&quot; of neural networks (essentially we approximate some other function by means of a sum of basis functions in some function space, but don&#39;t worry about that if you&#39;re not math-inclined). The algorithm &#39;learns&#39; to approximate this operation via supervised learning and gradient descent according to some loss function. We used the mean squared error (MSE) for our loss, but lots and lots of different loss functions could be used, a few of which we&#39;ll look at another time. . Question for reflection: Unlike fitting a line $y = mx+b$, the weighted sum in our models in this notebook had no constant &quot;bias&quot; term like $b$. How might we include such a term? . One thing we glossed over: &quot;batch size&quot; . Question: Should we apply the gradient descent &quot;update&quot; to the weights each time we process a single row of $X$ &amp; $Y$, or should we compute the combined loss of all the rows together at the same time, and then do the update? This is essentially asking the same question as &quot;When fitting a line $mx+b$ to a bunch of data points, should we use all the points together to update $m$ and $b,$ or should we do this one point at a time -- compute the gradients of the loss at one point, update the weights, compute gradients at another point, etc.?&quot; . The number of points you use is called the batch size and it is what&#39;s known as a &quot;hyperparameter&quot; -- it is not part of the model per se, but it is a(n important) choice you make when training the model. The batch size affects the learning as follows: Averaging the gradints for many data points (i..e. a large batch size) will produce a smoother loss function and will also usually make the code execute more quickly through the dataset, but updating the weights for every point will cause the algorithm to learn with fewer iterations. . One quick way to observe this is to go up to the Keras code above and change batch_size from 1 to 10, and re-execute the cell. How is the accuracy after 200 iteractions, compared to when batch_size=1? . Terminology: Technically, it&#39;s called &quot;batch training&quot; when you sum the gradients for all the data points before updating the weights, whereas using fewer points is &quot;minibatch training&quot;, and updating for each point (i.e. each row, for us) is Stochastic Gradient Descent* (SGD -- more on these terms here). In practice, there is a tradeoff between smaller vs. larger (mini)batches, which has been the subject of intense scrutiny by researchers over the years. We will have more to say on this later. . For discussion later: In our presentation above, were we using batch training, minibatch training or SGD? . . . *Note: many people will regard SGD as an optimization algorithm per se, and refer to doing SGD even for (mini)batch sizes larger than 1. . Optional: If you want to go really crazy . How about training on this dataset: $$ overbrace{ left[ { begin{array}{cc} 0 &amp; 0 0 &amp; 1 1 &amp; 0 1 &amp; 1 end{array} } right] }^{X} rightarrow overbrace{ left[ { begin{array}{c} 0 1 1 0 end{array} } right] }^Y. $$ Good luck! ;-) (Hint 1: This problem features prominently in the history of Neural Networks, involving Marvin Minsky and &quot;AI Winter.&quot; Hint 2: This whole lesson could instead be entitled &quot;My First Artificial Neuron.&quot;) . Additional Optional Exercise: Binary Math vs. One-Hot Encoding . For the 7-segment display, we used a one-hot encoding for our output, namely a set of true/false &quot;bits&quot; for each digit. One may wonder how effective this ouput-encoding method is, compared to a different bit-setting encoding method, namely binary representations. . Construct the target output matrix $Y$ for binary representations of the numbers 0 through 9. Your target matrix should have 10 rows and 4 columns (i.e, output bits for 1s, 2s, 4s, and 8s). | Using this $Y$ array, train the network as before, and plot the loss as a function of iteration. | Question: Which method works &#39;better&#39;? One-hot encoding or binary encoding? .",
            "url": "https://dnlam.github.io/fastblog/2021/06/04/The_Beauty_Of_Neural_Network.html",
            "relUrl": "/2021/06/04/The_Beauty_Of_Neural_Network.html",
            "date": " • Jun 4, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Optimal Policies with Dynamic Programming",
            "content": "Section: Dynamic Programming . The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies. . In order to shed light on the Chapter 4 of the bible Reinforcement Learning, the objectives of this notebook are: . Policy Evaluation and Policy Improvement | Value and Policy Iteration | Bellman Equations | . Context . We will give an example of how to apply DP into Gridworld City to deal with the City&#39;s Parking problem. The city council has created a Markov decision process (MDP) to model the demand for parking with a reward function that reflects its preferences. . States are nonnegative integers indicating how many parking spaces are occupied. | Actions are nonnegative integers designating the price of street parking. | The reward is a real value describing the city&#39;s preference for the situation. | Time is discretized by hour. | . Preliminaries . The BaseAgent, Environment and RLGlue should follow the our first Notebook on Reinforcement Learning Exploration/Exploitation. . The construction of a virtual ParkingWorld and the plot function are given below: . &lt;Figure size 432x288 with 0 Axes&gt; . Section 1: Policy Evaluation . First, the city council would like you to evaluate the quality of the existing pricing scheme. Policy evaluation works by iteratively applying the Bellman equation for $v_{ pi}$ to a working value function, as an update rule, as shown below. . $$ large v(s) leftarrow sum_a pi(a | s) sum_{s&#39;, r} p(s&#39;, r | s, a)[r + gamma v(s&#39;)]$$ This update can either occur &quot;in-place&quot; (i.e. the update rule is sequentially applied to each state) or with &quot;two-arrays&quot; (i.e. the update rule is simultaneously applied to each state). Both versions converge to $v_{ pi}$ but the in-place version usually converges faster. In this assignment, we will be implementing all update rules in-place, as is done in the pseudocode of chapter 4 of the textbook. . The policy evaluation can be expressed in the code below: . def evaluate_policy(env, V, pi, gamma, theta): delta = float(&#39;inf&#39;) while delta &gt; theta: delta = 0 for s in env.S: v = V[s] bellman_update(env, V, pi, s, gamma) delta = max(delta, abs(v - V[s])) return V . Then, the Bellman update will be: . def bellman_update(env, V, pi, s, gamma): &quot;&quot;&quot;Mutate ``V`` according to the Bellman update equation.&quot;&quot;&quot; v = 0 for a in env.A: transitions = env.transitions(s, a) for s_, (r, p) in enumerate(transitions): v += pi[s][a] * p * (r + gamma * V[s_]) V[s] = v . The observation shows that the value monotonically increases as more parking is used, until there is no parking left, in which case the value is lower. Because of the relatively simple reward function (more reward is accrued when many but not all parking spots are taken and less reward is accrued when few or all parking spots are taken) and the highly stochastic dynamics function (each state has positive probability of being reached each time step) the value functions of most policies will qualitatively resemble this graph. However, depending on the intelligence of the policy, the scale of the graph will differ. In other words, better policies will increase the expected return at every state rather than changing the relative desirability of the states. Intuitively, the value of a less desirable state can be increased by making it less likely to remain in a less desirable state. Similarly, the value of a more desirable state can be increased by making it more likely to remain in a more desirable state. That is to say, good policies are policies that spend more time in desirable states and less time in undesirable states. As we will see in this assignment, such a steady state distribution is achieved by setting the price to be low in low occupancy states (so that the occupancy will increase) and setting the price high when occupancy is high (so that full occupancy will be avoided). . Policy Iteration . Now the city council would like to propose a more efficient policy using policy iteration. Policy iteration works by alternating between evaluating the existing policy and making the policy greedy with respect to the existing value function. . def improve_policy(env, V, pi, gamma): policy_stable = True for s in env.S: old = pi[s].copy() q_greedify_policy(env, V, pi, s, gamma) if not np.array_equal(pi[s], old): policy_stable = False return pi, policy_stable def policy_iteration(env, gamma, theta): V = np.zeros(len(env.S)) pi = np.ones((len(env.S), len(env.A))) / len(env.A) policy_stable = False while not policy_stable: V = evaluate_policy(env, V, pi, gamma, theta) pi, policy_stable = improve_policy(env, V, pi, gamma) return V, pi . def q_greedify_policy(env, V, pi, s, gamma): &quot;&quot;&quot;Mutate ``pi`` to be greedy with respect to the q-values induced by ``V``.&quot;&quot;&quot; G = np.zeros_like(env.A, dtype=float) for a in env.A: transitions = env.transitions(s, a) for s_, (r, p) in enumerate(transitions): G[a] += p * (r + gamma * V[s_]) greed_actions = np.argwhere(G == np.amax(G)) for a in env.A: if a in greed_actions: pi[s, a] = 1 / len(greed_actions) else: pi[s, a] = 0 . Section 3: Value Iteration . Value iteration works by iteratively applying the Bellman optimality equation for $v_{ ast}$ to a working value function, as an update rule, as shown below. . $$ large v(s) leftarrow max_a sum_{s&#39;, r} p(s&#39;, r | s, a)[r + gamma v(s&#39;)]$$ . def value_iteration(env, gamma, theta): V = np.zeros(len(env.S)) while True: delta = 0 for s in env.S: v = V[s] bellman_optimality_update(env, V, s, gamma) delta = max(delta, abs(v - V[s])) if delta &lt; theta: break pi = np.ones((len(env.S), len(env.A))) / len(env.A) for s in env.S: q_greedify_policy(env, V, pi, s, gamma) return V, pi . def bellman_optimality_update(env, V, s, gamma): &quot;&quot;&quot;Mutate ``V`` according to the Bellman optimality update equation.&quot;&quot;&quot; vmax = - float(&#39;inf&#39;) for a in env.A: transitions = env.transitions(s, a) va = 0 for s_, (r, p) in enumerate(transitions): va += p * (r + gamma * V[s_]) vmax = max(va, vmax) V[s] = vmax . In the value iteration algorithm above, a policy is not explicitly maintained until the value function has converged. Below, we have written an identically behaving value iteration algorithm that maintains an updated policy. Writing value iteration in this form makes its relationship to policy iteration more evident. Policy iteration alternates between doing complete greedifications and complete evaluations. On the other hand, value iteration alternates between doing local greedifications and local evaluations. . def value_iteration2(env, gamma, theta): V = np.zeros(len(env.S)) pi = np.ones((len(env.S), len(env.A))) / len(env.A) while True: delta = 0 for s in env.S: v = V[s] q_greedify_policy(env, V, pi, s, gamma) bellman_update(env, V, pi, s, gamma) delta = max(delta, abs(v - V[s])) if delta &lt; theta: break return V, pi .",
            "url": "https://dnlam.github.io/fastblog/2020/06/29/Optimal_Policy_Dynamic_Programming.html",
            "relUrl": "/2020/06/29/Optimal_Policy_Dynamic_Programming.html",
            "date": " • Jun 29, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Bandits and Exploitation/Exploration in Reinforcement Learning",
            "content": "Objective of this notebook are to: . Help you understand the bandit problem | Understand the effects of epsilon on exploration and learn about exploitation/exploration trade-off | Introduce some of essential RL softwares that are useful to build a RL solver. | . Section 0: Preliminaries . Firstly, we will build a BaseAgent for our RL . from __future__ import print_function from abc import ABCMeta, abstractmethod class BaseAgent: &quot;&quot;&quot;Implements the agent for an RL-Glue environment. Note: agent_init, agent_start, agent_step, agent_end, agent_cleanup, and agent_message are required methods. &quot;&quot;&quot; __metaclass__ = ABCMeta def __init__(self): pass @abstractmethod def agent_init(self, agent_info= {}): &quot;&quot;&quot;Setup for the agent called when the experiment first starts.&quot;&quot;&quot; @abstractmethod def agent_start(self, observation): &quot;&quot;&quot;The first method called when the experiment starts, called after the environment starts. Args: observation (Numpy array): the state observation from the environment&#39;s evn_start function. Returns: The first action the agent takes. &quot;&quot;&quot; @abstractmethod def agent_step(self, reward, observation): &quot;&quot;&quot;A step taken by the agent. Args: reward (float): the reward received for taking the last action taken observation (Numpy array): the state observation from the environment&#39;s step based, where the agent ended up after the last step Returns: The action the agent is taking. &quot;&quot;&quot; @abstractmethod def agent_end(self, reward): &quot;&quot;&quot;Run when the agent terminates. Args: reward (float): the reward the agent received for entering the terminal state. &quot;&quot;&quot; @abstractmethod def agent_cleanup(self): &quot;&quot;&quot;Cleanup done after the agent ends.&quot;&quot;&quot; @abstractmethod def agent_message(self, message): &quot;&quot;&quot;A function used to pass information from the agent to the experiment. Args: message: The message passed to the agent. Returns: The response (or answer) to the message. &quot;&quot;&quot; . class Agent(BaseAgent): &quot;&quot;&quot;agent does *no* learning, selects action 0 always&quot;&quot;&quot; def __init__(self): self.last_action = None self.num_actions = None self.q_values = None self.step_size = None self.epsilon = None self.initial_value = 0.0 self.arm_count = [0.0 for _ in range(10)] def agent_init(self, agent_info={}): &quot;&quot;&quot;Setup for the agent called when the experiment first starts.&quot;&quot;&quot; # if &quot;actions&quot; in agent_info: # self.num_actions = agent_info[&quot;actions&quot;] # if &quot;state_array&quot; in agent_info: # self.q_values = agent_info[&quot;state_array&quot;] self.num_actions = agent_info.get(&quot;num_actions&quot;, 2) self.initial_value = agent_info.get(&quot;initial_value&quot;, 0.0) self.q_values = np.ones(agent_info.get(&quot;num_actions&quot;, 2)) * self.initial_value self.step_size = agent_info.get(&quot;step_size&quot;, 0.1) self.epsilon = agent_info.get(&quot;epsilon&quot;, 0.0) self.last_action = 0 def agent_start(self, observation): &quot;&quot;&quot;The first method called when the experiment starts, called after the environment starts. Args: observation (Numpy array): the state observation from the environment&#39;s evn_start function. Returns: The first action the agent takes. &quot;&quot;&quot; self.last_action = np.random.choice(self.num_actions) # set first action to 0 return self.last_action def agent_step(self, reward, observation): &quot;&quot;&quot;A step taken by the agent. Args: reward (float): the reward received for taking the last action taken observation (Numpy array): the state observation from the environment&#39;s step based, where the agent ended up after the last step Returns: The action the agent is taking. &quot;&quot;&quot; # local_action = 0 # choose the action here self.last_action = np.random.choice(self.num_actions) return self.last_action def agent_end(self, reward): &quot;&quot;&quot;Run when the agent terminates. Args: reward (float): the reward the agent received for entering the terminal state. &quot;&quot;&quot; pass def agent_cleanup(self): &quot;&quot;&quot;Cleanup done after the agent ends.&quot;&quot;&quot; pass def agent_message(self, message): &quot;&quot;&quot;A function used to pass information from the agent to the experiment. Args: message: The message passed to the agent. Returns: The response (or answer) to the message. &quot;&quot;&quot; pass . Then, we create an Environment and its abstract for our RL. This is the 10-armed Testbed introduced in section 2.3 of the textbook. We use this throughout this notebook to test our bandit agents. It has 10 arms, which are the actions the agent can take. Pulling an arm generates a stochastic reward from a Gaussian distribution with unit-variance. For each action, the expected value of that action is randomly sampled from a normal distribution, at the start of each run. If you are unfamiliar with the 10-armed Testbed please review it in the textbook before continuing. . &quot;&quot;&quot;Abstract environment base class. &quot;&quot;&quot; from __future__ import print_function from abc import ABCMeta, abstractmethod class BaseEnvironment: &quot;&quot;&quot;Implements the environment for an RLGlue environment Note: env_init, env_start, env_step, env_cleanup, and env_message are required methods. &quot;&quot;&quot; __metaclass__ = ABCMeta def __init__(self): reward = None observation = None termination = None self.reward_obs_term = (reward, observation, termination) @abstractmethod def env_init(self, env_info={}): &quot;&quot;&quot;Setup for the environment called when the experiment first starts. Note: Initialize a tuple with the reward, first state observation, boolean indicating if it&#39;s terminal. &quot;&quot;&quot; @abstractmethod def env_start(self): &quot;&quot;&quot;The first method called when the experiment starts, called before the agent starts. Returns: The first state observation from the environment. &quot;&quot;&quot; @abstractmethod def env_step(self, action): &quot;&quot;&quot;A step taken by the environment. Args: action: The action taken by the agent Returns: (float, state, Boolean): a tuple of the reward, state observation, and boolean indicating if it&#39;s terminal. &quot;&quot;&quot; @abstractmethod def env_cleanup(self): &quot;&quot;&quot;Cleanup done after the environment ends&quot;&quot;&quot; @abstractmethod def env_message(self, message): &quot;&quot;&quot;A message asking the environment for information Args: message: the message passed to the environment Returns: the response (or answer) to the message &quot;&quot;&quot; . class Environment(BaseEnvironment): &quot;&quot;&quot;Implements the environment for an RLGlue environment Note: env_init, env_start, env_step, env_cleanup, and env_message are required methods. &quot;&quot;&quot; actions = [0] def __init__(self): reward = None observation = None termination = None self.reward_obs_term = (reward, observation, termination) self.count = 0 self.arms = [] self.seed = None def env_init(self, env_info={}): &quot;&quot;&quot;Setup for the environment called when the experiment first starts. Note: Initialize a tuple with the reward, first state observation, boolean indicating if it&#39;s terminal. &quot;&quot;&quot; self.arms = np.random.randn(10)#[np.random.normal(0.0, 1.0) for _ in range(10)] local_observation = 0 # An empty NumPy array self.reward_obs_term = (0.0, local_observation, False) def env_start(self): &quot;&quot;&quot;The first method called when the experiment starts, called before the agent starts. Returns: The first state observation from the environment. &quot;&quot;&quot; return self.reward_obs_term[1] def env_step(self, action): &quot;&quot;&quot;A step taken by the environment. Args: action: The action taken by the agent Returns: (float, state, Boolean): a tuple of the reward, state observation, and boolean indicating if it&#39;s terminal. &quot;&quot;&quot; # if action == 0: # if np.random.random() &lt; 0.2: # reward = 14 # else: # reward = 6 # if action == 1: # reward = np.random.choice(range(10,14)) # if action == 2: # if np.random.random() &lt; 0.8: # reward = 174 # else: # reward = 7 # reward = np.random.normal(self.arms[action], 1.0) reward = self.arms[action] + np.random.randn() obs = self.reward_obs_term[1] self.reward_obs_term = (reward, obs, False) return self.reward_obs_term def env_cleanup(self): &quot;&quot;&quot;Cleanup done after the environment ends&quot;&quot;&quot; pass def env_message(self, message): &quot;&quot;&quot;A message asking the environment for information Args: message (string): the message passed to the environment Returns: string: the response (or answer) to the message &quot;&quot;&quot; if message == &quot;what is the current reward?&quot;: return &quot;{}&quot;.format(self.reward_obs_term[0]) # else return &quot;I don&#39;t know how to respond to your message&quot; . Here, we create RLGlue which was originally designed by Adam White, Brian Tanner, and Rich Sutton. This library will give you a solid framework to understand how reinforcement learning experiments work and how to run your own. . class RLGlue: &quot;&quot;&quot;RLGlue class args: env_name (string): the name of the module where the Environment class can be found agent_name (string): the name of the module where the Agent class can be found &quot;&quot;&quot; def __init__(self, env_class, agent_class): self.environment = env_class() self.agent = agent_class() self.total_reward = None self.last_action = None self.num_steps = None self.num_episodes = None def rl_init(self, agent_init_info={}, env_init_info={}): &quot;&quot;&quot;Initial method called when RLGlue experiment is created&quot;&quot;&quot; self.environment.env_init(env_init_info) self.agent.agent_init(agent_init_info) self.total_reward = 0.0 self.num_steps = 0 self.num_episodes = 0 def rl_start(self, agent_start_info={}, env_start_info={}): &quot;&quot;&quot;Starts RLGlue experiment Returns: tuple: (state, action) &quot;&quot;&quot; last_state = self.environment.env_start() self.last_action = self.agent.agent_start(last_state) observation = (last_state, self.last_action) return observation def rl_agent_start(self, observation): &quot;&quot;&quot;Starts the agent. Args: observation: The first observation from the environment Returns: The action taken by the agent. &quot;&quot;&quot; return self.agent.agent_start(observation) def rl_agent_step(self, reward, observation): &quot;&quot;&quot;Step taken by the agent Args: reward (float): the last reward the agent received for taking the last action. observation : the state observation the agent receives from the environment. Returns: The action taken by the agent. &quot;&quot;&quot; return self.agent.agent_step(reward, observation) def rl_agent_end(self, reward): &quot;&quot;&quot;Run when the agent terminates Args: reward (float): the reward the agent received when terminating &quot;&quot;&quot; self.agent.agent_end(reward) def rl_env_start(self): &quot;&quot;&quot;Starts RL-Glue environment. Returns: (float, state, Boolean): reward, state observation, boolean indicating termination &quot;&quot;&quot; self.total_reward = 0.0 self.num_steps = 1 this_observation = self.environment.env_start() return this_observation def rl_env_step(self, action): &quot;&quot;&quot;Step taken by the environment based on action from agent Args: action: Action taken by agent. Returns: (float, state, Boolean): reward, state observation, boolean indicating termination. &quot;&quot;&quot; ro = self.environment.env_step(action) (this_reward, _, terminal) = ro self.total_reward += this_reward if terminal: self.num_episodes += 1 else: self.num_steps += 1 return ro def rl_step(self): &quot;&quot;&quot;Step taken by RLGlue, takes environment step and either step or end by agent. Returns: (float, state, action, Boolean): reward, last state observation, last action, boolean indicating termination &quot;&quot;&quot; (reward, last_state, term) = self.environment.env_step(self.last_action) self.total_reward += reward if term: self.num_episodes += 1 self.agent.agent_end(reward) roat = (reward, last_state, None, term) else: self.num_steps += 1 self.last_action = self.agent.agent_step(reward, last_state) roat = (reward, last_state, self.last_action, term) return roat def rl_cleanup(self): &quot;&quot;&quot;Cleanup done at end of experiment.&quot;&quot;&quot; self.environment.env_cleanup() self.agent.agent_cleanup() def rl_agent_message(self, message): &quot;&quot;&quot;Message passed to communicate with agent during experiment Args: message: the message (or question) to send to the agent Returns: The message back (or answer) from the agent &quot;&quot;&quot; return self.agent.agent_message(message) def rl_env_message(self, message): &quot;&quot;&quot;Message passed to communicate with environment during experiment Args: message: the message (or question) to send to the environment Returns: The message back (or answer) from the environment &quot;&quot;&quot; return self.environment.env_message(message) def rl_episode(self, max_steps_this_episode): &quot;&quot;&quot;Runs an RLGlue episode Args: max_steps_this_episode (Int): the maximum steps for the experiment to run in an episode Returns: Boolean: if the episode should terminate &quot;&quot;&quot; is_terminal = False self.rl_start() while (not is_terminal) and ((max_steps_this_episode == 0) or (self.num_steps &lt; max_steps_this_episode)): rl_step_result = self.rl_step() is_terminal = rl_step_result[3] return is_terminal def rl_return(self): &quot;&quot;&quot;The total reward Returns: float: the total reward &quot;&quot;&quot; return self.total_reward def rl_num_steps(self): &quot;&quot;&quot;The total number of steps taken Returns: Int: the total number of steps taken &quot;&quot;&quot; return self.num_steps def rl_num_episodes(self): &quot;&quot;&quot;The number of episodes Returns Int: the total number of episodes &quot;&quot;&quot; return self.num_episodes . %matplotlib inline import numpy as np import matplotlib.pyplot as plt from tqdm import tqdm import time . Section 1: Greedy Agent . We want to create an agent that will find the action with the highest expected reward. One way an agent could operate is to always choose the action with the highest value based on the agent’s current estimates. This is called a greedy agent as it greedily chooses the action that it thinks has the highest value. Let&#39;s look at what happens in this case. . First we are going to implement the argmax function, which takes in a list of action values and returns an action with the highest value. Why are we implementing our own instead of using the argmax function that numpy uses? Numpy&#39;s argmax function returns the first instance of the highest value. We do not want that to happen as it biases the agent to choose a specific action in the case of ties. Instead we want to break ties between the highest values randomly. So we are going to implement our own argmax function. You may want to look at np.random.choice to randomly select from a list of values. . def argmax(q_values): &quot;&quot;&quot; Takes in a list of q_values and returns the index of the item with the highest value. Breaks ties randomly. returns: int - the index of the highest value in q_values &quot;&quot;&quot; top_value = float(&quot;-inf&quot;) ties = [] for i in range(len(q_values)): # if a value in q_values is greater than the highest value update top and reset ties to zero # if a value is equal to top value add the index to ties # return a random selection from ties. # YOUR CODE HERE if q_values[i] &gt; top_value: top_value = q_values[i] ties = [i] elif q_values[i] == top_value: ties.append(i) return np.random.choice(ties) . Next, we are going to create a GreedyAgent and implement the agent_step method. This method gets called each time the agent takes a step. The method has to return the action selected by the agent. This method also ensures the agent’s estimates are updated based on the signals it gets from the environment. . class GreedyAgent(Agent): def agent_step(self, reward, observation=None): &quot;&quot;&quot; Takes one step for the agent. It takes in a reward and observation and returns the action the agent chooses at that time step. Arguments: reward -- float, the reward the agent recieved from the environment after taking the last action. observation -- float, the observed state the agent is in. Do not worry about this as you will not use it until future lessons Returns: current_action -- int, the action chosen by the agent at the current time step. &quot;&quot;&quot; ### Useful Class Variables ### # self.q_values : An array with what the agent believes each of the values of the arm are. # self.arm_count : An array with a count of the number of times each arm has been pulled. # self.last_action : The action that the agent took on the previous time step ####################### # Update Q values Hint: Look at the algorithm in section 2.4 of the textbook. # increment the counter in self.arm_count for the action from the previous time step # update the step size using self.arm_count # update self.q_values for the action from the previous time step self.arm_count[self.last_action] += 1 self.q_values[self.last_action] += (reward - self.q_values[self.last_action]) / self.arm_count[self.last_action] current_action = argmax(self.q_values) self.last_action = current_action return current_action . Then, let&#39;s visualize the result . num_runs = 200 # The number of times we run the experiment num_steps = 1000 # The number of pulls of each arm the agent takes env = Environment # We set what environment we want to use to test agent = GreedyAgent # We choose what agent we want to use agent_info = {&quot;num_actions&quot;: 10} # We pass the agent the information it needs. Here how many arms there are. env_info = {} # We pass the environment the information it needs. In this case nothing. rewards = np.zeros((num_runs, num_steps)) average_best = 0 for run in tqdm(range(num_runs)): # tqdm is what creates the progress bar below np.random.seed(run) rl_glue = RLGlue(env, agent) # Creates a new RLGlue experiment with the env and agent we chose above rl_glue.rl_init(agent_info, env_info) # We pass RLGlue what it needs to initialize the agent and environment rl_glue.rl_start() # We start the experiment average_best += np.max(rl_glue.environment.arms) for i in range(num_steps): reward, _, action, _ = rl_glue.rl_step() # The environment and agent take a step and return # the reward, and action taken. rewards[run, i] = reward greedy_scores = np.mean(rewards, axis=0) plt.figure(figsize=(15, 5), dpi= 80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.plot([average_best / num_runs for _ in range(num_steps)], linestyle=&quot;--&quot;) plt.plot(greedy_scores) plt.legend([&quot;Best Possible&quot;, &quot;Greedy&quot;]) plt.title(&quot;Average Reward of Greedy Agent&quot;) plt.xlabel(&quot;Steps&quot;) plt.ylabel(&quot;Average reward&quot;) plt.show() . 100%|██████████| 200/200 [00:03&lt;00:00, 63.50it/s] . Section 2: Epsilon-Greedy Agent . We noticed about a trade off between Exploitation and Exploration, where it does not always take the greedy action. Instead, sometimes it takes an exploratory action. It does this so that it can find out what the best action really is. If we always choose what we think is the current best action is, we may miss out on taking the true best action, because we haven&#39;t explored enough times to find that best action. . Implement an epsilon-greedy agent below. Hint: we are implementing the algorithm from section 2.4 of this textbook. You may want to use your greedy code from above and look at np.random.random, as well as np.random.randint, to help you select random actions. . class EpsilonGreedyAgent(Agent): def agent_step(self, reward, observation): &quot;&quot;&quot; Takes one step for the agent. It takes in a reward and observation and returns the action the agent chooses at that time step. Arguments: reward -- float, the reward the agent recieved from the environment after taking the last action. observation -- float, the observed state the agent is in. Do not worry about this as you will not use it until future lessons Returns: current_action -- int, the action chosen by the agent at the current time step. &quot;&quot;&quot; ### Useful Class Variables ### # self.q_values : An array with what the agent believes each of the values of the arm are. # self.arm_count : An array with a count of the number of times each arm has been pulled. # self.last_action : The action that the agent took on the previous time step # self.epsilon : The probability an epsilon greedy agent will explore (ranges between 0 and 1) ####################### # Update Q values - this should be the same update as your greedy agent above self.arm_count[self.last_action] += 1 self.q_values[self.last_action] += (reward - self.q_values[self.last_action]) / self.arm_count[self.last_action] # Choose action using epsilon greedy # Randomly choose a number between 0 and 1 and see if it&#39;s less than self.epsilon if np.random.random() &lt; self.epsilon: current_action = np.random.randint(len(self.q_values)) else: current_action = argmax(self.q_values) self.last_action = current_action return current_action . Now that we have our epsilon greedy agent created. Let&#39;s compare it against the greedy agent with epsilon of 0.1. . num_runs = 200 num_steps = 1000 epsilon = 0.1 agent = EpsilonGreedyAgent env = Environment # ten arms agent_info = {&quot;num_actions&quot;: 10, &quot;epsilon&quot;: epsilon} env_info = {} all_rewards = np.zeros((num_runs, num_steps)) for run in tqdm(range(num_runs)): np.random.seed(run) rl_glue = RLGlue(env, agent) rl_glue.rl_init(agent_info, env_info) rl_glue.rl_start() for i in range(num_steps): reward, _, action, _ = rl_glue.rl_step() # The environment and agent take a step and return # the reward, and action taken. all_rewards[run, i] = reward # take the mean over runs scores = np.mean(all_rewards, axis=0) plt.figure(figsize=(15, 5), dpi= 80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.plot([1.55 for _ in range(num_steps)], linestyle=&quot;--&quot;) plt.plot(greedy_scores) plt.title(&quot;Average Reward of Greedy Agent vs. E-Greedy Agent&quot;) plt.plot(scores) plt.legend((&quot;Best Possible&quot;, &quot;Greedy&quot;, &quot;Epsilon: 0.1&quot;)) plt.xlabel(&quot;Steps&quot;) plt.ylabel(&quot;Average reward&quot;) plt.show() . 100%|██████████| 200/200 [00:03&lt;00:00, 63.69it/s] . Here, we noticed how much better the epsilon-greedy agent did. Because we occasionally choose a random action we were able to find a better long term policy. By acting greedily before our value estimates are accurate, we risk settling on a suboptimal action. . Section 3: Comparing values of epsilon . Can we do better than an epsilon of 0.1? Let&#39;s try several different values for epsilon and see how they perform. We try different settings of key performance parameters to understand how the agent might perform under different conditions. . Below we run an experiment where we sweep over different values for epsilon: . epsilons = [0.0, 0.01, 0.1, 0.4] plt.figure(figsize=(15, 5), dpi= 80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.plot([1.55 for _ in range(num_steps)], linestyle=&quot;--&quot;) n_q_values = [] n_averages = [] n_best_actions = [] num_runs = 200 for epsilon in epsilons: all_averages = [] for run in tqdm(range(num_runs)): agent = EpsilonGreedyAgent agent_info = {&quot;num_actions&quot;: 10, &quot;epsilon&quot;: epsilon} env_info = {&quot;random_seed&quot;: run} rl_glue = RLGlue(env, agent) rl_glue.rl_init(agent_info, env_info) rl_glue.rl_start() best_arm = np.argmax(rl_glue.environment.arms) scores = [0] averages = [] best_action_chosen = [] for i in range(num_steps): reward, state, action, is_terminal = rl_glue.rl_step() scores.append(scores[-1] + reward) averages.append(scores[-1] / (i + 1)) if action == best_arm: best_action_chosen.append(1) else: best_action_chosen.append(0) if epsilon == 0.1 and run == 0: n_q_values.append(np.copy(rl_glue.agent.q_values)) if epsilon == 0.1: n_averages.append(averages) n_best_actions.append(best_action_chosen) all_averages.append(averages) plt.plot(np.mean(all_averages, axis=0)) plt.legend([&quot;Best Possible&quot;] + epsilons) plt.xlabel(&quot;Steps&quot;) plt.ylabel(&quot;Average reward&quot;) plt.show() . 100%|██████████| 200/200 [00:04&lt;00:00, 48.39it/s] 100%|██████████| 200/200 [00:04&lt;00:00, 48.03it/s] 100%|██████████| 200/200 [00:03&lt;00:00, 55.24it/s] 100%|██████████| 200/200 [00:02&lt;00:00, 67.29it/s] . Section 4: The Effect of Step Size . In Section 1, we decayed the step size over time based on action-selection counts. The step-size was 1/N(A), where N(A) is the number of times action A was selected. This is the same as computing a sample average. We could also set the step size to be a constant value, such as 0.1. What would be the effect of doing that? And is it better to use a constant or the sample average method? . To investigate this question, let’s start by creating a new agent that has a constant step size. This will be nearly identical to the agent created above. You will use the same code to select the epsilon-greedy action. You will change the update to have a constant step size instead of using the 1/N(A) update. . class EpsilonGreedyAgentConstantStepsize(Agent): def agent_step(self, reward, observation): &quot;&quot;&quot; Takes one step for the agent. It takes in a reward and observation and returns the action the agent chooses at that time step. Arguments: reward -- float, the reward the agent recieved from the environment after taking the last action. observation -- float, the observed state the agent is in. Do not worry about this as you will not use it until future lessons Returns: current_action -- int, the action chosen by the agent at the current time step. &quot;&quot;&quot; ### Useful Class Variables ### # self.q_values : An array with what the agent believes each of the values of the arm are. # self.arm_count : An array with a count of the number of times each arm has been pulled. # self.last_action : An int of the action that the agent took on the previous time step. # self.step_size : A float which is the current step size for the agent. # self.epsilon : The probability an epsilon greedy agent will explore (ranges between 0 and 1) ####################### # Update q_values for action taken at previous time step # using self.step_size intead of using self.arm_count self.arm_count[self.last_action] += 1 self.q_values[self.last_action] += self.step_size * (reward - self.q_values[self.last_action]) # Choose action using epsilon greedy. This is the same as you implemented above. if np.random.random() &lt; self.epsilon: current_action = np.random.randint(len(self.q_values)) else: current_action = argmax(self.q_values) self.last_action = current_action return current_action . step_sizes = [0.01, 0.1, 0.5, 1.0, &#39;1/N(A)&#39;] epsilon = 0.1 num_steps = 1000 num_runs = 200 fig, ax = plt.subplots(figsize=(15, 5), dpi= 80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) q_values = {step_size: [] for step_size in step_sizes} true_values = {step_size: None for step_size in step_sizes} best_actions = {step_size: [] for step_size in step_sizes} for step_size in step_sizes: all_averages = [] for run in tqdm(range(num_runs)): np.random.seed(run) agent = EpsilonGreedyAgentConstantStepsize if step_size != &#39;1/N(A)&#39; else EpsilonGreedyAgent agent_info = {&quot;num_actions&quot;: 10, &quot;epsilon&quot;: epsilon, &quot;step_size&quot;: step_size, &quot;initial_value&quot;: 0.0} env_info = {} rl_glue = RLGlue(env, agent) rl_glue.rl_init(agent_info, env_info) rl_glue.rl_start() best_arm = np.argmax(rl_glue.environment.arms) if run == 0: true_values[step_size] = np.copy(rl_glue.environment.arms) best_action_chosen = [] for i in range(num_steps): reward, state, action, is_terminal = rl_glue.rl_step() if action == best_arm: best_action_chosen.append(1) else: best_action_chosen.append(0) if run == 0: q_values[step_size].append(np.copy(rl_glue.agent.q_values)) best_actions[step_size].append(best_action_chosen) ax.plot(np.mean(best_actions[step_size], axis=0)) plt.legend(step_sizes) plt.title(&quot;% Best Arm Pulled&quot;) plt.xlabel(&quot;Steps&quot;) plt.ylabel(&quot;% Best Arm Pulled&quot;) vals = ax.get_yticks() ax.set_yticklabels([&#39;{:,.2%}&#39;.format(x) for x in vals]) plt.show() . 100%|██████████| 200/200 [00:03&lt;00:00, 56.86it/s] 100%|██████████| 200/200 [00:03&lt;00:00, 60.23it/s] 100%|██████████| 200/200 [00:03&lt;00:00, 59.37it/s] 100%|██████████| 200/200 [00:03&lt;00:00, 59.35it/s] 100%|██████████| 200/200 [00:03&lt;00:00, 56.94it/s] C: Users ND258645 AppData Local Temp ipykernel_15840 3358721971.py:48: UserWarning: FixedFormatter should only be used together with FixedLocator ax.set_yticklabels([&#39;{:,.2%}&#39;.format(x) for x in vals]) . Notice first that we are now plotting the amount of time that the best action is taken rather than the average reward. To better understand the performance of an agent, it can be useful to measure specific behaviors, beyond just how much reward is accumulated. This measure indicates how close the agent’s behaviour is to optimal. . It seems as though 1/N(A) performed better than the others, in that it reaches a solution where it takes the best action most frequently. Now why might this be? Why did a step size of 0.5 start out better but end up performing worse? Why did a step size of 0.01 perform so poorly? . Let&#39;s dig into this further below. Let’s plot how well each agent tracks the true value, where each agent has a different step size method. You do not have to enter any code here, just follow along. . largest = 0 num_steps = 1000 for step_size in step_sizes: plt.figure(figsize=(15, 5), dpi= 80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) largest = np.argmax(true_values[step_size]) plt.plot([true_values[step_size][largest] for _ in range(num_steps)], linestyle=&quot;--&quot;) plt.title(&quot;Step Size: {}&quot;.format(step_size)) plt.plot(np.array(q_values[step_size])[:, largest]) plt.legend([&quot;True Expected Value&quot;, &quot;Estimated Value&quot;]) plt.xlabel(&quot;Steps&quot;) plt.ylabel(&quot;Value&quot;) plt.show() . These plots help clarify the performance differences between the different step sizes. A step size of 0.01 makes such small updates that the agent’s value estimate of the best action does not get close to the actual value. Step sizes of 0.5 and 1.0 both get close to the true value quickly, but are very susceptible to stochasticity in the rewards. The updates overcorrect too much towards recent rewards, and so oscillate around the true value. This means that on many steps, the action that pulls the best arm may seem worse than it actually is. A step size of 0.1 updates fairly quickly to the true value, and does not oscillate as widely around the true values as 0.5 and 1.0. This is one of the reasons that 0.1 performs quite well. Finally we see why 1/N(A) performed well. Early on while the step size is still reasonably high it moves quickly to the true expected value, but as it gets pulled more its step size is reduced which makes it less susceptible to the stochasticity of the rewards. . Does this mean that 1/N(A) is always the best? When might it not be? One possible setting where it might not be as effective is in non-stationary problems. You learned about non-stationarity in the lessons. Non-stationarity means that the environment may change over time. This could manifest itself as continual change over time of the environment, or a sudden change in the environment. . Let&#39;s look at how a sudden change in the reward distributions affects a step size like 1/N(A). This time we will run the environment for 2000 steps, and after 1000 steps we will randomly change the expected value of all of the arms. We compare two agents, both using epsilon-greedy with epsilon = 0.1. One uses a constant step size of 0.1, the other a step size of 1/N(A) that reduces over time. . epsilon = 0.1 num_steps = 2000 num_runs = 500 step_size = 0.1 plt.figure(figsize=(15, 5), dpi= 80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.plot([1.55 for _ in range(num_steps)], linestyle=&quot;--&quot;) for agent in [EpsilonGreedyAgent, EpsilonGreedyAgentConstantStepsize]: rewards = np.zeros((num_runs, num_steps)) for run in tqdm(range(num_runs)): agent_info = {&quot;num_actions&quot;: 10, &quot;epsilon&quot;: epsilon, &quot;step_size&quot;: step_size} np.random.seed(run) rl_glue = RLGlue(env, agent) rl_glue.rl_init(agent_info, env_info) rl_glue.rl_start() for i in range(num_steps): reward, state, action, is_terminal = rl_glue.rl_step() rewards[run, i] = reward if i == 1000: rl_glue.environment.arms = np.random.randn(10) plt.plot(np.mean(rewards, axis=0)) plt.legend([&quot;Best Possible&quot;, &quot;1/N(A)&quot;, &quot;0.1&quot;]) plt.xlabel(&quot;Steps&quot;) plt.ylabel(&quot;Average reward&quot;) plt.show() . 100%|██████████| 500/500 [00:14&lt;00:00, 34.20it/s] 100%|██████████| 500/500 [00:18&lt;00:00, 27.43it/s] . Now the agent with a step size of 1/N(A) performed better at the start but then performed worse when the environment changed! What happened? . Think about what the step size would be after 1000 steps. Let&#39;s say the best action gets chosen 500 times. That means the step size for that action is 1/500 or 0.002. At each step when we update the value of the action and the value is going to move only 0.002 * the error. That is a very tiny adjustment and it will take a long time for it to get to the true value. . The agent with step size 0.1, however, will always update in 1/10th of the direction of the error. This means that on average it will take ten steps for it to update its value to the sample mean. . These are the types of tradeoffs we have to think about in reinforcement learning. A larger step size moves us more quickly toward the true value, but can make our estimated values oscillate around the expected value. A step size that reduces over time can converge to close to the expected value, without oscillating. On the other hand, such a decaying stepsize is not able to adapt to changes in the environment. Nonstationarityand the related concept of partial observabilityis a common feature of reinforcement learning problems and when learning online. .",
            "url": "https://dnlam.github.io/fastblog/2020/06/22/Bandits_Exploration_Exploitation.html",
            "relUrl": "/2020/06/22/Bandits_Exploration_Exploitation.html",
            "date": " • Jun 22, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://dnlam.github.io/fastblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there, I am Lam DINH (Ngoc-Lam DINH), I graduated from Ha Noi University of Science and Technology (Vietnam) in 2016 with a degree in Electronics and Telecommunications Engineering. My training mainly focused on several areas: Digital Signal Processing, Wireless Communication and Embedded Programming. Then, I continued my study with a special interest in Signal Theory, Wireless Telecommunications and Optical Networks at Universidad Politecnica de Valencia (Spain) in 2017. . My Master’s degree is jointly awarded by the École Normale Supérieure Paris Saclay (France) and the Universidad Complutense de Madrid (Spain) in 2019. The courses are related to the application of molecular photonics to telecommunications and biosensors. . From 2019, I am conducting a PhD thesis at the Commissariat à l’Énergie Atomique (CEA) in Grenoble. My research addresses ultra-reliable and low-latency communications (URLLC) in 5G systems and beyond. . More Information . Personal Web Page .",
          "url": "https://dnlam.github.io/fastblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://dnlam.github.io/fastblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
{
  
    
        "post0": {
            "title": "Introduction of FastAI",
            "content": "1. What is FastAI? . FasiAI is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and low-level components that can be mixed and matched to build new approaches. . FastAI libraries include: . a dispatch system for Python along with a semantic type hierarchy for tensors | a GPU-optimized computer vision library | an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4-5 lines of code. | a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training | a new data block API | . The design of FastAI follows layered structure where we want the clarity and development speed of Keras and the customizability of PyTorch, which is not possible to be achieved both for the other frameworks. . FastAI was co-founded by Jeremy Howard, who is a data scientist, researcher, developer, educator, and entrepreneur, and Rachel Thomas, who is a professor of practice at Queensland University of Technology. . 1.1. FastAI by example . Let&#39;s go deeper to their FastAI codes to see how it works. Here is an example of how to fine-tune an ImageNet model on the Oxford IIT Pets dataset and achieve close to state-of-the-art accuracy within a couple of minutes of training on a single GPU. . from fastai.vision.all import * path = untar_data(URLs.PETS)/&#39;images&#39; . def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func(path, get_image_files(path), valid_pct= 0.2, seed= 42, label_func= is_cat, item_tfms= Resize(224)) learn = cnn_learner(dls, resnet34, metrics= error_rate) learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.176250 | 0.017708 | 0.007442 | 01:00 | . epoch train_loss valid_loss error_rate time . 0 | 0.062521 | 0.018719 | 0.005413 | 01:01 | . Each line of given code does one important task: . The second line (path = untar_data(URLs.PETS)/&#39;images&#39;) downloads a standard dataset from the fast.ai datasets collection (if not previously downloaded) to a configurable location (~/.fastai/data by default), extracts it (if not previously extracted), and returns a pathlib.Path object with the extracted location. | Then, dls=ImageDataLoaders.from_name_func(...) sets up the DataLoaders object and represents a combination of training and validation data. | . After defining DataLoaders object, we can easily look at the data with a single line of code. . dls.show_batch() . Let&#39;s analyze the parameters inside the ImageDataLoader: . valid_pct is the percentage of validation set compared to training set to avoid over-fitting. By defaults, valid_pct=0.2. As being quoted by Jeremy &quot;Overfitting is the single most important and challenging issue. It is easy to create a model that does a great job at making predictions on the exact data it has been trained on, but it is much harder to make accurate predictions on data the model has never seen before.&quot; | . Afterwards, we created a Learner, which provides an abstraction combining an optimizer, a model, and the data to train. THis line of code learn = cnn_learner(dls, resnet34, metrics= error_rate) will download an ImageNet-pretrained model, if not already available, remove the classification head of the model, and set appropriate defaults for the optimizer, weight decay, learning rate and so forth. . Basically, a Learner contains our data (i.e dls), architecture (i.e. resnet34) which is a mathematical function that we are optimizing and a metrics (i.e, error_rate). a Learner will figure out which are the best parameters for the architecture to match the label in the dataset. . When we are talking about the metrics, which is a function that measures the quality of the model’s predictions using the validation set, it should be noted that the metrics is not necessarily the same as loss. The loss measures how parameters changing influences the results of performances (better or worse). . To fits the model, the next line of code learn.fine_tune(1) tells us how to do. Model fitting is simply looking at how many times to look at each image (epochs). Instead of using fit, we use fine_tune method because we started with a pre-trained model and we don&#39; want to throw away all the capabilities that it already has. By performing fine_tune, the parameters of a pretrained model are updated by training for additional epochs using a different task to that used for pretraining. . In sums, fine_tune is used for transfer learning, in which we used a pretrained model for a task different to what is was originally trained for. . 2. Foudation of .",
            "url": "https://dnlam.github.io/fastblog/2022/03/16/_intro_fastai.html",
            "relUrl": "/2022/03/16/_intro_fastai.html",
            "date": " • Mar 16, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "The Beauty of Neural Network",
            "content": "The Sample Problem . Consider a system that tries to map groups of 3 inputs to some corresponding output which is a single number. In the following picture, we&#39;ll show each set of 3 inputs as a row of a matrix $X$, and each output as the corresponding row of $Y$: . $$ overbrace{ left[ { begin{array}{ccc} 0 &amp; 0 &amp; 1 0 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 end{array} } right] }^{X} rightarrow overbrace{ left[ { begin{array}{c} 0 0 1 1 end{array} } right] }^Y. $$Even though this system has an exact solution (namely, $Y$ equals the first column of $X$), usually we&#39;ll need to be satisfied with a system that maps our inputs $X$ to some approximate &quot;prediction&quot; $ tilde{Y}$, which we hope to bring closer to the &quot;target&quot; $Y$ by means of successive improvements. . The way we&#39;ll get our prediction $ tilde{Y}$ is by means of a weighted sum of each set of 3 inputs, and some nonlinear function $f$ which we call the &quot;activation function&quot; (or just &quot;activation&quot;). Pictorially, the process looks like the following, for each row $i$ of $X$ and $Y$, (where the columns of $X$ are shown arranged vertically instead of horizonally): . . In terms of matrix multiplication, since X is a 4x3 matrix, and Y is a 4x1 matrix, that implies that our weights should be a 3x1 matrix consisting of (unknown) values $w_0$, $w_1$ and $w_2$. The calculation can be written as: . $$ f left( overbrace{ left[ { begin{array}{ccc} 0 &amp; 0 &amp; 1 0 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 end{array} } right] }^ text{X} overbrace{ left[ { begin{array}{c} w_0 w_1 w_2 end{array} } right] }^{w} right) = overbrace{ left[ { begin{array}{c} 0 0 1 1 end{array} } right] }^{ tilde{Y}} $$Our nonlinear activation function $f$ is taken to operate on each row element one at a time, and we&#39;ll let $f_i$ denote the $i$th row of this completed activation, i.e.: . $$ f_i = f left( sum_j X_{ij}w_j right) = tilde{Y}_i . $$The particular activation function we will use is the &quot;sigmoid&quot;, . $$ f(x) = {1 over{1+e^{-x}}}, $$-- click here to see a plot of this function -- which has the derivative . $$ {df over dx} = {e^{-x} over(1 + e^{-x})^2} $$which can be shown (Hint: exercise for &quot;mathy&quot; students!) to simplify to $$ {df over dx}= f(1-f). $$ . The overall problem then amounts to finding the values of the &quot;weights&quot; $w_0, w_1,$ and $w_2$ so that the $ tilde{Y}$ we calculate is as close to the target $Y$ as possible. . To do this, we will seek to minimize a loss function defined as a sum across all data points we have, i.e. all 4 rows. The loss function $L$ we will choose is the mean square error loss, or MSE, we will use a &#39;better&#39; loss function for this problem): . $$ L = {1 over N} sum_{i=0}^{N-1} left[ tilde{Y}_i - Y_i right]^2, $$or in terms of the activation function $$ L = {1 over N} sum_{i=0}^{N-1} left[ f_i - Y_i right]^2. $$ . Each of the weights $w_j$ ($j=0..2$) will start with random values, and then be updated via gradient descent, i.e. . $$ w_j^{new} = w_j^{old} - alpha{ partial L over partial w_j} $$where $ alpha$ is the learning rate, chosen to be some small parameter. For the MSE loss shown above, the partial derivatives with respect to each of the weights is . $$ { partial L over partial w_j} = {2 over N} sum_{i=0}^{N-1} left[ tilde{Y}_i - Y_i right]{ partial f_i over partial w_j} = {2 over N} sum_{i=0}^{N-1} left[ tilde{Y}_i - Y_i right]f_i(1-f_i)X_{ij}. $$Absorbing the factor of 2/N into our choice of $ alpha$, and writing the summation as a dot product, and noting that $f_i = tilde{Y}_i$, we can write the update for all the weights together as . $$ w = w - alpha X^T cdot left( [ tilde{Y}-Y]* tilde{Y}*(1- tilde{Y}) right) $$where the $ cdot$ denotes a matrix-matrix product (i.e. a dot product for successive rows of $X^T$) and $*$ denotes elementwise multiplication. . To clarify the above expression in terms of matrix dimensions, we can see that $w$, a 3x1 matrix, can be made by multipyting $X^T$ (a 3 x4 matrix) with the term in parentheses, i.e. the product of elementwise terms involving $ tilde{Y}$, which is a 4x1 matrix. In other words, a 3x4 matrix, times a 4x1 matrix, yields a 3x1 matrix. . Actual Code . The full code for all of this is then... . # https://iamtrask.github.io/2015/07/12/basic-python-network/ import numpy as np # sigmoid activation def sigmoid(x,deriv=False): if(deriv==True): return x*(1-x) return 1/(1+np.exp(-x)) # input dataset X = np.array([ [0,0,1], [0,1,1], [1,0,1], [1,1,1] ]) # target output dataset Y = np.array([[0,0,1,1]]).T # seed random numbers to make calculation # deterministic (just a good practice) np.random.seed(1) # initialize weights randomly with mean 0 w = 2*np.random.random((3,1)) - 1 alpha = 1.0 # learning rate loss_history = [] # keep a record of how the loss proceeded, blank for now for iter in range(1000): # forward propagation Y_pred = sigmoid(np.dot(X,w)) # prediction, i.e. tilde{Y} # how much did we miss? diff = Y_pred - Y loss_history.append((diff**2).mean()) # add to the history of the loss # update weights w -= alpha * np.dot( X.T, diff*sigmoid(Y_pred, deriv=True)) print(&quot;Output After Training:&quot;) print(&quot;Y_pred = (should be two 0&#39;s followed by two 1&#39;s) n&quot;,Y_pred) print(&quot;weights = n&quot;,w) . Output After Training: Y_pred = (should be two 0&#39;s followed by two 1&#39;s) [[0.03178421] [0.02576499] [0.97906682] [0.97414645]] weights = [[ 7.26283009] [-0.21614618] [-3.41703015]] . Note that, because of our nonlinear activation, we don&#39;t get the solution $w_0=1, w_1=0, w_2=0$. . Plotting the loss vs. iteration number, we see... . %matplotlib inline import matplotlib.pyplot as plt plt.loglog(loss_history) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.show() . Change the activation function . Another popular choice of activation function is the rectified linear unit or ReLU. The function ReLU(x) is zero for x &lt;= 0, and equal to x (i.e. a straight line at 45 degrees for) x &gt;0. It can be written as max(x,0) or x * (x&gt;0), and its derivative is 1 for positive x, and zero otherwise. . Click here to see a graph of ReLU . Modifying our earlier code to use ReLU activation instead of sigmoid looks like this: . def relu(x,deriv=False): # relu activation if(deriv==True): return 1*(x&gt;0) return x*(x&gt;0) # seed random numbers to make calculation # deterministic (just a good practice) np.random.seed(1) # initialize weights randomly (but only &gt;0 because ReLU clips otherwise) w = np.random.random((3,1)) alpha = 0.3 # learning rate new_loss_history = [] # keep a record of how the error proceeded for iter in range(1000): # forward propagation Y_pred = relu(np.dot(X,w)) # how much did we miss? diff = Y_pred - Y new_loss_history.append((diff**2).mean()) # add to the record of the loss # update weights w -= alpha * np.dot( X.T, diff*relu(Y_pred, deriv=True)) print(&quot;Output After Training:&quot;) print(&quot;Y_pred = (should be two 0&#39;s followed by two 1&#39;s) n&quot;,Y_pred) print(&quot;weights = n&quot;,w) . Output After Training: Y_pred = (should be two 0&#39;s followed by two 1&#39;s) [[-0.] [-0.] [ 1.] [ 1.]] weights = [[ 1.01784368e+00] [ 8.53961786e-17] [-1.78436793e-02]] . print( w[2] - (1-w[0]) ) . [-3.46944695e-17] . Plot old results with new results: . %matplotlib inline import matplotlib.pyplot as plt plt.loglog(loss_history,label=&quot;sigmoid&quot;) plt.loglog(new_loss_history,label=&quot;relu&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.legend() plt.show() . Looks like ReLU may be a better choice than sigmoid for this problem! . Exercise: Read a 7-segment display . A 7-segment display is used for displaying numerical digits 0 through 9, usually by lighting up LEDs or parts of a liquid crystal display (LCD). The segments are labelled $a$ through $g$ according to the following diagram: . . Diagram of the network . The 7 inputs &quot;a&quot; through &quot;g&quot; will be mapped to 10 outputs for the individual digits, and each output can range from 0 (&quot;false&quot; or &quot;no&quot;) to 1 (&quot;true&quot; or &quot;yes&quot;) for that digit. The input and outputs will be connected by a matrix of weights. Pictorially, this looks like the following (Not shown: activation function $f$): . . ...where again, this network operates on a single data point at a time, datapoints which are rows of X and Y. What is shown in the above diagram are the columns of $X$ and $Y$ for a single row (/ single data point). . Create the dataset . Let the input X be the segments $a$ through $g$ are the columns of the input $X$, and are either 1 for on or 0 for off. Let the columns of the target $Y$ be the digits 0-9 themselves arranged in a &quot;one hot&quot; encoding scheme, as follows: . Digit | One-Hot Encoding for $Y$ | . 0 | 1,0,0,0,0,0,0,0,0,0 | . 1 | 0,1,0,0,0,0,0,0,0,0 | . 2 | 0,0,1,0,0,0,0,0,0,0 | . ... | ... | . 9 | 0,0,0,0,0,0,0,0,0,1 | . The values in the columns for $Y$ are essentially true/false &quot;bits&quot; for each digit, answering the question &quot;Is this digit the appropriate output?&quot; with a &quot;yes&quot;(=1) or &quot;no&quot; (=0) response. . The input $X$ will be a 10x7 matrix, and the target $Y$ will be a 10x10 matrix. Each row of $X$ will be the segments to produce the digit for that row. For example, the zeroth row of $X$ should show segments on which make an image of the digit zero, namely segments a, b, c, d, e, and f but not g, so that the zeroth row of X should be [1,1,1,1,1,1,0]. . Define numpy arrays for both $X$ and $Y$ (Hint: for $Y$, check out np.eye()): . # for the 7-segment display. The following is just a &quot;stub&quot; to get you started. X = np.array([ [1,1,1,1,1,1,0], [], [], [] ]) Y = np.array([ [1,0,0,0,0,0,0,0,0,0], [], [] ]) . Initialize the weights . Previously the dimensions of the weight matrix $w$ were 3x1 because we were mapping each row of 3 elements in $X$ to each row of 1 element of $Y$. For this new problem, each row of $X$ has 7 elements, and we want to map those to the 10 elements in each 1-hot-encoded row of $Y$, so what should the dimensions of the weights matrix $w$ be? . Write some numpy code to randomly initialize the weights matrix: . np.random.seed(1) # initial RNG so everybody gets similar results w = np.random.random(( , )) # Students, fill in the array dimensions here . File &#34;&lt;ipython-input-7-20f53a51cded&gt;&#34;, line 1 w = np.random.random(( , )) ^ SyntaxError: invalid syntax . Train the network . Having created an $X$ and its matching $Y$, and initalized the weights $w$ randomly, train a neural network such as the ones above to learn to map a row of X to a row of Y, i.e. train it to recognize digits on 7-segment displays. Do this below. . # Use sigmoid activation, and 1000 iterations, and learning rate of 0.9 # Question: What happens if you use ReLU instead? Try it later. Is ReLU always the best choice? # And then print out your Y_pred &amp; weights matrix, and limit it to 3 significant digits print(&quot;Output After Training:&quot;) np.set_printoptions(formatter={&#39;float&#39;: lambda x: &quot;{0:0.3f}&quot;.format(x)}) # 3 sig figs print(&quot;Y_pred= n&quot;,Y_pred) print(&quot;weights = n&quot;,repr(w)) # the repr() makes it so it can be copied &amp; pasted back into Python code . Final Check: Keras version . Keras is a neural network library that lets us write NN applications very compactly. Try running the following using the X and Y from your 7-segment dataset: . import keras from keras.models import Sequential from keras.layers import Dense, Activation model = Sequential([ Dense(10, input_shape=(7,)), Activation(&#39;sigmoid&#39;) ]) model.compile(optimizer=&#39;adam&#39;, # We&#39;ll talk about optimizer choices and loss choices later loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.fit(X, Y, epochs=200, batch_size=1) print(&quot; nY_tilde = n&quot;, model.predict(X) ) . Follow-up: Remarks . Re-stating what we just did . The original problem (posed at the top of this notebook) involves mapping some points from a 3-dimensional space into points in a 1-dimensional space, i.e. to points on the number line. The mapping is done by the combination of a weighted sum (a linear operation) and a nonlinear &quot;activation function&quot; applied to that sum. The use of an activation function like a sigmoid was originally intended to serve as an analogy of activation of biological neurons. Nonlinear activation functions are source of the &quot;power&quot; of neural networks (essentially we approximate some other function by means of a sum of basis functions in some function space, but don&#39;t worry about that if you&#39;re not math-inclined). The algorithm &#39;learns&#39; to approximate this operation via supervised learning and gradient descent according to some loss function. We used the mean squared error (MSE) for our loss, but lots and lots of different loss functions could be used, a few of which we&#39;ll look at another time. . Question for reflection: Unlike fitting a line $y = mx+b$, the weighted sum in our models in this notebook had no constant &quot;bias&quot; term like $b$. How might we include such a term? . One thing we glossed over: &quot;batch size&quot; . Question: Should we apply the gradient descent &quot;update&quot; to the weights each time we process a single row of $X$ &amp; $Y$, or should we compute the combined loss of all the rows together at the same time, and then do the update? This is essentially asking the same question as &quot;When fitting a line $mx+b$ to a bunch of data points, should we use all the points together to update $m$ and $b,$ or should we do this one point at a time -- compute the gradients of the loss at one point, update the weights, compute gradients at another point, etc.?&quot; . The number of points you use is called the batch size and it is what&#39;s known as a &quot;hyperparameter&quot; -- it is not part of the model per se, but it is a(n important) choice you make when training the model. The batch size affects the learning as follows: Averaging the gradints for many data points (i..e. a large batch size) will produce a smoother loss function and will also usually make the code execute more quickly through the dataset, but updating the weights for every point will cause the algorithm to learn with fewer iterations. . One quick way to observe this is to go up to the Keras code above and change batch_size from 1 to 10, and re-execute the cell. How is the accuracy after 200 iteractions, compared to when batch_size=1? . Terminology: Technically, it&#39;s called &quot;batch training&quot; when you sum the gradients for all the data points before updating the weights, whereas using fewer points is &quot;minibatch training&quot;, and updating for each point (i.e. each row, for us) is Stochastic Gradient Descent* (SGD -- more on these terms here). In practice, there is a tradeoff between smaller vs. larger (mini)batches, which has been the subject of intense scrutiny by researchers over the years. We will have more to say on this later. . For discussion later: In our presentation above, were we using batch training, minibatch training or SGD? . . . *Note: many people will regard SGD as an optimization algorithm per se, and refer to doing SGD even for (mini)batch sizes larger than 1. . Optional: If you want to go really crazy . How about training on this dataset: $$ overbrace{ left[ { begin{array}{cc} 0 &amp; 0 0 &amp; 1 1 &amp; 0 1 &amp; 1 end{array} } right] }^{X} rightarrow overbrace{ left[ { begin{array}{c} 0 1 1 0 end{array} } right] }^Y. $$ Good luck! ;-) (Hint 1: This problem features prominently in the history of Neural Networks, involving Marvin Minsky and &quot;AI Winter.&quot; Hint 2: This whole lesson could instead be entitled &quot;My First Artificial Neuron.&quot;) . Additional Optional Exercise: Binary Math vs. One-Hot Encoding . For the 7-segment display, we used a one-hot encoding for our output, namely a set of true/false &quot;bits&quot; for each digit. One may wonder how effective this ouput-encoding method is, compared to a different bit-setting encoding method, namely binary representations. . Construct the target output matrix $Y$ for binary representations of the numbers 0 through 9. Your target matrix should have 10 rows and 4 columns (i.e, output bits for 1s, 2s, 4s, and 8s). | Using this $Y$ array, train the network as before, and plot the loss as a function of iteration. | Question: Which method works &#39;better&#39;? One-hot encoding or binary encoding? .",
            "url": "https://dnlam.github.io/fastblog/2022/03/16/_The_Beauty_Of_Neural_Network.html",
            "relUrl": "/2022/03/16/_The_Beauty_Of_Neural_Network.html",
            "date": " • Mar 16, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Optimal Policies with Dynamic Programming",
            "content": "Section: Dynamic Programming . The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies. . In order to shed light on the Chapter 4 of the bible Reinforcement Learning, the objectives of this notebook are: . Policy Evaluation and Policy Improvement | Value and Policy Iteration | Bellman Equations | . Context . We will give an example of how to apply DP into Gridworld City to deal with the City&#39;s Parking problem. The city council has created a Markov decision process (MDP) to model the demand for parking with a reward function that reflects its preferences. . States are nonnegative integers indicating how many parking spaces are occupied. | Actions are nonnegative integers designating the price of street parking. | The reward is a real value describing the city&#39;s preference for the situation. | Time is discretized by hour. | . Preliminaries . The BaseAgent, Environment and RLGlue should follow the our first Notebook on Reinforcement Learning Exploration/Exploitation. . The construction of a virtual ParkingWorld and the plot function are given below: . &lt;Figure size 432x288 with 0 Axes&gt; . Section 1: Policy Evaluation . First, the city council would like you to evaluate the quality of the existing pricing scheme. Policy evaluation works by iteratively applying the Bellman equation for $v_{ pi}$ to a working value function, as an update rule, as shown below. . $$ large v(s) leftarrow sum_a pi(a | s) sum_{s&#39;, r} p(s&#39;, r | s, a)[r + gamma v(s&#39;)]$$ This update can either occur &quot;in-place&quot; (i.e. the update rule is sequentially applied to each state) or with &quot;two-arrays&quot; (i.e. the update rule is simultaneously applied to each state). Both versions converge to $v_{ pi}$ but the in-place version usually converges faster. In this assignment, we will be implementing all update rules in-place, as is done in the pseudocode of chapter 4 of the textbook. . The policy evaluation can be expressed in the code below: . def evaluate_policy(env, V, pi, gamma, theta): delta = float(&#39;inf&#39;) while delta &gt; theta: delta = 0 for s in env.S: v = V[s] bellman_update(env, V, pi, s, gamma) delta = max(delta, abs(v - V[s])) return V . Then, the Bellman update will be: . def bellman_update(env, V, pi, s, gamma): &quot;&quot;&quot;Mutate ``V`` according to the Bellman update equation.&quot;&quot;&quot; v = 0 for a in env.A: transitions = env.transitions(s, a) for s_, (r, p) in enumerate(transitions): v += pi[s][a] * p * (r + gamma * V[s_]) V[s] = v . The observation shows that the value monotonically increases as more parking is used, until there is no parking left, in which case the value is lower. Because of the relatively simple reward function (more reward is accrued when many but not all parking spots are taken and less reward is accrued when few or all parking spots are taken) and the highly stochastic dynamics function (each state has positive probability of being reached each time step) the value functions of most policies will qualitatively resemble this graph. However, depending on the intelligence of the policy, the scale of the graph will differ. In other words, better policies will increase the expected return at every state rather than changing the relative desirability of the states. Intuitively, the value of a less desirable state can be increased by making it less likely to remain in a less desirable state. Similarly, the value of a more desirable state can be increased by making it more likely to remain in a more desirable state. That is to say, good policies are policies that spend more time in desirable states and less time in undesirable states. As we will see in this assignment, such a steady state distribution is achieved by setting the price to be low in low occupancy states (so that the occupancy will increase) and setting the price high when occupancy is high (so that full occupancy will be avoided). . Policy Iteration . Now the city council would like to propose a more efficient policy using policy iteration. Policy iteration works by alternating between evaluating the existing policy and making the policy greedy with respect to the existing value function. . def improve_policy(env, V, pi, gamma): policy_stable = True for s in env.S: old = pi[s].copy() q_greedify_policy(env, V, pi, s, gamma) if not np.array_equal(pi[s], old): policy_stable = False return pi, policy_stable def policy_iteration(env, gamma, theta): V = np.zeros(len(env.S)) pi = np.ones((len(env.S), len(env.A))) / len(env.A) policy_stable = False while not policy_stable: V = evaluate_policy(env, V, pi, gamma, theta) pi, policy_stable = improve_policy(env, V, pi, gamma) return V, pi . def q_greedify_policy(env, V, pi, s, gamma): &quot;&quot;&quot;Mutate ``pi`` to be greedy with respect to the q-values induced by ``V``.&quot;&quot;&quot; G = np.zeros_like(env.A, dtype=float) for a in env.A: transitions = env.transitions(s, a) for s_, (r, p) in enumerate(transitions): G[a] += p * (r + gamma * V[s_]) greed_actions = np.argwhere(G == np.amax(G)) for a in env.A: if a in greed_actions: pi[s, a] = 1 / len(greed_actions) else: pi[s, a] = 0 . Section 3: Value Iteration . Value iteration works by iteratively applying the Bellman optimality equation for $v_{ ast}$ to a working value function, as an update rule, as shown below. . $$ large v(s) leftarrow max_a sum_{s&#39;, r} p(s&#39;, r | s, a)[r + gamma v(s&#39;)]$$ . def value_iteration(env, gamma, theta): V = np.zeros(len(env.S)) while True: delta = 0 for s in env.S: v = V[s] bellman_optimality_update(env, V, s, gamma) delta = max(delta, abs(v - V[s])) if delta &lt; theta: break pi = np.ones((len(env.S), len(env.A))) / len(env.A) for s in env.S: q_greedify_policy(env, V, pi, s, gamma) return V, pi . def bellman_optimality_update(env, V, s, gamma): &quot;&quot;&quot;Mutate ``V`` according to the Bellman optimality update equation.&quot;&quot;&quot; vmax = - float(&#39;inf&#39;) for a in env.A: transitions = env.transitions(s, a) va = 0 for s_, (r, p) in enumerate(transitions): va += p * (r + gamma * V[s_]) vmax = max(va, vmax) V[s] = vmax . In the value iteration algorithm above, a policy is not explicitly maintained until the value function has converged. Below, we have written an identically behaving value iteration algorithm that maintains an updated policy. Writing value iteration in this form makes its relationship to policy iteration more evident. Policy iteration alternates between doing complete greedifications and complete evaluations. On the other hand, value iteration alternates between doing local greedifications and local evaluations. . def value_iteration2(env, gamma, theta): V = np.zeros(len(env.S)) pi = np.ones((len(env.S), len(env.A))) / len(env.A) while True: delta = 0 for s in env.S: v = V[s] q_greedify_policy(env, V, pi, s, gamma) bellman_update(env, V, pi, s, gamma) delta = max(delta, abs(v - V[s])) if delta &lt; theta: break return V, pi .",
            "url": "https://dnlam.github.io/fastblog/2022/03/16/_Optimal_Policy_Dynamic_Programming.html",
            "relUrl": "/2022/03/16/_Optimal_Policy_Dynamic_Programming.html",
            "date": " • Mar 16, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Bandits and Exploitation/Exploration in Reinforcement Learning",
            "content": "Objective of this notebook are to: . Help you understand the bandit problem | Understand the effects of epsilon on exploration and learn about exploitation/exploration trade-off | Introduce some of essential RL softwares that are useful to build a RL solver. | . Section 0: Preliminaries . Firstly, we will build a BaseAgent for our RL . from __future__ import print_function from abc import ABCMeta, abstractmethod class BaseAgent: &quot;&quot;&quot;Implements the agent for an RL-Glue environment. Note: agent_init, agent_start, agent_step, agent_end, agent_cleanup, and agent_message are required methods. &quot;&quot;&quot; __metaclass__ = ABCMeta def __init__(self): pass @abstractmethod def agent_init(self, agent_info= {}): &quot;&quot;&quot;Setup for the agent called when the experiment first starts.&quot;&quot;&quot; @abstractmethod def agent_start(self, observation): &quot;&quot;&quot;The first method called when the experiment starts, called after the environment starts. Args: observation (Numpy array): the state observation from the environment&#39;s evn_start function. Returns: The first action the agent takes. &quot;&quot;&quot; @abstractmethod def agent_step(self, reward, observation): &quot;&quot;&quot;A step taken by the agent. Args: reward (float): the reward received for taking the last action taken observation (Numpy array): the state observation from the environment&#39;s step based, where the agent ended up after the last step Returns: The action the agent is taking. &quot;&quot;&quot; @abstractmethod def agent_end(self, reward): &quot;&quot;&quot;Run when the agent terminates. Args: reward (float): the reward the agent received for entering the terminal state. &quot;&quot;&quot; @abstractmethod def agent_cleanup(self): &quot;&quot;&quot;Cleanup done after the agent ends.&quot;&quot;&quot; @abstractmethod def agent_message(self, message): &quot;&quot;&quot;A function used to pass information from the agent to the experiment. Args: message: The message passed to the agent. Returns: The response (or answer) to the message. &quot;&quot;&quot; . class Agent(BaseAgent): &quot;&quot;&quot;agent does *no* learning, selects action 0 always&quot;&quot;&quot; def __init__(self): self.last_action = None self.num_actions = None self.q_values = None self.step_size = None self.epsilon = None self.initial_value = 0.0 self.arm_count = [0.0 for _ in range(10)] def agent_init(self, agent_info={}): &quot;&quot;&quot;Setup for the agent called when the experiment first starts.&quot;&quot;&quot; # if &quot;actions&quot; in agent_info: # self.num_actions = agent_info[&quot;actions&quot;] # if &quot;state_array&quot; in agent_info: # self.q_values = agent_info[&quot;state_array&quot;] self.num_actions = agent_info.get(&quot;num_actions&quot;, 2) self.initial_value = agent_info.get(&quot;initial_value&quot;, 0.0) self.q_values = np.ones(agent_info.get(&quot;num_actions&quot;, 2)) * self.initial_value self.step_size = agent_info.get(&quot;step_size&quot;, 0.1) self.epsilon = agent_info.get(&quot;epsilon&quot;, 0.0) self.last_action = 0 def agent_start(self, observation): &quot;&quot;&quot;The first method called when the experiment starts, called after the environment starts. Args: observation (Numpy array): the state observation from the environment&#39;s evn_start function. Returns: The first action the agent takes. &quot;&quot;&quot; self.last_action = np.random.choice(self.num_actions) # set first action to 0 return self.last_action def agent_step(self, reward, observation): &quot;&quot;&quot;A step taken by the agent. Args: reward (float): the reward received for taking the last action taken observation (Numpy array): the state observation from the environment&#39;s step based, where the agent ended up after the last step Returns: The action the agent is taking. &quot;&quot;&quot; # local_action = 0 # choose the action here self.last_action = np.random.choice(self.num_actions) return self.last_action def agent_end(self, reward): &quot;&quot;&quot;Run when the agent terminates. Args: reward (float): the reward the agent received for entering the terminal state. &quot;&quot;&quot; pass def agent_cleanup(self): &quot;&quot;&quot;Cleanup done after the agent ends.&quot;&quot;&quot; pass def agent_message(self, message): &quot;&quot;&quot;A function used to pass information from the agent to the experiment. Args: message: The message passed to the agent. Returns: The response (or answer) to the message. &quot;&quot;&quot; pass . Then, we create an Environment and its abstract for our RL. This is the 10-armed Testbed introduced in section 2.3 of the textbook. We use this throughout this notebook to test our bandit agents. It has 10 arms, which are the actions the agent can take. Pulling an arm generates a stochastic reward from a Gaussian distribution with unit-variance. For each action, the expected value of that action is randomly sampled from a normal distribution, at the start of each run. If you are unfamiliar with the 10-armed Testbed please review it in the textbook before continuing. . &quot;&quot;&quot;Abstract environment base class. &quot;&quot;&quot; from __future__ import print_function from abc import ABCMeta, abstractmethod class BaseEnvironment: &quot;&quot;&quot;Implements the environment for an RLGlue environment Note: env_init, env_start, env_step, env_cleanup, and env_message are required methods. &quot;&quot;&quot; __metaclass__ = ABCMeta def __init__(self): reward = None observation = None termination = None self.reward_obs_term = (reward, observation, termination) @abstractmethod def env_init(self, env_info={}): &quot;&quot;&quot;Setup for the environment called when the experiment first starts. Note: Initialize a tuple with the reward, first state observation, boolean indicating if it&#39;s terminal. &quot;&quot;&quot; @abstractmethod def env_start(self): &quot;&quot;&quot;The first method called when the experiment starts, called before the agent starts. Returns: The first state observation from the environment. &quot;&quot;&quot; @abstractmethod def env_step(self, action): &quot;&quot;&quot;A step taken by the environment. Args: action: The action taken by the agent Returns: (float, state, Boolean): a tuple of the reward, state observation, and boolean indicating if it&#39;s terminal. &quot;&quot;&quot; @abstractmethod def env_cleanup(self): &quot;&quot;&quot;Cleanup done after the environment ends&quot;&quot;&quot; @abstractmethod def env_message(self, message): &quot;&quot;&quot;A message asking the environment for information Args: message: the message passed to the environment Returns: the response (or answer) to the message &quot;&quot;&quot; . class Environment(BaseEnvironment): &quot;&quot;&quot;Implements the environment for an RLGlue environment Note: env_init, env_start, env_step, env_cleanup, and env_message are required methods. &quot;&quot;&quot; actions = [0] def __init__(self): reward = None observation = None termination = None self.reward_obs_term = (reward, observation, termination) self.count = 0 self.arms = [] self.seed = None def env_init(self, env_info={}): &quot;&quot;&quot;Setup for the environment called when the experiment first starts. Note: Initialize a tuple with the reward, first state observation, boolean indicating if it&#39;s terminal. &quot;&quot;&quot; self.arms = np.random.randn(10)#[np.random.normal(0.0, 1.0) for _ in range(10)] local_observation = 0 # An empty NumPy array self.reward_obs_term = (0.0, local_observation, False) def env_start(self): &quot;&quot;&quot;The first method called when the experiment starts, called before the agent starts. Returns: The first state observation from the environment. &quot;&quot;&quot; return self.reward_obs_term[1] def env_step(self, action): &quot;&quot;&quot;A step taken by the environment. Args: action: The action taken by the agent Returns: (float, state, Boolean): a tuple of the reward, state observation, and boolean indicating if it&#39;s terminal. &quot;&quot;&quot; # if action == 0: # if np.random.random() &lt; 0.2: # reward = 14 # else: # reward = 6 # if action == 1: # reward = np.random.choice(range(10,14)) # if action == 2: # if np.random.random() &lt; 0.8: # reward = 174 # else: # reward = 7 # reward = np.random.normal(self.arms[action], 1.0) reward = self.arms[action] + np.random.randn() obs = self.reward_obs_term[1] self.reward_obs_term = (reward, obs, False) return self.reward_obs_term def env_cleanup(self): &quot;&quot;&quot;Cleanup done after the environment ends&quot;&quot;&quot; pass def env_message(self, message): &quot;&quot;&quot;A message asking the environment for information Args: message (string): the message passed to the environment Returns: string: the response (or answer) to the message &quot;&quot;&quot; if message == &quot;what is the current reward?&quot;: return &quot;{}&quot;.format(self.reward_obs_term[0]) # else return &quot;I don&#39;t know how to respond to your message&quot; . Here, we create RLGlue which was originally designed by Adam White, Brian Tanner, and Rich Sutton. This library will give you a solid framework to understand how reinforcement learning experiments work and how to run your own. . class RLGlue: &quot;&quot;&quot;RLGlue class args: env_name (string): the name of the module where the Environment class can be found agent_name (string): the name of the module where the Agent class can be found &quot;&quot;&quot; def __init__(self, env_class, agent_class): self.environment = env_class() self.agent = agent_class() self.total_reward = None self.last_action = None self.num_steps = None self.num_episodes = None def rl_init(self, agent_init_info={}, env_init_info={}): &quot;&quot;&quot;Initial method called when RLGlue experiment is created&quot;&quot;&quot; self.environment.env_init(env_init_info) self.agent.agent_init(agent_init_info) self.total_reward = 0.0 self.num_steps = 0 self.num_episodes = 0 def rl_start(self, agent_start_info={}, env_start_info={}): &quot;&quot;&quot;Starts RLGlue experiment Returns: tuple: (state, action) &quot;&quot;&quot; last_state = self.environment.env_start() self.last_action = self.agent.agent_start(last_state) observation = (last_state, self.last_action) return observation def rl_agent_start(self, observation): &quot;&quot;&quot;Starts the agent. Args: observation: The first observation from the environment Returns: The action taken by the agent. &quot;&quot;&quot; return self.agent.agent_start(observation) def rl_agent_step(self, reward, observation): &quot;&quot;&quot;Step taken by the agent Args: reward (float): the last reward the agent received for taking the last action. observation : the state observation the agent receives from the environment. Returns: The action taken by the agent. &quot;&quot;&quot; return self.agent.agent_step(reward, observation) def rl_agent_end(self, reward): &quot;&quot;&quot;Run when the agent terminates Args: reward (float): the reward the agent received when terminating &quot;&quot;&quot; self.agent.agent_end(reward) def rl_env_start(self): &quot;&quot;&quot;Starts RL-Glue environment. Returns: (float, state, Boolean): reward, state observation, boolean indicating termination &quot;&quot;&quot; self.total_reward = 0.0 self.num_steps = 1 this_observation = self.environment.env_start() return this_observation def rl_env_step(self, action): &quot;&quot;&quot;Step taken by the environment based on action from agent Args: action: Action taken by agent. Returns: (float, state, Boolean): reward, state observation, boolean indicating termination. &quot;&quot;&quot; ro = self.environment.env_step(action) (this_reward, _, terminal) = ro self.total_reward += this_reward if terminal: self.num_episodes += 1 else: self.num_steps += 1 return ro def rl_step(self): &quot;&quot;&quot;Step taken by RLGlue, takes environment step and either step or end by agent. Returns: (float, state, action, Boolean): reward, last state observation, last action, boolean indicating termination &quot;&quot;&quot; (reward, last_state, term) = self.environment.env_step(self.last_action) self.total_reward += reward if term: self.num_episodes += 1 self.agent.agent_end(reward) roat = (reward, last_state, None, term) else: self.num_steps += 1 self.last_action = self.agent.agent_step(reward, last_state) roat = (reward, last_state, self.last_action, term) return roat def rl_cleanup(self): &quot;&quot;&quot;Cleanup done at end of experiment.&quot;&quot;&quot; self.environment.env_cleanup() self.agent.agent_cleanup() def rl_agent_message(self, message): &quot;&quot;&quot;Message passed to communicate with agent during experiment Args: message: the message (or question) to send to the agent Returns: The message back (or answer) from the agent &quot;&quot;&quot; return self.agent.agent_message(message) def rl_env_message(self, message): &quot;&quot;&quot;Message passed to communicate with environment during experiment Args: message: the message (or question) to send to the environment Returns: The message back (or answer) from the environment &quot;&quot;&quot; return self.environment.env_message(message) def rl_episode(self, max_steps_this_episode): &quot;&quot;&quot;Runs an RLGlue episode Args: max_steps_this_episode (Int): the maximum steps for the experiment to run in an episode Returns: Boolean: if the episode should terminate &quot;&quot;&quot; is_terminal = False self.rl_start() while (not is_terminal) and ((max_steps_this_episode == 0) or (self.num_steps &lt; max_steps_this_episode)): rl_step_result = self.rl_step() is_terminal = rl_step_result[3] return is_terminal def rl_return(self): &quot;&quot;&quot;The total reward Returns: float: the total reward &quot;&quot;&quot; return self.total_reward def rl_num_steps(self): &quot;&quot;&quot;The total number of steps taken Returns: Int: the total number of steps taken &quot;&quot;&quot; return self.num_steps def rl_num_episodes(self): &quot;&quot;&quot;The number of episodes Returns Int: the total number of episodes &quot;&quot;&quot; return self.num_episodes . %matplotlib inline import numpy as np import matplotlib.pyplot as plt from tqdm import tqdm import time . Section 1: Greedy Agent . We want to create an agent that will find the action with the highest expected reward. One way an agent could operate is to always choose the action with the highest value based on the agent’s current estimates. This is called a greedy agent as it greedily chooses the action that it thinks has the highest value. Let&#39;s look at what happens in this case. . First we are going to implement the argmax function, which takes in a list of action values and returns an action with the highest value. Why are we implementing our own instead of using the argmax function that numpy uses? Numpy&#39;s argmax function returns the first instance of the highest value. We do not want that to happen as it biases the agent to choose a specific action in the case of ties. Instead we want to break ties between the highest values randomly. So we are going to implement our own argmax function. You may want to look at np.random.choice to randomly select from a list of values. . def argmax(q_values): &quot;&quot;&quot; Takes in a list of q_values and returns the index of the item with the highest value. Breaks ties randomly. returns: int - the index of the highest value in q_values &quot;&quot;&quot; top_value = float(&quot;-inf&quot;) ties = [] for i in range(len(q_values)): # if a value in q_values is greater than the highest value update top and reset ties to zero # if a value is equal to top value add the index to ties # return a random selection from ties. # YOUR CODE HERE if q_values[i] &gt; top_value: top_value = q_values[i] ties = [i] elif q_values[i] == top_value: ties.append(i) return np.random.choice(ties) . Next, we are going to create a GreedyAgent and implement the agent_step method. This method gets called each time the agent takes a step. The method has to return the action selected by the agent. This method also ensures the agent’s estimates are updated based on the signals it gets from the environment. . class GreedyAgent(Agent): def agent_step(self, reward, observation=None): &quot;&quot;&quot; Takes one step for the agent. It takes in a reward and observation and returns the action the agent chooses at that time step. Arguments: reward -- float, the reward the agent recieved from the environment after taking the last action. observation -- float, the observed state the agent is in. Do not worry about this as you will not use it until future lessons Returns: current_action -- int, the action chosen by the agent at the current time step. &quot;&quot;&quot; ### Useful Class Variables ### # self.q_values : An array with what the agent believes each of the values of the arm are. # self.arm_count : An array with a count of the number of times each arm has been pulled. # self.last_action : The action that the agent took on the previous time step ####################### # Update Q values Hint: Look at the algorithm in section 2.4 of the textbook. # increment the counter in self.arm_count for the action from the previous time step # update the step size using self.arm_count # update self.q_values for the action from the previous time step self.arm_count[self.last_action] += 1 self.q_values[self.last_action] += (reward - self.q_values[self.last_action]) / self.arm_count[self.last_action] current_action = argmax(self.q_values) self.last_action = current_action return current_action . Then, let&#39;s visualize the result . num_runs = 200 # The number of times we run the experiment num_steps = 1000 # The number of pulls of each arm the agent takes env = Environment # We set what environment we want to use to test agent = GreedyAgent # We choose what agent we want to use agent_info = {&quot;num_actions&quot;: 10} # We pass the agent the information it needs. Here how many arms there are. env_info = {} # We pass the environment the information it needs. In this case nothing. rewards = np.zeros((num_runs, num_steps)) average_best = 0 for run in tqdm(range(num_runs)): # tqdm is what creates the progress bar below np.random.seed(run) rl_glue = RLGlue(env, agent) # Creates a new RLGlue experiment with the env and agent we chose above rl_glue.rl_init(agent_info, env_info) # We pass RLGlue what it needs to initialize the agent and environment rl_glue.rl_start() # We start the experiment average_best += np.max(rl_glue.environment.arms) for i in range(num_steps): reward, _, action, _ = rl_glue.rl_step() # The environment and agent take a step and return # the reward, and action taken. rewards[run, i] = reward greedy_scores = np.mean(rewards, axis=0) plt.figure(figsize=(15, 5), dpi= 80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.plot([average_best / num_runs for _ in range(num_steps)], linestyle=&quot;--&quot;) plt.plot(greedy_scores) plt.legend([&quot;Best Possible&quot;, &quot;Greedy&quot;]) plt.title(&quot;Average Reward of Greedy Agent&quot;) plt.xlabel(&quot;Steps&quot;) plt.ylabel(&quot;Average reward&quot;) plt.show() . 100%|██████████| 200/200 [00:03&lt;00:00, 63.50it/s] . Section 2: Epsilon-Greedy Agent . We noticed about a trade off between Exploitation and Exploration, where it does not always take the greedy action. Instead, sometimes it takes an exploratory action. It does this so that it can find out what the best action really is. If we always choose what we think is the current best action is, we may miss out on taking the true best action, because we haven&#39;t explored enough times to find that best action. . Implement an epsilon-greedy agent below. Hint: we are implementing the algorithm from section 2.4 of this textbook. You may want to use your greedy code from above and look at np.random.random, as well as np.random.randint, to help you select random actions. . class EpsilonGreedyAgent(Agent): def agent_step(self, reward, observation): &quot;&quot;&quot; Takes one step for the agent. It takes in a reward and observation and returns the action the agent chooses at that time step. Arguments: reward -- float, the reward the agent recieved from the environment after taking the last action. observation -- float, the observed state the agent is in. Do not worry about this as you will not use it until future lessons Returns: current_action -- int, the action chosen by the agent at the current time step. &quot;&quot;&quot; ### Useful Class Variables ### # self.q_values : An array with what the agent believes each of the values of the arm are. # self.arm_count : An array with a count of the number of times each arm has been pulled. # self.last_action : The action that the agent took on the previous time step # self.epsilon : The probability an epsilon greedy agent will explore (ranges between 0 and 1) ####################### # Update Q values - this should be the same update as your greedy agent above self.arm_count[self.last_action] += 1 self.q_values[self.last_action] += (reward - self.q_values[self.last_action]) / self.arm_count[self.last_action] # Choose action using epsilon greedy # Randomly choose a number between 0 and 1 and see if it&#39;s less than self.epsilon if np.random.random() &lt; self.epsilon: current_action = np.random.randint(len(self.q_values)) else: current_action = argmax(self.q_values) self.last_action = current_action return current_action . Now that we have our epsilon greedy agent created. Let&#39;s compare it against the greedy agent with epsilon of 0.1. . num_runs = 200 num_steps = 1000 epsilon = 0.1 agent = EpsilonGreedyAgent env = Environment # ten arms agent_info = {&quot;num_actions&quot;: 10, &quot;epsilon&quot;: epsilon} env_info = {} all_rewards = np.zeros((num_runs, num_steps)) for run in tqdm(range(num_runs)): np.random.seed(run) rl_glue = RLGlue(env, agent) rl_glue.rl_init(agent_info, env_info) rl_glue.rl_start() for i in range(num_steps): reward, _, action, _ = rl_glue.rl_step() # The environment and agent take a step and return # the reward, and action taken. all_rewards[run, i] = reward # take the mean over runs scores = np.mean(all_rewards, axis=0) plt.figure(figsize=(15, 5), dpi= 80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.plot([1.55 for _ in range(num_steps)], linestyle=&quot;--&quot;) plt.plot(greedy_scores) plt.title(&quot;Average Reward of Greedy Agent vs. E-Greedy Agent&quot;) plt.plot(scores) plt.legend((&quot;Best Possible&quot;, &quot;Greedy&quot;, &quot;Epsilon: 0.1&quot;)) plt.xlabel(&quot;Steps&quot;) plt.ylabel(&quot;Average reward&quot;) plt.show() . 100%|██████████| 200/200 [00:03&lt;00:00, 63.69it/s] . Here, we noticed how much better the epsilon-greedy agent did. Because we occasionally choose a random action we were able to find a better long term policy. By acting greedily before our value estimates are accurate, we risk settling on a suboptimal action. . Section 3: Comparing values of epsilon . Can we do better than an epsilon of 0.1? Let&#39;s try several different values for epsilon and see how they perform. We try different settings of key performance parameters to understand how the agent might perform under different conditions. . Below we run an experiment where we sweep over different values for epsilon: . epsilons = [0.0, 0.01, 0.1, 0.4] plt.figure(figsize=(15, 5), dpi= 80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.plot([1.55 for _ in range(num_steps)], linestyle=&quot;--&quot;) n_q_values = [] n_averages = [] n_best_actions = [] num_runs = 200 for epsilon in epsilons: all_averages = [] for run in tqdm(range(num_runs)): agent = EpsilonGreedyAgent agent_info = {&quot;num_actions&quot;: 10, &quot;epsilon&quot;: epsilon} env_info = {&quot;random_seed&quot;: run} rl_glue = RLGlue(env, agent) rl_glue.rl_init(agent_info, env_info) rl_glue.rl_start() best_arm = np.argmax(rl_glue.environment.arms) scores = [0] averages = [] best_action_chosen = [] for i in range(num_steps): reward, state, action, is_terminal = rl_glue.rl_step() scores.append(scores[-1] + reward) averages.append(scores[-1] / (i + 1)) if action == best_arm: best_action_chosen.append(1) else: best_action_chosen.append(0) if epsilon == 0.1 and run == 0: n_q_values.append(np.copy(rl_glue.agent.q_values)) if epsilon == 0.1: n_averages.append(averages) n_best_actions.append(best_action_chosen) all_averages.append(averages) plt.plot(np.mean(all_averages, axis=0)) plt.legend([&quot;Best Possible&quot;] + epsilons) plt.xlabel(&quot;Steps&quot;) plt.ylabel(&quot;Average reward&quot;) plt.show() . 100%|██████████| 200/200 [00:04&lt;00:00, 48.39it/s] 100%|██████████| 200/200 [00:04&lt;00:00, 48.03it/s] 100%|██████████| 200/200 [00:03&lt;00:00, 55.24it/s] 100%|██████████| 200/200 [00:02&lt;00:00, 67.29it/s] . Section 4: The Effect of Step Size . In Section 1, we decayed the step size over time based on action-selection counts. The step-size was 1/N(A), where N(A) is the number of times action A was selected. This is the same as computing a sample average. We could also set the step size to be a constant value, such as 0.1. What would be the effect of doing that? And is it better to use a constant or the sample average method? . To investigate this question, let’s start by creating a new agent that has a constant step size. This will be nearly identical to the agent created above. You will use the same code to select the epsilon-greedy action. You will change the update to have a constant step size instead of using the 1/N(A) update. . class EpsilonGreedyAgentConstantStepsize(Agent): def agent_step(self, reward, observation): &quot;&quot;&quot; Takes one step for the agent. It takes in a reward and observation and returns the action the agent chooses at that time step. Arguments: reward -- float, the reward the agent recieved from the environment after taking the last action. observation -- float, the observed state the agent is in. Do not worry about this as you will not use it until future lessons Returns: current_action -- int, the action chosen by the agent at the current time step. &quot;&quot;&quot; ### Useful Class Variables ### # self.q_values : An array with what the agent believes each of the values of the arm are. # self.arm_count : An array with a count of the number of times each arm has been pulled. # self.last_action : An int of the action that the agent took on the previous time step. # self.step_size : A float which is the current step size for the agent. # self.epsilon : The probability an epsilon greedy agent will explore (ranges between 0 and 1) ####################### # Update q_values for action taken at previous time step # using self.step_size intead of using self.arm_count self.arm_count[self.last_action] += 1 self.q_values[self.last_action] += self.step_size * (reward - self.q_values[self.last_action]) # Choose action using epsilon greedy. This is the same as you implemented above. if np.random.random() &lt; self.epsilon: current_action = np.random.randint(len(self.q_values)) else: current_action = argmax(self.q_values) self.last_action = current_action return current_action . step_sizes = [0.01, 0.1, 0.5, 1.0, &#39;1/N(A)&#39;] epsilon = 0.1 num_steps = 1000 num_runs = 200 fig, ax = plt.subplots(figsize=(15, 5), dpi= 80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) q_values = {step_size: [] for step_size in step_sizes} true_values = {step_size: None for step_size in step_sizes} best_actions = {step_size: [] for step_size in step_sizes} for step_size in step_sizes: all_averages = [] for run in tqdm(range(num_runs)): np.random.seed(run) agent = EpsilonGreedyAgentConstantStepsize if step_size != &#39;1/N(A)&#39; else EpsilonGreedyAgent agent_info = {&quot;num_actions&quot;: 10, &quot;epsilon&quot;: epsilon, &quot;step_size&quot;: step_size, &quot;initial_value&quot;: 0.0} env_info = {} rl_glue = RLGlue(env, agent) rl_glue.rl_init(agent_info, env_info) rl_glue.rl_start() best_arm = np.argmax(rl_glue.environment.arms) if run == 0: true_values[step_size] = np.copy(rl_glue.environment.arms) best_action_chosen = [] for i in range(num_steps): reward, state, action, is_terminal = rl_glue.rl_step() if action == best_arm: best_action_chosen.append(1) else: best_action_chosen.append(0) if run == 0: q_values[step_size].append(np.copy(rl_glue.agent.q_values)) best_actions[step_size].append(best_action_chosen) ax.plot(np.mean(best_actions[step_size], axis=0)) plt.legend(step_sizes) plt.title(&quot;% Best Arm Pulled&quot;) plt.xlabel(&quot;Steps&quot;) plt.ylabel(&quot;% Best Arm Pulled&quot;) vals = ax.get_yticks() ax.set_yticklabels([&#39;{:,.2%}&#39;.format(x) for x in vals]) plt.show() . 100%|██████████| 200/200 [00:03&lt;00:00, 56.86it/s] 100%|██████████| 200/200 [00:03&lt;00:00, 60.23it/s] 100%|██████████| 200/200 [00:03&lt;00:00, 59.37it/s] 100%|██████████| 200/200 [00:03&lt;00:00, 59.35it/s] 100%|██████████| 200/200 [00:03&lt;00:00, 56.94it/s] C: Users ND258645 AppData Local Temp ipykernel_15840 3358721971.py:48: UserWarning: FixedFormatter should only be used together with FixedLocator ax.set_yticklabels([&#39;{:,.2%}&#39;.format(x) for x in vals]) . Notice first that we are now plotting the amount of time that the best action is taken rather than the average reward. To better understand the performance of an agent, it can be useful to measure specific behaviors, beyond just how much reward is accumulated. This measure indicates how close the agent’s behaviour is to optimal. . It seems as though 1/N(A) performed better than the others, in that it reaches a solution where it takes the best action most frequently. Now why might this be? Why did a step size of 0.5 start out better but end up performing worse? Why did a step size of 0.01 perform so poorly? . Let&#39;s dig into this further below. Let’s plot how well each agent tracks the true value, where each agent has a different step size method. You do not have to enter any code here, just follow along. . largest = 0 num_steps = 1000 for step_size in step_sizes: plt.figure(figsize=(15, 5), dpi= 80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) largest = np.argmax(true_values[step_size]) plt.plot([true_values[step_size][largest] for _ in range(num_steps)], linestyle=&quot;--&quot;) plt.title(&quot;Step Size: {}&quot;.format(step_size)) plt.plot(np.array(q_values[step_size])[:, largest]) plt.legend([&quot;True Expected Value&quot;, &quot;Estimated Value&quot;]) plt.xlabel(&quot;Steps&quot;) plt.ylabel(&quot;Value&quot;) plt.show() . These plots help clarify the performance differences between the different step sizes. A step size of 0.01 makes such small updates that the agent’s value estimate of the best action does not get close to the actual value. Step sizes of 0.5 and 1.0 both get close to the true value quickly, but are very susceptible to stochasticity in the rewards. The updates overcorrect too much towards recent rewards, and so oscillate around the true value. This means that on many steps, the action that pulls the best arm may seem worse than it actually is. A step size of 0.1 updates fairly quickly to the true value, and does not oscillate as widely around the true values as 0.5 and 1.0. This is one of the reasons that 0.1 performs quite well. Finally we see why 1/N(A) performed well. Early on while the step size is still reasonably high it moves quickly to the true expected value, but as it gets pulled more its step size is reduced which makes it less susceptible to the stochasticity of the rewards. . Does this mean that 1/N(A) is always the best? When might it not be? One possible setting where it might not be as effective is in non-stationary problems. You learned about non-stationarity in the lessons. Non-stationarity means that the environment may change over time. This could manifest itself as continual change over time of the environment, or a sudden change in the environment. . Let&#39;s look at how a sudden change in the reward distributions affects a step size like 1/N(A). This time we will run the environment for 2000 steps, and after 1000 steps we will randomly change the expected value of all of the arms. We compare two agents, both using epsilon-greedy with epsilon = 0.1. One uses a constant step size of 0.1, the other a step size of 1/N(A) that reduces over time. . epsilon = 0.1 num_steps = 2000 num_runs = 500 step_size = 0.1 plt.figure(figsize=(15, 5), dpi= 80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.plot([1.55 for _ in range(num_steps)], linestyle=&quot;--&quot;) for agent in [EpsilonGreedyAgent, EpsilonGreedyAgentConstantStepsize]: rewards = np.zeros((num_runs, num_steps)) for run in tqdm(range(num_runs)): agent_info = {&quot;num_actions&quot;: 10, &quot;epsilon&quot;: epsilon, &quot;step_size&quot;: step_size} np.random.seed(run) rl_glue = RLGlue(env, agent) rl_glue.rl_init(agent_info, env_info) rl_glue.rl_start() for i in range(num_steps): reward, state, action, is_terminal = rl_glue.rl_step() rewards[run, i] = reward if i == 1000: rl_glue.environment.arms = np.random.randn(10) plt.plot(np.mean(rewards, axis=0)) plt.legend([&quot;Best Possible&quot;, &quot;1/N(A)&quot;, &quot;0.1&quot;]) plt.xlabel(&quot;Steps&quot;) plt.ylabel(&quot;Average reward&quot;) plt.show() . 100%|██████████| 500/500 [00:14&lt;00:00, 34.20it/s] 100%|██████████| 500/500 [00:18&lt;00:00, 27.43it/s] . Now the agent with a step size of 1/N(A) performed better at the start but then performed worse when the environment changed! What happened? . Think about what the step size would be after 1000 steps. Let&#39;s say the best action gets chosen 500 times. That means the step size for that action is 1/500 or 0.002. At each step when we update the value of the action and the value is going to move only 0.002 * the error. That is a very tiny adjustment and it will take a long time for it to get to the true value. . The agent with step size 0.1, however, will always update in 1/10th of the direction of the error. This means that on average it will take ten steps for it to update its value to the sample mean. . These are the types of tradeoffs we have to think about in reinforcement learning. A larger step size moves us more quickly toward the true value, but can make our estimated values oscillate around the expected value. A step size that reduces over time can converge to close to the expected value, without oscillating. On the other hand, such a decaying stepsize is not able to adapt to changes in the environment. Nonstationarityand the related concept of partial observabilityis a common feature of reinforcement learning problems and when learning online. .",
            "url": "https://dnlam.github.io/fastblog/2022/03/16/Bandits_Exploration_Exploitation.html",
            "relUrl": "/2022/03/16/Bandits_Exploration_Exploitation.html",
            "date": " • Mar 16, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://dnlam.github.io/fastblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there, I am Lam DINH (Ngoc-Lam DINH), I graduated from Ha Noi University of Science and Technology (Vietnam) in 2016 with a degree in Electronics and Telecommunications Engineering. My training mainly focused on several areas: Digital Signal Processing, Wireless Communication and Embedded Programming. Then, I continued my study with a special interest in Signal Theory, Wireless Telecommunications and Optical Networks at Universidad Politecnica de Valencia (Spain) in 2017. . My Master’s degree is jointly awarded by the École Normale Supérieure Paris Saclay (France) and the Universidad Complutense de Madrid (Spain) in 2019. The courses are related to the application of molecular photonics to telecommunications and biosensors. . From 2019, I am conducting a PhD thesis at the Commissariat à l’Énergie Atomique (CEA) in Grenoble. My research addresses ultra-reliable and low-latency communications (URLLC) in 5G systems and beyond. . More Information . Personal Web Page .",
          "url": "https://dnlam.github.io/fastblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://dnlam.github.io/fastblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
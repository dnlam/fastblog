{
  
    
        "post0": {
            "title": "Natural Language Processing (NLP) - Part 2",
            "content": "",
            "url": "https://dnlam.github.io/fastblog/2022/04/07/Language_Model.html",
            "relUrl": "/2022/04/07/Language_Model.html",
            "date": " • Apr 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Natural Language Processing (NLP) - Part 1",
            "content": "Objectives . In this notebook, we are going to deep dive into natural language processing (NLP) using Deep Learning (info). Relying on the pretrained language model, we are going to fine-tuning it to classify the reviews and it works as sentiment analysis. . Based on a language model which has been trained to guess what the next word in the text is, we will apply transfer learning method for this NLP task. . We will start with the Wikipedia language model with a subset which we called Wikitext103. Then, we are going to create an ImdB language model which predicts the next word of a movie reviews. This intermediate learning will help us to learn about IMDb-specific kinds of words like the name of actors and directors. Afterwards, we end up with our classifier. . Text Preprocessing . In order to build a language model with many complexities such as different sentence lengths in long documents, we can build a neural network model to deal with that issue. . Previously, we talked about categorical variables (words) which can be used as independant variables for a neural network (using embeding matrix). Then, we could do the same thing with text! First, we concatenate all of the documents in our dataset into a big long string and split it into words. Our independant variables will be the sequence of words starting with the first word and ending with the second last, and our dependant variable would be the sequence of words starting with the second word and ending with the last words. . In our vocab, it might exist the very common words and new words. For new words, because we don&#39;t have any pre-knowledge, so we will just initialize the corresponding row with a random vector. . These above steps can be listed as below: . Tokenization: convert the text into a list of words | Numericalization: make a list of all the unique words which appear, and convert each word into a number, by looking up its index in the vocab. | Language model data loader creation : handle creating dependant variables | Language model creation: handle input list by using recurrent neural network. | . Tokenization . Basically, tokenization convert the text into list of words. Firstly, we will grap our IMDb dataset and try out the tokenizer with all the text files. . from fastai.text.all import * path = untar_data(URLs.IMDB) . files = get_text_files(path,folders=[&#39;train&#39;,&#39;test&#39;,&#39;unsup&#39;]) . The default English word tokenizer that FastAI used is called SpaCy which uses a sophisticated riles engine for particular words and URLs. Rather than directly using SpacyTokenizer, we are going to use WordTokenizer which always points to fastai&#39;s current default word tokenizer. . txt = files[0].open().read() txt[:60] spacy = WordTokenizer() toks = first(spacy([txt])) print(coll_repr(toks,30)) . (#212) [&#39;I&#39;,&#39;did&#39;,&#34;n&#39;t&#34;,&#39;know&#39;,&#39;what&#39;,&#39;to&#39;,&#39;expect&#39;,&#39;when&#39;,&#39;I&#39;,&#39;started&#39;,&#39;watching&#39;,&#39;this&#39;,&#39;movie&#39;,&#39;,&#39;,&#39;by&#39;,&#39;the&#39;,&#39;end&#39;,&#39;of&#39;,&#39;it&#39;,&#39;I&#39;,&#39;was&#39;,&#39;pulling&#39;,&#39;my&#39;,&#39;hairs&#39;,&#39;out&#39;,&#39;.&#39;,&#39;This&#39;,&#39;was&#39;,&#39;one&#39;,&#39;of&#39;...] . Subword tokenization . In additions to word tokenizer, subword tokenizer is really useful for langueges which the spaces are not neccesary for separations of components in a sentence (e.g: Chinese). To handle this, we will do 2 steps: . Analyze a corpus of documents to find the most commonly occuring groups of letters which form the vocab | Tokenize the corpus using this vocab of subword units | . For example, we will first look into 2000 movie reviews . txts = L(o.open().read() for o in files[:2000]) . def subword(sz): sp = SubwordTokenizer(vocab_sz=sz) sp.setup(txts) return &#39; &#39;.join(first(sp([txt]))[:40]) . subword(1000) . sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=tmp/texts.out --vocab_size=1000 --model_prefix=tmp/spm --character_coverage=0.99999 --model_type=unigram --unk_id=9 --pad_id=-1 --bos_id=-1 --eos_id=-1 --minloglevel=2 --user_defined_symbols=▁xxunk,▁xxpad,▁xxbos,▁xxeos,▁xxfld,▁xxrep,▁xxwrep,▁xxup,▁xxmaj --hard_vocab_limit=false . &#34;▁I ▁didn &#39; t ▁know ▁what ▁to ▁expect ▁when ▁I ▁start ed ▁watching ▁this ▁movie , ▁by ▁the ▁end ▁of ▁it ▁I ▁was ▁p ul ling ▁my ▁ ha ir s ▁out . ▁This ▁was ▁one ▁of ▁the ▁most ▁pa&#34; . Then, the long underscore is when we replace the space and we can know where the sentences actually start and stop. . subword(10000) . &#34;▁I ▁didn &#39; t ▁know ▁what ▁to ▁expect ▁when ▁I ▁started ▁watching ▁this ▁movie , ▁by ▁the ▁end ▁of ▁it ▁I ▁was ▁pull ing ▁my ▁hair s ▁out . ▁This ▁was ▁one ▁of ▁the ▁most ▁pathetic ▁movies ▁of ▁this ▁year&#34; . If we use a larger vocab, then most common English words will end up in the vocab thelselves, and we will not need as many to represent a sentence. So, there is a compromise to take into account when choosing subword vocab: A larger vocab means more fetwer tokens per sentence which means faster training, less memory, less state for the model to remember but it comes to the downside of larger embedding matrix and requiring more data to learn. . Numericalization . In order to numericalize, we need to call setup first to create the vocab. . tkn = Tokenizer(spacy) toks300 = txts[:300].map(tkn) toks300[0] . (#231) [&#39;xxbos&#39;,&#39;i&#39;,&#39;did&#39;,&#34;n&#39;t&#34;,&#39;know&#39;,&#39;what&#39;,&#39;to&#39;,&#39;expect&#39;,&#39;when&#39;,&#39;i&#39;...] . num = Numericalize() num.setup(toks300) coll_repr(num.vocab,20) . &#34;(#2576) [&#39;xxunk&#39;,&#39;xxpad&#39;,&#39;xxbos&#39;,&#39;xxeos&#39;,&#39;xxfld&#39;,&#39;xxrep&#39;,&#39;xxwrep&#39;,&#39;xxup&#39;,&#39;xxmaj&#39;,&#39;the&#39;,&#39;.&#39;,&#39;,&#39;,&#39;a&#39;,&#39;and&#39;,&#39;of&#39;,&#39;to&#39;,&#39;i&#39;,&#39;is&#39;,&#39;it&#39;,&#39;this&#39;...]&#34; . The results return our rule tokens first and it is followed by word appeanrances, in frequency order. Once we created our Numerical object, we can use it as if it were a function. . nums = num(toks)[:20] nums . TensorText([ 0, 90, 32, 133, 63, 15, 495, 73, 0, 670, 160, 19, 26, 11, 70, 9, 138, 14, 18, 0]) . &#39; &#39;.join(num.vocab[o] for o in nums) . &#34;xxunk did n&#39;t know what to expect when xxunk started watching this movie , by the end of it xxunk&#34; . Now, we have already had numerical data, we need to put them in batches for our model. . Batches of texts . Recalling the batch creation for the images when we have to reshape all the images to be same size before grouping them together in a single tensor for the efficient calculation purposes. It is a little bit different when dealing with texts because it is not desiable to resize the text length. Also, we want the model read texts in order so that it can efficiently predict what the next word is. This suggests that each new batch should begin precisely where the previous one left off. . So, the text stream will be cut into a certain number of batches (with batch size) with preserving the order of the tokens. Because we want the model to read continuous rows of the text. . To recap, at every epoch, we shuffle our collection of ducuments and cocatenate them into a stream of tokens. Then, that stream will be cut into a batch of fixed size consecutive mini stream. The model will read these mini stream in order and it will produce the same activation. . In FastAI, it is all done with LMDataLoader. . nums300 = toks300.map(num) . dl = LMDataLoader(nums300) . x,y = first(dl) x.shape, y.shape . (torch.Size([64, 72]), torch.Size([64, 72])) . the batch size is 64x72. 64 is the default batch size and 72 is the default sequence length. . Training a Text Classifier . Create a language model using DataBlock . By default, fastai handles tokenization and numericallization automatically when TextBlock is passed to DataBlock. . get_imdb = partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) dls_lm = DataBlock( blocks=TextBlock.from_folder(path, is_lm=True), get_items=get_imdb, splitter=RandomSplitter(0.1)).dataloaders(path, path=path, bs=128, seq_len=80) . Fine tuning the language model . Then, we are going to create a learner which is going to learn and predict the next word of a movie review. It will take the data from data loader, pretrained model (AWD_LSTM), Dropout technique and metrics into account. . learn = language_model_learner( dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16() . . 100.00% [105070592/105067061 00:07&lt;00:00] Then we will do training (fit_one_cycle instead of fine_tuning) because we will be saving the intermediate model results during the training process. . learn.fit_one_cycle(1,2e-2) . epoch train_loss valid_loss accuracy perplexity time . 0 | 3.935816 | 3.946663 | 0.296058 | 51.762356 | 11:19 | . After few miniutes, we got the accuracy of prediction using transfer learning which is about 29 percent. . In order to intermediately save the pretrained model, we can easily do it with pytorch and il will create a file in learn.path/models. Afterwards, we can load the content of the file without any difficulty. . learn.save(&#39;one_epoch_training&#39;) . Path(&#39;/home/nd258645/.fastai/data/imdb/models/one_epoch_training.pth&#39;) . learn.load(&#39;one_epoch_training&#39;) . &lt;fastai.text.learner.LMLearner at 0x7f646b89ba60&gt; . After loading the pre-saved model, we can unfreeze it and train it for few more epochs. Then, let&#39;s see the improvement of the accuracy. . learn.unfreeze() learn.fit_one_cycle(10,2e-3) . epoch train_loss valid_loss accuracy perplexity time . 0 | 3.715323 | 3.882135 | 0.303489 | 48.527718 | 11:47 | . 1 | 3.671195 | 3.838855 | 0.309147 | 46.472214 | 12:30 | . 2 | 3.589375 | 3.815329 | 0.312512 | 45.391689 | 12:23 | . 3 | 3.482135 | 3.809059 | 0.314260 | 45.107956 | 12:22 | . 4 | 3.368312 | 3.814509 | 0.314907 | 45.354500 | 11:48 | . 5 | 3.245186 | 3.834200 | 0.314792 | 46.256393 | 11:50 | . 6 | 3.130364 | 3.868983 | 0.313907 | 47.893631 | 12:59 | . 7 | 3.026153 | 3.904342 | 0.313124 | 49.617428 | 11:51 | . 8 | 2.938276 | 3.930502 | 0.311893 | 50.932560 | 12:14 | . 9 | 2.903000 | 3.942299 | 0.311487 | 51.536942 | 13:07 | . Then, we save our model except the last activation function layer. To do that, we can save it with save_encoder . learn.save_encoder(&#39;finetuned&#39;) . In this step, we have fune tuned the language model. Now, we will fine tune this language model using the IMDb sentiment labels. . Text generation . We can self create some random words and we can create sentences and each contains 40 words and we will predict the content of those with a kind of randomization. . TEXT = &quot;I liked this movie so&quot; N_WORDS = 40 N_SENTENCES = 2 preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)] . Let&#39;s see the generation of new inventing words . print(&quot; n&quot;.join(preds)) . i liked this movie so much . The acting was so well - done and the plot was really a bit off . But all of the actors , for the most part , were just hilarious . If you &#39;re looking i liked this movie so much i could n&#39;t help but be interested in this movie as a chick flick . The story line is great . The movie does a great job of taking itself to the classic destination . It . Creating the classifier DataLoaders . Previously, we built a language model to predict the next word of a document given the pre text. Now, we are going to move to the classifer which predict the sentiment of a document. . dls_clas = DataBlock( blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) ).dataloaders(path, path=path, bs=128, seq_len=72) . IndexError Traceback (most recent call last) Input In [34], in &lt;cell line: 1&gt;() -&gt; 1 dls_clas = DataBlock( 2 blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock), 3 get_y = parent_label, 4 get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), 5 splitter=GrandparentSplitter(valid_name=&#39;test&#39;) 6 ).dataloaders(path, path=path, bs=128, seq_len=72) File /home/tmpext4/nd258645/conda-env/lib/python3.8/site-packages/fastai/data/block.py:113, in DataBlock.dataloaders(self, source, path, verbose, **kwargs) 112 def dataloaders(self, source, path=&#39;.&#39;, verbose=False, **kwargs): --&gt; 113 dsets = self.datasets(source, verbose=verbose) 114 kwargs = {**self.dls_kwargs, **kwargs, &#39;verbose&#39;: verbose} 115 return dsets.dataloaders(path=path, after_item=self.item_tfms, after_batch=self.batch_tfms, **kwargs) File /home/tmpext4/nd258645/conda-env/lib/python3.8/site-packages/fastai/data/block.py:110, in DataBlock.datasets(self, source, verbose) 108 splits = (self.splitter or RandomSplitter())(items) 109 pv(f&#34;{len(splits)} datasets of sizes {&#39;,&#39;.join([str(len(s)) for s in splits])}&#34;, verbose) --&gt; 110 return Datasets(items, tfms=self._combine_type_tfms(), splits=splits, dl_type=self.dl_type, n_inp=self.n_inp, verbose=verbose) File /home/tmpext4/nd258645/conda-env/lib/python3.8/site-packages/fastai/data/core.py:328, in Datasets.__init__(self, items, tfms, tls, n_inp, dl_type, **kwargs) 326 def __init__(self, items=None, tfms=None, tls=None, n_inp=None, dl_type=None, **kwargs): 327 super().__init__(dl_type=dl_type) --&gt; 328 self.tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))]) 329 self.n_inp = ifnone(n_inp, max(1, len(self.tls)-1)) File /home/tmpext4/nd258645/conda-env/lib/python3.8/site-packages/fastai/data/core.py:328, in &lt;listcomp&gt;(.0) 326 def __init__(self, items=None, tfms=None, tls=None, n_inp=None, dl_type=None, **kwargs): 327 super().__init__(dl_type=dl_type) --&gt; 328 self.tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))]) 329 self.n_inp = ifnone(n_inp, max(1, len(self.tls)-1)) File /home/tmpext4/nd258645/conda-env/lib/python3.8/site-packages/fastcore/foundation.py:97, in _L_Meta.__call__(cls, x, *args, **kwargs) 95 def __call__(cls, x=None, *args, **kwargs): 96 if not args and not kwargs and x is not None and isinstance(x,cls): return x &gt; 97 return super().__call__(x, *args, **kwargs) File /home/tmpext4/nd258645/conda-env/lib/python3.8/site-packages/fastai/data/core.py:254, in TfmdLists.__init__(self, items, tfms, use_list, do_setup, split_idx, train_setup, splits, types, verbose, dl_type) 252 if do_setup: 253 pv(f&#34;Setting up {self.tfms}&#34;, verbose) --&gt; 254 self.setup(train_setup=train_setup) File /home/tmpext4/nd258645/conda-env/lib/python3.8/site-packages/fastai/data/core.py:272, in TfmdLists.setup(self, train_setup) 270 self.tfms.setup(self, train_setup) 271 if len(self) != 0: --&gt; 272 x = super().__getitem__(0) if self.splits is None else super().__getitem__(self.splits[0])[0] 273 self.types = [] 274 for f in self.tfms.fs: File /home/tmpext4/nd258645/conda-env/lib/python3.8/site-packages/fastcore/foundation.py:111, in L.__getitem__(self, idx) --&gt; 111 def __getitem__(self, idx): return self._get(idx) if is_indexer(idx) else L(self._get(idx), use_list=None) File /home/tmpext4/nd258645/conda-env/lib/python3.8/site-packages/fastcore/foundation.py:115, in L._get(self, i) 114 def _get(self, i): --&gt; 115 if is_indexer(i) or isinstance(i,slice): return getattr(self.items,&#39;iloc&#39;,self.items)[i] 116 i = mask2idxs(i) 117 return (self.items.iloc[list(i)] if hasattr(self.items,&#39;iloc&#39;) 118 else self.items.__array__()[(i,)] if hasattr(self.items,&#39;__array__&#39;) 119 else [self.items[i_] for i_ in i]) IndexError: list index out of range . Let&#39;s see some example of data set. . dls_clas.show_batch(max_n=5) . NameError Traceback (most recent call last) Input In [29], in &lt;cell line: 1&gt;() -&gt; 1 dls_clas.show_batch(max_n=5) NameError: name &#39;dls_clas&#39; is not defined .",
            "url": "https://dnlam.github.io/fastblog/2022/03/29/NLP.html",
            "relUrl": "/2022/03/29/NLP.html",
            "date": " • Mar 29, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Collaborative Filtering",
            "content": "General context . When we think about Netflix, we might have watched lots of movies that are science_fiction, action, horror etc. Netflix may not know these particular properties of the films you watched, but it would be able to see that other people that watched the same movies could watch other movies that you are not watching yet. By doing recommendation approach, Netflix can recommend us the contents of the movies that we have not watched before but relevant to what we liked. . This approach is called collaborative filtering. The key foundation idea is that of latent factors which decides what kinds of movies you want to watch. . Data set . Indeed, we can not have access to NEtflix&#39;s entire dataset of movie watching history, but there is a great dataset that we can yous, called MovieLen which contains tens millions of movies ranking. . from fastai.collab import * from fastai.tabular.all import * . path = untar_data(URLs.ML_100k) . . 100.15% [4931584/4924029 00:01&lt;00:00] The information of the movies is structured as a table, where each column are respectively user, movie, rating and timestamp. Then, we need to indicate them when reading the file with pandas. . ratings = pd.read_csv(path/&#39;u.data&#39;,delimiter=&#39; t&#39;, header=None, names=[&#39;user&#39;,&#39;movie&#39;,&#39;rating&#39;,&#39;timestamp&#39;]) . ratings.head() . user movie rating timestamp . 0 196 | 242 | 3 | 881250949 | . 1 186 | 302 | 3 | 891717742 | . 2 22 | 377 | 1 | 878887116 | . 3 244 | 51 | 2 | 880606923 | . 4 166 | 346 | 1 | 886397596 | . To have a more user-friendly interface, Figure below shows the same data cross-tabulated into a human-friendly table. As the example, the empty cells in the table are the things that we would like our model to fill in based on the other informations. . . Basically, our objective is to recommend the movies to the people that might like them. In order to weight for each movie, how much the match of each category it is, we use the factos range between -1 and 1. For example, in oder to represent the movie The Last Skywalker for each category of science-fiction, action and old movies, we could use an array. . last_skywalker = np.array([0.98,0.9,-0.9]) . Then we can score the interests of each user for each category by an array as well . user1 = np.array([0.8,0.6,-0.4]) . Then, we calculate the matche between the combination which is a dot product: . (user1*last_skywalker).sum() . 1.6840000000000002 . Since we dont know what the latent factors are, and we dont know how to score them for each user and movie, we should learn them. . Learning the Latent factors . Step 1 of this approach is to randomly initialize some parameters. These parameters will be set as latent factors for each user and movie. For the illustrative purposes, we will use 5. . Step 2 of this approach is to calculate our predictions. By simply applying dot product of each movie with the user, by doing so, we can ontain a great match if an particular user likes a category of movies and the latent movies factor shows a lot of action. . Step 3 is to calculate our loss between our prediction and already obtained data. . With this in place, we can optimize our parameters using SGD, such as to minimize the loss. . . Creating the DataLoaders . movies = pd.read_csv(path/&#39;u.item&#39;, delimiter=&#39;|&#39;, encoding=&#39;latin-1&#39;, usecols=(0,1), names=(&#39;movie&#39;,&#39;title&#39;), header=None) . movies.head() . movie title . 0 1 | Toy Story (1995) | . 1 2 | GoldenEye (1995) | . 2 3 | Four Rooms (1995) | . 3 4 | Get Shorty (1995) | . 4 5 | Copycat (1995) | . We will use merge the movies and our ratings . ratings = ratings.merge(movies) ratings.head() . user movie rating timestamp title . 0 196 | 242 | 3 | 881250949 | Kolya (1996) | . 1 63 | 242 | 3 | 875747190 | Kolya (1996) | . 2 226 | 242 | 5 | 883888671 | Kolya (1996) | . 3 154 | 242 | 3 | 879138235 | Kolya (1996) | . 4 306 | 242 | 5 | 876503793 | Kolya (1996) | . By using DataLoaders, it takes by default the first column fir user, the second column for the item and the third will be used for ratings. . dls = CollabDataLoaders.from_df(ratings,item_name=&#39;title&#39;,bs=64) dls.show_batch() . user title rating . 0 679 | Santa Clause, The (1994) | 3 | . 1 1 | Exotica (1994) | 4 | . 2 259 | Apocalypse Now (1979) | 5 | . 3 450 | Courage Under Fire (1996) | 4 | . 4 774 | True Lies (1994) | 1 | . 5 533 | Leaving Las Vegas (1995) | 1 | . 6 561 | Star Trek: The Wrath of Khan (1982) | 3 | . 7 683 | Father of the Bride (1950) | 3 | . 8 417 | So I Married an Axe Murderer (1993) | 3 | . 9 424 | English Patient, The (1996) | 4 | . Then, with Pytorch, we represent our movies and user latent factor tables as matrices . n_users = len(dls.classes[&#39;user&#39;]) n_movies = len(dls.classes[&#39;title&#39;]) n_factors=5 . user_factors = torch.randn(n_users,n_factors) movie_factors = torch.randn(n_movies,n_factors) . By looking up an index, we can find the factors of user and movie. It can be seen as a matrix product. By replacing our indices with one hot encoded vectors, we can represent it. . one_hot_3 = one_hot(3,n_users).float() # latent factors of user 3 user_factors.t() @ one_hot_3 . tensor([ 1.0129, -0.1466, -0.3618, 1.1011, -0.4564]) . Collaborative Filtering from Scratch . class DotProduct(Module): def __init__(self,n_user,n_movies,n_factors): self.user_factors = Embedding(n_users,n_factors) self.movie_factors = Embedding(n_movies, n_factors) def forward(self,x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return (users*movies).sum(dim=1) . in this class, forward is a special Pytorch method name to notify us that a new Pytoch Module has just been created. . Then, we will create a Learner to optimize the parameters. We will use the plain Leaner class here: . model = DotProduct(n_users,n_movies,50) learn = Learner(dls,model,loss_func=MSELossFlat()) . learn.fit_one_cycle(5,5e-3) . epoch train_loss valid_loss time . 0 | 1.339669 | 1.277186 | 00:09 | . 1 | 1.108406 | 1.090037 | 00:09 | . 2 | 0.967013 | 0.980453 | 00:08 | . 3 | 0.858148 | 0.892111 | 00:09 | . 4 | 0.790688 | 0.873726 | 00:09 | . To make the model slightly better, we can force those prediction between 0 and 5. Then, we need to apply sigmoid_range, like previous post. . class DotProduct(Module): def __init__(self,n_user,n_movies,n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users,n_factors) self.movie_factors = Embedding(n_movies, n_factors) self.y_range=y_range def forward(self,x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return sigmoid_range((users*movies).sum(dim=1),*self.y_range) . model = DotProduct(n_users,n_movies,50) learn = Learner(dls,model,loss_func=MSELossFlat()) learn.fit_one_cycle(5,5e-3) . epoch train_loss valid_loss time . 0 | 1.018326 | 0.995652 | 00:09 | . 1 | 0.892207 | 0.899135 | 00:08 | . 2 | 0.662484 | 0.868134 | 00:08 | . 3 | 0.471682 | 0.875005 | 00:08 | . 4 | 0.354474 | 0.880524 | 00:08 | . We will try to add bias to the weights and see what happens. . class DotProductBias(Module): def __init__(self,n_user,n_movies,n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users,n_factors) self.movie_factors = Embedding(n_movies, n_factors) self.y_range=y_range self.user_bias = Embedding(n_users,1) self.movie_bias = Embedding(n_movies,1) def forward(self,x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) res = (users*movies).sum(dim=1, keepdim=True) res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1]) return sigmoid_range((users*movies).sum(dim=1),*self.y_range) . model = DotProduct(n_users,n_movies,50) learn = Learner(dls,model,loss_func=MSELossFlat()) learn.fit_one_cycle(5,5e-3) . epoch train_loss valid_loss time . 0 | 0.967305 | 0.991018 | 00:09 | . 1 | 0.853391 | 0.898885 | 00:09 | . 2 | 0.670772 | 0.872404 | 00:09 | . 3 | 0.468066 | 0.882217 | 00:08 | . 4 | 0.354991 | 0.886515 | 00:08 | . In stead of being better, it becomes worse because it is overfitting very quickly. So we need to find a way to train with more epoch and avoid overfitting. To do that, we will use a regularization technique which is so-called weight decay . Weight decay . One possible way to reduce the overfitting effect is to reduce the capacity of the model which is basically how much space does it have to find answers. Weight decay or, L2 regularization, consists in adding of loss function the sum of all the weights squared. Then, to reduce the whole loss function, we need to reduce the weights. Then we reduce the likelihood of the big changes in the loss. As the results, the small changes in the weight can lead to the small changes in the loss. By doing that, we can prevent the model doing overfitting that happens with very sharp changes. . The downside of limiting the weights is that we limit the space of trying the possibilities. But it generalizes better . loss_with_wd = loss + wd * (parameters**2).sum() . model = DotProductBias(n_users,n_movies,50) learn = Learner(dls,model,loss_func=MSELossFlat()) learn.fit_one_cycle(5,5e-3,wd=0.1) . epoch train_loss valid_loss time . 0 | 1.036622 | 1.010733 | 00:09 | . 1 | 0.927190 | 0.930999 | 00:08 | . 2 | 0.801266 | 0.869415 | 00:08 | . 3 | 0.650525 | 0.836668 | 00:08 | . 4 | 0.576153 | 0.834967 | 00:09 | . By doing weight decay, as the results, we see the training loss increase but the validation loss slightly decrease. It means that the generalization works. . Creating Embedding module . Previously, we talked about embeding layer which is a shortcut of doing matrix multiplication for us and indexing the array. We can create our own embeding layer. . class T(Module): def __init__(self): self.a = nn.Parameter(torch.ones(3)) . By wrapping with nn.Parameter, Pytorch will assume that are parameters to be learned. . L(T().parameters()) . (#1) [Parameter containing: tensor([1., 1., 1.], requires_grad=True)] . Let&#39;s create a tensor as a parameter . def create_params(size): return nn.Parameter(torch.zeros(*size).normal_(0, 0.01)) . Let&#39;s use this to create DotProductBias again, but without Embedding: . class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = create_params([n_users, n_factors]) self.user_bias = create_params([n_users]) self.movie_factors = create_params([n_movies, n_factors]) self.movie_bias = create_params([n_movies]) self.y_range = y_range def forward(self, x): users = self.user_factors[x[:,0]] movies = self.movie_factors[x[:,1]] res = (users*movies).sum(dim=1) res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]] return sigmoid_range(res, *self.y_range) . Then we will train it again, we will see that there is no effect of embedding a layer. . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.950444 | 0.946037 | 00:12 | . 1 | 0.863254 | 0.873889 | 00:11 | . 2 | 0.721644 | 0.831558 | 00:11 | . 3 | 0.584056 | 0.819517 | 00:11 | . 4 | 0.491901 | 0.821385 | 00:11 | . Using fastai.collab . The structured above can be created using fastai.collab . learn = collab_learner(dls,n_factors=50,y_range=(0,5.5)) . learn.fit_one_cycle(5,5e-3,wd=0.1) . epoch train_loss valid_loss time . 0 | 0.956470 | 0.959027 | 00:10 | . 1 | 0.871686 | 0.870266 | 00:10 | . 2 | 0.729089 | 0.828011 | 00:08 | . 3 | 0.596814 | 0.816535 | 00:10 | . 4 | 0.490293 | 0.816821 | 00:10 | . Then, we can show the names of layers . learn.model . EmbeddingDotBias( (u_weight): Embedding(944, 50) (i_weight): Embedding(1665, 50) (u_bias): Embedding(944, 1) (i_bias): Embedding(1665, 1) ) . Now, we have succesfully trained a model. . Deep Learning for Collaborative Filtering . To turn our architecture into a deep learning model, the first step is to take the results of the embedding lookup and concatenate those activations together. This gives us a matrix which we can then pass through linear layers and nonlinearities in the usual way. . Since we&#39;ll be concatenating the embeddings, rather than taking their dot product, the two embedding matrices can have different sizes (i.e., different numbers of latent factors). fastai has a function get_emb_sz that returns recommended sizes for embedding matrices for your data, based on a heuristic that fast.ai has found tends to work well in practice: . embs = get_emb_sz(dls) class CollabNN(Module): def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100): self.user_factors = Embedding(*user_sz) self.item_factors = Embedding(*item_sz) self.layers = nn.Sequential( nn.Linear(user_sz[1]+item_sz[1], n_act), nn.ReLU(), nn.Linear(n_act, 1)) self.y_range = y_range def forward(self, x): embs = self.user_factors(x[:,0]),self.item_factors(x[:,1]) x = self.layers(torch.cat(embs, dim=1)) return sigmoid_range(x, *self.y_range) . model = CollabNN(*embs) . CollabNN creates our Embedding layers in the same way as previous classes in this chapter, except that we now use the embs sizes. self.layers is identical to the mini-neural net we created in &lt;&gt; for MNIST. Then, in forward, we apply the embeddings, concatenate the results, and pass this through the mini-neural net. Finally, we apply sigmoid_range as we have in previous models.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.01) . epoch train_loss valid_loss time . 0 | 0.945726 | 0.957030 | 00:16 | . 1 | 0.873854 | 0.894093 | 00:10 | . 2 | 0.881610 | 0.876000 | 00:10 | . 3 | 0.812847 | 0.863269 | 00:10 | . 4 | 0.778358 | 0.865262 | 00:10 | . learn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50]) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.963573 | 0.969183 | 00:12 | . 1 | 0.907858 | 0.930464 | 00:12 | . 2 | 0.892568 | 0.885867 | 00:11 | . 3 | 0.818842 | 0.858451 | 00:11 | . 4 | 0.735407 | 0.864158 | 00:11 | . &lt;/div&gt; .",
            "url": "https://dnlam.github.io/fastblog/2022/03/27/Collaborative_filtering.html",
            "relUrl": "/2022/03/27/Collaborative_filtering.html",
            "date": " • Mar 27, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Multi-Label Classification and Regression",
            "content": "Objective: . In this part, we will be looking on the other types of compiter vision problems, multi-label classification and regression. The first one is when you want to predict more than one label per image and the later occurs when our labels are one or several numbers - quantities in stead of categories. . Multi-label Classification . First, we will use PASCAL dataset which is famous of having more than one kind if classified object per image. . from fastai.vision.all import * path= untar_data(URLs.PASCAL_2007) . df = pd.read_csv(path/&#39;train.csv&#39;) df.head() . fname labels is_valid . 0 000005.jpg | chair | True | . 1 000007.jpg | car | True | . 2 000009.jpg | horse person | True | . 3 000012.jpg | car | False | . 4 000016.jpg | bicycle | True | . Constructing a DataBlock . Ultil now, we should see the differences between a Dataset and DataLoader. . Dataset:: is a collection which returns a tuple of independant and dependant variable for a single item | DataLoader:: is an iterator which provides a stream of mini-batches, where each mini-batch is a couple of a batch of independant and a batch of dependant variables. | . By using DataBlock, we will create our datasets and dataloader from scratch. . dblock=DataBlock() dsets = dblock.datasets(df) # datablock create datasets which contains training set and validation set len(dsets.train), len(dsets.valid) . (4009, 1002) . Let&#39;s grab the dependant variable and independant variable . x,y = dsets.train[0] x,y . (fname 001536.jpg labels tvmonitor person is_valid True Name: 764, dtype: object, fname 001536.jpg labels tvmonitor person is_valid True Name: 764, dtype: object) . x[&#39;fname&#39;],y[&#39;labels&#39;] . (&#39;001536.jpg&#39;, &#39;tvmonitor person&#39;) . dblock = DataBlock(get_x=lambda r: r[&#39;fname&#39;], get_y = lambda r: r[&#39;labels&#39;]) dsets = dblock.datasets(df) dsets.train[1] . (&#39;007772.jpg&#39;, &#39;cow&#39;) . Let&#39;s work with the complete parh of inputs . To open the path (independant variable) as an image, we will need a conversion for each of the thing in the tuple. we will need ImageBlock to open image and a specilized block to open the category, e.g MultiCategoryBlock . dblock = DataBlock(blocks=(ImageBlock,MultiCategoryBlock),get_x=get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (PILImage mode=RGB size=500x333, TensorMultiCategory([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])) . The long list of the output category is the label which encoded as one hot encoding . . idxs = torch.where(dsets.train[0][1]==1.)[0] dsets.train.vocab[idxs] . (#3) [&#39;bicycle&#39;,&#39;motorbike&#39;,&#39;person&#39;] . To separate validate items and training items, we use splitter . def splitter(df): train = df.index[~df[&#39;is_valid&#39;]].tolist() valid = df.index[df[&#39;is_valid&#39;]].tolist() return train,valid dblock = DataBlock(blocks=(ImageBlock,MultiCategoryBlock), splitter=splitter, get_x=get_x, get_y=get_y) dsets = dblock.datasets(df) dsets.valid[0] . (PILImage mode=RGB size=500x375, TensorMultiCategory([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])) . dblock = DataBlock(blocks=(ImageBlock,MultiCategoryBlock), splitter=splitter, get_x=get_x, get_y=get_y, item_tfms=RandomResizedCrop(128,min_scale=0.35)) dls=dblock.dataloaders(df) . dls.show_batch(nrows=1,ncols=3) . Now, our data is ready to be trained . Binary cross entropy . We will create our Learner. Basically, there are 4 main things in the Learner: . a DataLoader object | a model | an Optimizer | a loss function | . Then we will grab one batch of data and put it into independant and dependant variable. Then, we will pass the independant variable to our learning model and it will return the activation of the last layer. The activation at the last layer has size of (64,20) corresponding to the batch size(64) and output categories (20). Then, the objective is to calculate the probabilities of each of 20 categories. . learn=cnn_learner(dls,resnet18) learn.model.cuda() # model moved to CUDA x,y = dls.train.one_batch() activs = learn.model(x) activs.shape . torch.Size([64, 20]) . Naturally, the output of the last layer have not been normalized yet since the output of each categories is not within 0 and 1. Then, we need to scale it as mnist loss with the addition of log into it: . def binary_cross_entropy(inputs,targets): inputs = inputs.sigmoid() return torch.where(targets==1, 1-inputs, inputs).log().mean() . In pytorch, we can use it directly with F.binary_cross_entropy, and its module equivalent nn.BCELoss. which calculate cross entropy on a one-hot encoded target, but do not include initial sigmoid. To include it, we will want to use F.binary_cross_entropy_with_logits (or nn.BCELossWithLogitsLoss) which do both sigmoid and binary cross entropy in a single function. . We should note that we can not apply softmax and nll_loss because we might need to find multiple categories in a single image, so we can not restrict the sum of all activations to 1. . loss_func = nn.BCEWithLogitsLoss() loss = loss_func(activs,y) loss . TensorMultiCategory(1.0607, device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;) . Then, we need to build a metric which is accuracy to apply for multilabel problem. Previously, we built the accuracy for a single label which returns an argmax with highest probability of existance. It will not work in case of multi-label classification because we have more than one prediction on a single image. . So, the idea is to compare our activation with a certain threshold! Picking a good threshold is important, if we pick a threshold is too low, we will be failling to select correctly labeled object generally. . def accuracy_multi(inp,targ, thres=0.5, sigmoid=True): if sigmoid: inp = inp.sigmoid() return((inp&gt;thres)==targ.bool()).float().mean() . learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi,thres=0.2)) learn.fine_tune(3,base_lr=3e-3, freeze_epochs=4) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.939793 | 0.695435 | 0.234343 | 00:39 | . 1 | 0.824612 | 0.563170 | 0.289701 | 00:30 | . 2 | 0.600273 | 0.191579 | 0.840677 | 00:32 | . 3 | 0.357971 | 0.124514 | 0.938367 | 00:30 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.132505 | 0.116672 | 0.944721 | 00:31 | . 1 | 0.116956 | 0.107039 | 0.950498 | 00:31 | . 2 | 0.097030 | 0.104662 | 0.952470 | 00:32 | . In order to know which is the right value of threshold to pick, we will try several levels and see what works best. . preds,targs = learn.get_preds() xs = torch.linspace(0.05,0.95,29) accs = [accuracy_multi(preds,targs,thres=i,sigmoid=False) for i in xs] plt.plot(xs,accs) . [&lt;matplotlib.lines.Line2D at 0x2b98e2e24370&gt;] . Practically, we have used validation set to train the hyperparameters. . Regression . Different from classification where the set of dependant variables are set of categories, dependant variables in regression problem are continuous number, for instance, we can predict product purchases from given images, texts and tabular data. . As an example of regression, in the following, we will make a prediction of the facial posision in the images. We will use biwi headpose dataset for this. . Assemble the data . path.ls().sorted() . (#50) [Path(&#39;01&#39;),Path(&#39;01.obj&#39;),Path(&#39;02&#39;),Path(&#39;02.obj&#39;),Path(&#39;03&#39;),Path(&#39;03.obj&#39;),Path(&#39;04&#39;),Path(&#39;04.obj&#39;),Path(&#39;05&#39;),Path(&#39;05.obj&#39;)...] . There are 24 directories numbered from 01 to 24 (different persons photographed) and a corresponding .obj file. Let&#39;s look inside one of these directories . (path/&#39;01&#39;).ls().sorted() . (#1000) [Path(&#39;01/depth.cal&#39;),Path(&#39;01/frame_00003_pose.txt&#39;),Path(&#39;01/frame_00003_rgb.jpg&#39;),Path(&#39;01/frame_00004_pose.txt&#39;),Path(&#39;01/frame_00004_rgb.jpg&#39;),Path(&#39;01/frame_00005_pose.txt&#39;),Path(&#39;01/frame_00005_rgb.jpg&#39;),Path(&#39;01/frame_00006_pose.txt&#39;),Path(&#39;01/frame_00006_rgb.jpg&#39;),Path(&#39;01/frame_00007_pose.txt&#39;)...] . Inside each sub-directory, we have different frames, each of them come with an image ( _rgb.jpg) and a pose file ( _pose.txt). We can write a function that turns each image into a pose file. . img_files = get_image_files(path) def img2pose(x): return Path(f&#39;{str(x)[:-7]}pose.txt&#39; ) . img2pose(img_files[0]) . Path(&#39;06/frame_00113_pose.txt&#39;) . im = PILImage.create(img_files[0]) . To show the center of the head in each image, we will have a function that does that. . cal = np.genfromtxt(path/&#39;01&#39;/&#39;rgb.cal&#39;,skip_footer=6) def get_ctr(f): ctr=np.genfromtxt(img2pose(f),skip_header=3) c1 = ctr[0]*cal[0][0]/ctr[2] + cal[0][2] c2 = ctr[1]*cal[1][1]/ctr[2] + cal[1][2] return tensor([c1,c2]) . get_ctr(img_files[0]) . tensor([379.1756, 318.0512]) . We can pass this function to DataBlock as get_y, since it is responsible for lebelling each item. . biwi = DataBlock( blocks=(ImageBlock,PointBlock), get_items=get_image_files, get_y=get_ctr, splitter=FuncSplitter(lambda o: o.parent.name==&#39;13&#39;), batch_tfms=[*aug_transforms(size=(240,320)), Normalize.from_stats(*imagenet_stats)]) . dls = biwi.dataloaders(path) dls.show_batch(max_n=9, figsize=(8,6)) . /home/tmpext4/nd258645/conda-env/lib/python3.8/site-packages/torch/_tensor.py:1023: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release. torch.linalg.solve has its arguments reversed and does not return the LU factorization. To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack. X = torch.solve(B, A).solution should be replaced with X = torch.linalg.solve(A, B) (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:760.) ret = func(*args, **kwargs) . Training a model . Let&#39;s do the training using cnn_trainer, y_range is implemented in fastai using sigmoid_range and it tells us which range of dependant variable we expected to see. . learn = cnn_learner(dls,resnet18,y_range=(-1,1)) . By default, MSELoss is chosen for the loss function since it examines how close we are with the target. . dls.loss_func . FlattenedLoss of MSELoss() . Let&#39;s pick a good learning rate . learn.lr_find() . SuggestedLRs(valley=0.00363078061491251) . Then, we will try a learning rate of 0.5e-2 . lr=0.5e-2 learn.fine_tune(3,lr) . epoch train_loss valid_loss time . 0 | 0.054448 | 0.024040 | 02:13 | . epoch train_loss valid_loss time . 0 | 0.005767 | 0.000990 | 02:18 | . 1 | 0.002924 | 0.000274 | 02:15 | . 2 | 0.001518 | 0.000123 | 02:08 | . Amazingly, the obtained loss is around 0.000123 and it seems terrifically accurate. To show the initial targets and resulted predictions, we will show several outcomes: . learn.show_results(ds_idx=1,max_n=3,figsize=(6,8)) . So, we can build a really good regression model with using transfer learning and flexible API! . Conclusion . Previously, we worked with single label classification and we extended in this notebook multi-level classification and regression. To deal with different kind of tasks, choosing a right loss function is important! .",
            "url": "https://dnlam.github.io/fastblog/2022/03/24/Multilabel_classification.html",
            "relUrl": "/2022/03/24/Multilabel_classification.html",
            "date": " • Mar 24, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Image Classification with FastAI",
            "content": "Objectives . So far, from Part 1, we understood how to create and deploy a model. I practice, to make your model really works, there are a lots of details that we have to check including: . different types of layers | regularization methods | optimizers | how to put layers into architectures. | labelling techniques and much more | . In this post, we will enlighten them on. . Dogs, Cats and Pet Breeds . from fastai.vision.all import * path=untar_data(URLs.PETS) path.ls() (path/&quot;images&quot;).ls() . (#7393) [Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/Siamese_87.jpg&#39;),Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/chihuahua_126.jpg&#39;),Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/german_shorthaired_97.jpg&#39;),Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/Bombay_157.jpg&#39;),Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/Bengal_12.jpg&#39;),Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/japanese_chin_116.jpg&#39;),Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/havanese_109.jpg&#39;),Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/scottish_terrier_122.jpg&#39;),Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_146.jpg&#39;),Path(&#39;/home/nd258645/.fastai/data/oxford-iiit-pet/images/boxer_176.jpg&#39;)...] . In order to extract information from strings of dataset, we can use regular expression (regex). A regular expression (link) is a special string, written in regular expression language and specifies a general rule for deciding whether another string passes a test. As the example given below, we will take a file name from scratch and then we use regex to grap all the parts of regular expression that have parentheses around them. . fname=(path/&quot;images&quot;).ls()[1] re.findall(r&#39;(.+)_ d+.jpg$&#39;,fname.name) . [&#39;chihuahua&#39;] . In the next part, we will give an example of using regex to label the whole dataset by RegexLabeller. get_y will take RegexLabellerfunction and changes it to a function which will be passed the &#39;name&#39; attribute. . Then, 2 last lines resize and aug_transforms() do image augmentation. . pets = DataBlock(blocks=(ImageBlock,CategoryBlock), # independant and dependant variable get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;),&#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224,min_scale=0.75)) dls=pets.dataloaders(path/&quot;images&quot;) . /home/tmpext4/nd258645/conda-env/lib/python3.8/site-packages/torch/_tensor.py:1023: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release. torch.linalg.solve has its arguments reversed and does not return the LU factorization. To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack. X = torch.solve(B, A).solution should be replaced with X = torch.linalg.solve(A, B) (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:760.) ret = func(*args, **kwargs) . Presizing . Presizing will grap a square randomly in the original picture. Then the second step of aug_transform will grap a random warped crop (possibly rotated) and will turn that into a square. . Because these steps will change the images (lower quality) since it requires an interpolation after each step, so FastAI (Resize()) will coordinate the image transformation in a non lossy way. And only once at the end, we will do the interpolation. . . learn=cnn_learner(dls,resnet34,metrics=error_rate) learn.fine_tune(2) . /home/tmpext4/nd258645/conda-env/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . epoch train_loss valid_loss error_rate time . 0 | 1.483577 | 0.336734 | 0.116373 | 01:17 | . epoch train_loss valid_loss error_rate time . 0 | 0.523443 | 0.271582 | 0.094723 | 01:12 | . 1 | 0.313463 | 0.219852 | 0.077131 | 01:15 | . Cross-entropy loss . Cross-entropy loss is really much the same as MNIST loss we have defined before but it provides at least 2 benefits: . It works even when our dependant variable has more than 2 categories | Faster and more reliable training. | . The purpose of the Cross-Entropy loss is to take the output probabilities and measure the distance from the truth table. By means of model training, we will minimize the Cross-Entropy loss. . View the activation function and labels . Let&#39;s look at the mini-batch (64) of our training model . x,y=dls.one_batch() y . [&#39;Abyssinian&#39;, &#39;Bengal&#39;, &#39;Birman&#39;, &#39;Bombay&#39;, &#39;British_Shorthair&#39;, &#39;Egyptian_Mau&#39;, &#39;Maine_Coon&#39;, &#39;Persian&#39;, &#39;Ragdoll&#39;, &#39;Russian_Blue&#39;, &#39;Siamese&#39;, &#39;Sphynx&#39;, &#39;american_bulldog&#39;, &#39;american_pit_bull_terrier&#39;, &#39;basset_hound&#39;, &#39;beagle&#39;, &#39;boxer&#39;, &#39;chihuahua&#39;, &#39;english_cocker_spaniel&#39;, &#39;english_setter&#39;, &#39;german_shorthaired&#39;, &#39;great_pyrenees&#39;, &#39;havanese&#39;, &#39;japanese_chin&#39;, &#39;keeshond&#39;, &#39;leonberger&#39;, &#39;miniature_pinscher&#39;, &#39;newfoundland&#39;, &#39;pomeranian&#39;, &#39;pug&#39;, &#39;saint_bernard&#39;, &#39;samoyed&#39;, &#39;scottish_terrier&#39;, &#39;shiba_inu&#39;, &#39;staffordshire_bull_terrier&#39;, &#39;wheaten_terrier&#39;, &#39;yorkshire_terrier&#39;] . dls.vocab . [&#39;Abyssinian&#39;, &#39;Bengal&#39;, &#39;Birman&#39;, &#39;Bombay&#39;, &#39;British_Shorthair&#39;, &#39;Egyptian_Mau&#39;, &#39;Maine_Coon&#39;, &#39;Persian&#39;, &#39;Ragdoll&#39;, &#39;Russian_Blue&#39;, &#39;Siamese&#39;, &#39;Sphynx&#39;, &#39;american_bulldog&#39;, &#39;american_pit_bull_terrier&#39;, &#39;basset_hound&#39;, &#39;beagle&#39;, &#39;boxer&#39;, &#39;chihuahua&#39;, &#39;english_cocker_spaniel&#39;, &#39;english_setter&#39;, &#39;german_shorthaired&#39;, &#39;great_pyrenees&#39;, &#39;havanese&#39;, &#39;japanese_chin&#39;, &#39;keeshond&#39;, &#39;leonberger&#39;, &#39;miniature_pinscher&#39;, &#39;newfoundland&#39;, &#39;pomeranian&#39;, &#39;pug&#39;, &#39;saint_bernard&#39;, &#39;samoyed&#39;, &#39;scottish_terrier&#39;, &#39;shiba_inu&#39;, &#39;staffordshire_bull_terrier&#39;, &#39;wheaten_terrier&#39;, &#39;yorkshire_terrier&#39;] . Then, we can show the predictions (the activation results of final layer of our neural network) of one mini-batch . preds,_ = learn.get_preds(dl=[(x,y)]) preds[0] . tensor([1.1638e-05, 2.7098e-07, 7.4142e-05, 7.0873e-07, 4.7944e-07, 3.0185e-08, 1.1611e-05, 6.6417e-06, 1.0301e-05, 4.4232e-08, 1.4760e-07, 8.9016e-08, 5.7057e-09, 7.4013e-08, 5.1872e-07, 3.6632e-06, 4.5897e-07, 7.8231e-06, 5.5353e-07, 2.9222e-08, 2.3144e-08, 1.1901e-07, 1.5109e-06, 4.9122e-06, 4.9140e-05, 1.6393e-06, 4.1687e-07, 1.9792e-07, 9.9980e-01, 1.4814e-07, 4.8275e-08, 8.0303e-07, 3.9702e-07, 1.0734e-05, 6.0649e-08, 8.1256e-07, 1.1867e-06]) . The results return 37 probabilities between 0 and 1, which add up to 1 in total. To transform the activation of our model into predictions like this, we used soft-max activation function . Soft-max . Soft-max activation function is an extension of Sigmoid function to handle more than 2 categories. So we can obtain multi activations for multi label categories. The output of each final layer shows the likelyhood of the input item being a particular category. . As indicated in the example below, the unnormalized outputs of the neural network will be converted into probability by using Softmax fucntion. It measures how likely in terms of probability an input item belongs to a particular category. . . def softmax(x): return exp(x) / exp(x).sum(dim=1,keepdim=True) . When we apply Sigmoid activation function for each individual final layer, we can not guarantee that the sum of those outputs will be added up to 1. That&#39;s the reason for why we apply Softmax where it calculates the exponential of each outcome to the sum of exponential of all possible outcomes. . acts=torch.randn((6,2))*2 sm_acts=torch.softmax(acts,dim=1) sm_acts . tensor([[0.0734, 0.9266], [0.2011, 0.7989], [0.8459, 0.1541], [0.9867, 0.0133], [0.9817, 0.0183], [0.0025, 0.9975]]) . Because the exponential function grows very fast, so softmax activation function really want to pick one class among the others, so it will be ideal for training a classifier when we have various categories. . Entropy . The concept of entropy was proposed by Shannon in the field of information theory. By definition, Entropy of a random variable X measures the level of uncertainty ingerent in the variables possible outcomes. . For p(x) - probability distribution and a random variable X, entropy H(x) is defined as follows: . $$ H(X)= left { begin{matrix} - int_{x} p(x)* log{p(x)} ,&amp; text{if X is continuous} - sum_{x} p(x)* log{p(x)} , &amp; text{if X is discrete} end{matrix} right. $$The negative sign is used to deal with the logarithm of a value in range between 0 and 1. Thus, the greater value of entropy H(x) (events have comparable probabilities), the greater of uncertainty for probability distribution and vice versa. . In the context of Machine Learning, the comparison between predicted distribution and true distribution provides us the information about the differences between those. The larger gap between those distributions, the more uncertainty of our model will be. That is what the cross-entropy loss determine: . $$ L_{CE} = - sum _{i=1} ^{n} t_i * log {p_i}, quad text{for n classes} $$where $t_i$ is the truth label and $p_i$ is the Softmax probability of the $i^{th}$ class. . In Pytorch, cross-entropy loss are available in either class instance or function instance. By default, Pytorch loss function takes the mean of the loss of all categories, so we can use reduction=&#39;none&#39; to explicitly show the individual loss. . targ = tensor([0,1,0,0,1,1]) loss_func = nn.CrossEntropyLoss() loss_func(acts,targ) . tensor(1.1703) . targ = tensor([0,1,0,0,1,1]) F.cross_entropy(acts,targ) . tensor(1.1703) . nn.CrossEntropyLoss(reduction=&#39;none&#39;)(acts,targ) . tensor([2.6122e+00, 2.2455e-01, 1.6730e-01, 1.3394e-02, 4.0018e+00, 2.5001e-03]) . Model interpretation . In order for human to understand the optimization process, we can use confusion matrix to see where our model is doing well and where its doing badly . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12),dpi=60) . To have a cleaner view of what is going on, we can choose to show the cells of confusion matrix with the most incorrect predictions. . interp.most_confused(min_val=5) . [(&#39;american_pit_bull_terrier&#39;, &#39;staffordshire_bull_terrier&#39;, 10), (&#39;Egyptian_Mau&#39;, &#39;Bengal&#39;, 7)] . Improving our model . In this part, we will study a range of techniques to improve the training of our model and make it better. It includes: . Learning rate finder | Transfer Learning process | . The learning rate finder . One of the most critical part in training a model os to find a good learning rate. If the predefined learning rate is too small, it will takes many epochs to train our model and it might result in time consuming and overfitting exposure. IF it is set too high, the detrimental effects of error rate increasing might be seen. . learn = cnn_learner(dls,resnet34,metrics=error_rate) learn.fine_tune(1,base_lr=0.2) . epoch train_loss valid_loss error_rate time . 0 | 9.363783 | 43.051037 | 0.903924 | 01:27 | . epoch train_loss valid_loss error_rate time . 0 | 7.759617 | 29.350748 | 0.971583 | 01:25 | . In 2015, resercher Leslie Smith came up with a brilliant idea of learning rate finder link. His idea starts with a very small learning rate and examine the loss over one mini-batch. Then, he increase learning rate by a certain percentage and keep doing that until the loss get worse. That the point we know that we have over-reacted and we should choose a learning rate lower than this point. . lr_min,lr_steep = learn.lr_find() . ValueError Traceback (most recent call last) Input In [39], in &lt;cell line: 1&gt;() -&gt; 1 lr_min,lr_steep = learn.lr_find() ValueError: not enough values to unpack (expected 2, got 1) . learn = cnn_learner(dls,resnet34,metrics=error_rate) learn.fine_tune(2,base_lr=0.008) . epoch train_loss valid_loss error_rate time . 0 | 1.014067 | 0.404690 | 0.130582 | 01:29 | . epoch train_loss valid_loss error_rate time . 0 | 0.865397 | 0.785443 | 0.220568 | 01:28 | . 1 | 0.480247 | 0.299356 | 0.093369 | 01:24 | . Unfreezing and transfer learning . Transfer learning will take a set of parameters that have been previously trained and throw away the last layer of pretrained model. Then we replace by a layer with random weights and we train that. . The next task is to fine tune the newly added weight to align with our new objective. fine_tun is a method we called to operate that. It does 2 things: . train the randomly added layers for one epoch with all other layer frozen | unfreeze all of the layers and train them all for the number of epoch requested. | . learn.fine_tune?? . There are several parameters in fine_tune we shoud notice: . self.freeze(): make only the last layer&#39;s weight get step and freeze the other layers. | self.fit_one_cycle() : update the newly added weights in one cycle, it trains model without using fine_tune. In summary, it starts training at a lower learning rate, gradually increase ot for the first section of training and then gradually decrease it again for the last section of training. | . learn = cnn_learner(dls,resnet34,metrics=error_rate) learn.fit_one_cycle(3,3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.131244 | 0.339623 | 0.106225 | 01:30 | . 1 | 0.533571 | 0.272031 | 0.083897 | 01:26 | . 2 | 0.332284 | 0.235257 | 0.075101 | 01:26 | . learn.unfreeze() learn.lr_find() . SuggestedLRs(valley=6.918309736647643e-06) . learn.fit_one_cycle(6,lr_max=1e-5) . epoch train_loss valid_loss error_rate time . 0 | 0.247704 | 0.222702 | 0.071719 | 01:26 | . 1 | 0.245497 | 0.211186 | 0.069689 | 01:26 | . 2 | 0.211546 | 0.210558 | 0.067659 | 01:30 | . 3 | 0.198770 | 0.204503 | 0.063599 | 01:29 | . 4 | 0.190857 | 0.202116 | 0.066306 | 01:27 | . 5 | 0.177307 | 0.202761 | 0.067659 | 01:23 | . Intuitively speaking, in transfer learning, the deepest layers (beginning layers) of our pretrained model might not need as high a learning rate as the last ones, so we should probably use different learning rate approach for different layers. That what FastAI called discriminative learning rate . . Discriminative Learning rate . The idea behind this is simple: we apply lower learning rate for the early layers of the neural network and higher learning rate for the later ones. . In FastAI, we will pass a Python slice object anywhere that a learning rate is expected. The first value of slice is the learning rate of the starting layer and the last value of the slice is the final layer. The layers in between share the multiplicatively equidistant learning rate throughout the slice. . learn = cnn_learner(dls,resnet34,metrics=error_rate) learn.fit_one_cycle(3,3e-3) learn.unfreeze() learn.fit_one_cycle(12,lr_max=slice(1e-6,1e-4)) . epoch train_loss valid_loss error_rate time . 0 | 1.105149 | 0.319647 | 0.104195 | 01:32 | . 1 | 0.509816 | 0.279788 | 0.093369 | 01:28 | . 2 | 0.319550 | 0.241098 | 0.087280 | 01:32 | . epoch train_loss valid_loss error_rate time . 0 | 0.257214 | 0.241996 | 0.083221 | 01:32 | . 1 | 0.256226 | 0.228803 | 0.078484 | 01:31 | . 2 | 0.240209 | 0.225714 | 0.077131 | 01:33 | . 3 | 0.202059 | 0.217118 | 0.071042 | 01:25 | . 4 | 0.179049 | 0.209164 | 0.070365 | 01:25 | . 5 | 0.177175 | 0.212497 | 0.065629 | 01:28 | . 6 | 0.165256 | 0.202155 | 0.067659 | 01:31 | . 7 | 0.150717 | 0.210655 | 0.070365 | 01:33 | . 8 | 0.147985 | 0.206712 | 0.063599 | 01:34 | . 9 | 0.132928 | 0.202588 | 0.066306 | 01:33 | . 10 | 0.136363 | 0.203536 | 0.066306 | 01:24 | . 11 | 0.122196 | 0.202970 | 0.064276 | 01:25 | . Deeper Architecture . It literally increase the numbers of layers of our architecture (more activation functions, linear model). For instance, resnet architecture comes with 18,34,50,101,152 layer variants, pretrained with ImageNet. A larger version of the resnet returns a better training loss, but it can suffer more from overfitting. . In additions, the bigger model and larger batch-size is, the memory requirement for GPU is higher. . Another downside of training data with deeper architecture is time consuming, as it will take more time to train a larger model.µ . Take home messages . So, in this part, we have learned some important practical tips: . Preparing data for modelling (presizing) | Fitting the model (learning rate finder, unfreezing, fine_tune, discriminative learning rate, epochs, deeper architecture) | Entropy loss discusison | .",
            "url": "https://dnlam.github.io/fastblog/2022/03/20/image_classification_fastai.html",
            "relUrl": "/2022/03/20/image_classification_fastai.html",
            "date": " • Mar 20, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Introduction about FastAI",
            "content": "1. What is FastAI? . FasiAI is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and low-level components that can be mixed and matched to build new approaches. . FastAI libraries include: . a dispatch system for Python along with a semantic type hierarchy for tensors | a GPU-optimized computer vision library | an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4-5 lines of code. | a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training | a new data block API | . The design of FastAI follows layered structure where we want the clarity and development speed of Keras and the customizability of PyTorch, which is not possible to be achieved both for the other frameworks. . FastAI was co-founded by Jeremy Howard, who is a data scientist, researcher, developer, educator, and entrepreneur, and Rachel Thomas, who is a professor of practice at Queensland University of Technology. . 1.1. FastAI by example . Let&#39;s go deeper to their FastAI codes to see how it works. Here is an example of how to fine-tune an ImageNet model on the Oxford IIT Pets dataset and achieve close to state-of-the-art accuracy within a couple of minutes of training on a single GPU. . from fastai.vision.all import * path = untar_data(URLs.PETS)/&#39;images&#39; . search_images_bing . &lt;function fastbook.search_images_bing(key, term, min_sz=128, max_images=150)&gt; . def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func(path, get_image_files(path), valid_pct= 0.2, seed= 42, label_func= is_cat, item_tfms= Resize(224)) learn = cnn_learner(dls, resnet34, metrics= error_rate) learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.176250 | 0.017708 | 0.007442 | 01:00 | . epoch train_loss valid_loss error_rate time . 0 | 0.062521 | 0.018719 | 0.005413 | 01:01 | . Each line of given code does one important task: . The second line (path = untar_data(URLs.PETS)/&#39;images&#39;) downloads a standard dataset from the fast.ai datasets collection (if not previously downloaded) to a configurable location (~/.fastai/data by default), extracts it (if not previously extracted), and returns a pathlib.Path object with the extracted location. | Then, dls=ImageDataLoaders.from_name_func(...) sets up the DataLoaders object and represents a combination of training and validation data. | . After defining DataLoaders object, we can easily look at the data with a single line of code. . dls.show_batch() . Let&#39;s analyze the parameters inside the ImageDataLoader: . valid_pct is the percentage of validation set compared to training set to avoid over-fitting. By defaults, valid_pct=0.2. As being quoted by Jeremy &quot;Overfitting is the single most important and challenging issue. It is easy to create a model that does a great job at making predictions on the exact data it has been trained on, but it is much harder to make accurate predictions on data the model has never seen before.&quot; | . Afterwards, we created a Learner, which provides an abstraction combining an optimizer, a model, and the data to train. THis line of code learn = cnn_learner(dls, resnet34, metrics= error_rate) will download an ImageNet-pretrained model, if not already available, remove the classification head of the model, and set appropriate defaults for the optimizer, weight decay, learning rate and so forth. . Basically, a Learner contains our data (i.e dls), architecture (i.e. resnet34) which is a mathematical function that we are optimizing and a metrics (i.e, error_rate). a Learner will figure out which are the best parameters for the architecture to match the label in the dataset. . When we are talking about the metrics, which is a function that measures the quality of the model’s predictions using the validation set, it should be noted that the metrics is not necessarily the same as loss. The loss measures how parameters changing influences the results of performances (better or worse). . To fits the model, the next line of code learn.fine_tune(1) tells us how to do. Model fitting is simply looking at how many times to look at each image (epochs). Instead of using fit, we use fine_tune method because we started with a pre-trained model and we don&#39; want to throw away all the capabilities that it already has. By performing fine_tune, the parameters of a pretrained model are updated by training for additional epochs using a different task to that used for pretraining. . In sums, fine_tune is used for transfer learning, in which we used a pretrained model for a task different to what is was originally trained for. . 2. End-to-End Digit Classifier with FastAI . 2.1. From Data to DataLoaders . To train our model with images, the first thing we should consider is the sizes of image inputs, because we don&#39;t feed the model one image at a time but a several of them (mini-batch ). To group them in a big array (usually called a tensor )) that is going to go through our model, they all need to be of the same size. In FastAI, the size modification of each single image, category is done via Item transform), for example Resize() function. . The, a mini-batch of items will be ready to be fed to GPU via DataLoaders Class. By default, fastai will give us 64 items at a time, all stacked up into a single tensor. . Instead of Resize, RandomResizeCrop is also super popupar since it change how we look at the same image differently on each epoch and it is a simple technique to avoid overfitting. . Data Augmentation . Data augmentation refers to creating random variations of our input data, such that they appear different but do not change the meaning of the data. One of the best way to do data augmentation is to use &lt;/i&gt; aug_transform()&lt;/i&gt;. It will return a list of different augmentation (e.g: contrast, bright , rotation etc). . It should be noted that the data augmentation is applied into a batch of equally sized items. So, we can apply these augmentations to an entire batch of them using GPU. . 2. Training a Digit Classifier . In this part, we will enlighten the role of . Firstly, we will download the well known MNIST dataset using fastAI . path = untar_data(URLs.MNIST_SAMPLE) . . 100.14% [3219456/3214948 00:00&lt;00:00] Then, we will look at the train folder which contains image digits of &#39;3&#39; and &#39;7&#39;. . threes=(path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens=(path/&#39;train&#39;/&#39;7&#39;).ls().sorted() threes . (#6131) [Path(&#39;train/3/10.png&#39;),Path(&#39;train/3/10000.png&#39;),Path(&#39;train/3/10011.png&#39;),Path(&#39;train/3/10031.png&#39;),Path(&#39;train/3/10034.png&#39;),Path(&#39;train/3/10042.png&#39;),Path(&#39;train/3/10052.png&#39;),Path(&#39;train/3/1007.png&#39;),Path(&#39;train/3/10074.png&#39;),Path(&#39;train/3/10091.png&#39;)...] . Let&#39;s look at one particular handwriting image in the &#39;7&#39; folder . im7_path = sevens[1] im7=Image.open(im7_path) im7 . To transform an image into a numeric value, we can use array method which is a part of Numpy array. For instance, to show a few numbers from the image: . array(im7)[7:16,8:16] . array([[ 0, 0, 15, 157, 254, 197, 0, 0], [ 0, 9, 220, 254, 254, 230, 104, 0], [ 0, 169, 254, 254, 231, 126, 40, 11], [183, 251, 254, 226, 81, 70, 180, 229], [254, 254, 255, 254, 254, 254, 255, 254], [254, 254, 254, 254, 253, 250, 212, 169], [254, 181, 77, 77, 48, 0, 0, 0], [195, 29, 0, 0, 0, 0, 0, 75], [ 0, 0, 0, 0, 0, 0, 59, 217]], dtype=uint8) . tensor(im7)[7:16,8:16] . tensor([[ 0, 0, 15, 157, 254, 197, 0, 0], [ 0, 9, 220, 254, 254, 230, 104, 0], [ 0, 169, 254, 254, 231, 126, 40, 11], [183, 251, 254, 226, 81, 70, 180, 229], [254, 254, 255, 254, 254, 254, 255, 254], [254, 254, 254, 254, 253, 250, 212, 169], [254, 181, 77, 77, 48, 0, 0, 0], [195, 29, 0, 0, 0, 0, 0, 75], [ 0, 0, 0, 0, 0, 0, 59, 217]], dtype=torch.uint8) . The beauty of using pytorch tensor over Numpy array is that the calculation of Pytorch tensor can be done in GPU. . Evenmore, we can use Panda to represent numeric values of an image because it has a very convenient thing which is so-called background_gradient that turn the background into gradient . im7_t = tensor(im7) df=pd.DataFrame(im7_t[7:20,7:20]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . &nbsp; 0 1 2 3 4 5 6 7 8 9 10 11 12 . 0 0 | 0 | 0 | 15 | 157 | 254 | 197 | 0 | 0 | 0 | 0 | 0 | 18 | . 1 0 | 0 | 9 | 220 | 254 | 254 | 230 | 104 | 0 | 0 | 0 | 65 | 216 | . 2 0 | 0 | 169 | 254 | 254 | 231 | 126 | 40 | 11 | 70 | 180 | 254 | 254 | . 3 40 | 183 | 251 | 254 | 226 | 81 | 70 | 180 | 229 | 254 | 254 | 254 | 254 | . 4 208 | 254 | 254 | 255 | 254 | 254 | 254 | 255 | 254 | 254 | 254 | 254 | 254 | . 5 254 | 254 | 254 | 254 | 254 | 253 | 250 | 212 | 169 | 125 | 167 | 254 | 254 | . 6 254 | 254 | 181 | 77 | 77 | 48 | 0 | 0 | 0 | 128 | 254 | 254 | 253 | . 7 157 | 195 | 29 | 0 | 0 | 0 | 0 | 0 | 75 | 248 | 254 | 254 | 139 | . 8 0 | 0 | 0 | 0 | 0 | 0 | 0 | 59 | 217 | 254 | 254 | 170 | 15 | . 9 0 | 0 | 0 | 0 | 0 | 0 | 15 | 217 | 254 | 254 | 214 | 15 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 5 | 113 | 254 | 254 | 238 | 55 | 0 | 0 | . 11 0 | 0 | 0 | 0 | 2 | 89 | 254 | 254 | 239 | 81 | 0 | 0 | 0 | . 12 0 | 0 | 0 | 0 | 141 | 254 | 254 | 240 | 106 | 0 | 0 | 0 | 0 | . As we can see, the background white pixels are stored as the number zero, black is 255, and shaded grey are something in between. In MNIST dataset, an entire image contains 28 pixels across and 28 pixels down, for a total of 768 pixels. . In the next mission, we will be going to create a model which help us to recognize &#39;3&#39; and &#39;7&#39; . First, we will create a list of all sevens and threes images by using tensor. . seven_tensors = [tensor(Image.open(o)) for o in sevens] three_tensors = [tensor(Image.open(o)) for o in threes] len(three_tensors),len(seven_tensors) . (6131, 6265) . show_image(three_tensors[2]) three_tensors[2].shape . torch.Size([28, 28]) . Then, we use Machine Learning approach that is described by Dr.Samuel to solve the differentiation problem: . &lt;/i&gt; &quot;Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would “learn” from its experience.&quot; &lt;/i&gt; . So, lets think about a function with parameter. Instead of finding an ideal image and compare every single image with an ideal image, we come up with a weight for each pixel of the image. The accumulated information of the weighted pixels gives us valuable information to differentiate between images. . To be more specific, we will build a Machine Learning classifier according to the following steps: . Initialize the weights. | For each image, use these weights to predict whether it appears to be a 3 or a 7. | Based on these predictions, calculate how good the model is (its loss). | Calculate the gradient, which measures for each weight how changing that weight would change the loss. | Step (that is, change) all the weights based on that calculation. | Go back to step 2 and repeat the process. | Iterate until you decide to stop the training process (for instance, because the model is good enough or you don’t want to wait any longer). | . . Calculate Gradients . Pytorch has a built-in engine that helps us to calculate gradient effeciently and simply. In order to do this, we start with a tensor and it comes up with a special method requires_grad()_. The purpose of using this method is when we perform any calculation on a tensor, it will remember that calculation it does so that we can take the derivatives later. Then we will call a special method backward which refers to the back propagation and do the derivative for us. Afterwards, we can view the gradients by checking the grad attribute of our tensor. . def f(x): return x**2 xt=tensor(3.).requires_grad_() xt yt=f(xt) yt yt.backward() xt.grad . tensor(6.) . Stepping with a learning rate . Deciding to change our parameter based on the value of the gradients is an important part. Gradient tells us the slop of a function, but does not tell us exactly how far to adjust our parameters. That&#39;ss where the learning rate appears. . w -= gradient(w) * lr . The update of the parameters will be inversed to the gradient and is multiplied by a learning rate. If we take a learning rate is too small, it will be needing more time for our algorithm to converge. If we take a large learning rate, it can results in the loss getting even worse. So picking up a good learning rate is really important. . Getting back to the MNIST, we need gradient to improve our model using SGD. In order to calculate gradient, we need some loss function that represent us how good our model is. . # stack the tensor together stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes = torch.stack(three_tensors).float()/255 # create the items and labels train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28) train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1) train_x.shape,train_y.shape # create dataset dset = list(zip(train_x,train_y)) . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape,valid_7_tens.shape valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) . Step 1: Initialize parameter . def init_params(size,std=1.0): return (torch.randn(size)*std).requires_grad_() . weights=init_params((28*28,1)) bias = init_params(1) . We can now calculate a prediction for one image . (train_x[0]*weights.T).sum() + bias . tensor([-3.8565], grad_fn=&lt;AddBackward0&gt;) . By utilising the power of a GPU, we can predict a set of images by using matrix multiplication to loop between the pixels of an image and between images . def linear(xb): return xb@weights + bias preds = linear(train_x) preds . tensor([[-3.8565], [-5.0067], [-9.2098], ..., [-8.2678], [ 0.1213], [ 1.4608]], grad_fn=&lt;AddBackward0&gt;) . To check out the accuracy, to decide if an output represents a 3 or 7, we can simply apply binary method for that. if the output is greater than 0, it represent a 3 and vice versa. . corrects = (preds&gt;0.0).float() == train_y corrects . tensor([[False], [False], [False], ..., [ True], [False], [False]]) . Notice that we can not apply the accuracy for the loss function here because a small changes of paramters does not lead to the significant changes of the results, so we need to build a new loss function to estimate the prediction. The following function will give a first try of measuring the distance between predictions and targets: . def mnist_loss(predictions,targets): return torch.where(targets==1,1-predictions,predictions).mean() . One problem with mnist_loss is that it assumes the predictions are always between 0 and 1. Then, we need to ensure that it is always the case. That&#39;s the place for the activation function - Sigmoid function . Sigmoid . def sigmoid(x): return 1/(1+torch.exp(-x)) . plot_function(torch.sigmoid, title=&#39;Sigmoid&#39;,min=-4,max=4) . /home/tmpext4/nd258645/conda-env/lib/python3.8/site-packages/fastbook/__init__.py:74: UserWarning: Not providing a value for linspace&#39;s steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/RangeFactories.cpp:25.) x = torch.linspace(min,max) . Then, let&#39;s update mnist_loss with sigmoid function: . def mnist_loss(predictions,targets): predictions=predictions.sigmoid() return torch.where(targets==1,1-predictions,predictions).mean() . SGD and mini-batches . In order to change or update the weight, we will walk through the step method (optimisation step). To take an optimiser step we need to calculate the loss over one or more data items. If we perform the optimisation step for every single item, it would take a very long time. On the other hand, if we perform the step for the whole data set at once, it could return an unstable gradient. . So, we will take a compromise between the two: we calculate the average loss for a few data items at a time(mini-batches). The number of data items in a batch is call batch size . A larger batch size means you will get more accurate and stable estimation of our dataset&#39;s gradienton the loss function, but it will take longer and you will get less mini-batches per epoch. Then, choosing a good batch size is one of the decision that is to help deep learning to train our model quickly and accurately. . In Pytorch and fastAI, there is a class that will do the shuffling and minibatch collation for us, called DataLoader . coll = range(15) dl = DataLoader(coll,batch_size=5,shuffle=True) list(dl) . [tensor([ 3, 12, 8, 10, 2]), tensor([ 9, 4, 7, 14, 5]), tensor([ 1, 13, 0, 6, 11])] . dl = DataLoader(dset,batch_size=256) valid_dl = DataLoader(valid_dset,batch_size=256) # def calc_grad(xb,yb,model): preds=model(xb) loss=mnist_loss(preds,yb) loss.backward() batch=train_x[:4] calc_grad(batch,train_y[:4],linear) weights.grad.mean(),bias.grad . (tensor(-0.0029), tensor([-0.0204])) . calc_grad(batch,train_y[:4],linear) weights.grad.mean(),bias.grad . (tensor(-0.0039), tensor([-0.0272])) . When we will perform the calc_grad twice, although we have not changed anything related to the weights, but the gradient results return different values!!! The reason for that is that loss.backward actually adds the gradients of loss into any gradients that are currently stored. So we have to set the current gradients to zero first. . weights.grad.zero_() bias.grad.zero_(); . Then, we will update the weights and bias based on the gradient and learning rate. . def train_epoch(model,lr,params): for xb,yb in dl: calc_grad(xb,yb,model) for p in params: p.data -= p.grad*lr p.grad.zero_() . (preds&gt;0.0).float() == train_y . tensor([[False], [False], [False], ..., [ True], [False], [False]]) . Then, we calculate the accuracy . def batch_accuracy(xb,yb): preds = xb.sigmoid() corrects=(preds&gt;0.5)==yb return corrects.float().mean() . batch_accuracy(linear(batch),train_y[:4]) . tensor(0.) . def validate_epoch(model): accs = [batch_accuracy(model(xb),yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(),4) . validate_epoch(linear) . 0.4085 . Let&#39;s train in one epoch . lr = 1. params=weights,bias train_epoch(linear,lr,params) validate_epoch(linear) . 0.5849 . for i in range(20): train_epoch(linear,lr,params) print(validate_epoch(linear),end=&#39; &#39;) . 0.973 0.973 0.9735 0.9745 0.9749 0.9754 0.9759 0.9759 0.9759 0.9764 0.9774 0.9779 0.9779 0.9784 0.9784 0.9784 0.9789 0.9794 0.9794 0.9794 . So, we have succesfully built a SGD optimizer of a simple linear function anf get the accuracy upto 97.94% . Creating an optimizer . In order to automate the initialization of an optimizer, Pytorch provides some useful functions to replace our linear() function with Pytorch&#39;s nn.Linear module. . nn.Linear does the same thing as our init_params and Linear together. It contains both weights and bias in a single class. . linear_model = nn.Linear(28*28,1) w,b = linear_model.parameters() w.shape,b.shape . (torch.Size([1, 784]), torch.Size([1])) . The, we can use this information to create an optimizer: . class BasicOptim: def __init__(self,params,lr): self.params, self.lr= list(params),lr def step(self,*args,**kwargs): for p in self.params: p.data-= p.grad.data*self.lr def zero_grad(self,*args,**kwargs): for p in self.params: p.grad=None . opt = BasicOptim(linear_model.parameters(),lr) . Then, the new training loop should be: . def train_epoch(model): for xb,yb in dl: calc_grad(xb,yb,model) opt.step() opt.zero_grad() def train_model(model,epochs): for i in range(epochs): train_epoch(model) print(validate_epoch(model), end=&#39; &#39;) . train_model(linear_model,30) . 0.4932 0.8076 0.8554 0.917 0.935 0.9487 0.9575 0.9633 0.9653 0.9677 0.9697 0.9716 0.9736 0.9751 0.976 0.9765 0.9775 0.9775 0.9785 0.9785 0.979 0.979 0.979 0.979 0.9795 0.9795 0.9799 0.9804 0.9809 0.9814 . In FastAI, it provides us an API of SGD class which does the same thing as BasicOptim . linear_model = nn.Linear(28*28,1) opt = SGD(linear_model.parameters(),lr) train_model(linear_model,30) . 0.4932 0.789 0.853 0.9155 0.935 0.9492 0.9555 0.9638 0.9658 0.9672 0.9697 0.9716 0.9731 0.9751 0.9755 0.977 0.9775 0.978 0.978 0.9785 0.979 0.979 0.979 0.9795 0.9795 0.9795 0.9799 0.9804 0.9804 0.9814 . FastAI also provides Learner.fit, which we can use instead of train_model. To create a Learner we first need to create DataLoader, by passing in our training and validation DataLoaders. . dls=DataLoaders(dl,valid_dl) . learn=Learner(dls,nn.Linear(28*28,1),opt_func=SGD,loss_func=mnist_loss,metrics=batch_accuracy) . learn.fit(10,lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.009775 | 0.017821 | 0.985280 | 00:00 | . 1 | 0.009761 | 0.017807 | 0.985280 | 00:00 | . 2 | 0.009744 | 0.017793 | 0.985280 | 00:00 | . 3 | 0.009726 | 0.017780 | 0.985280 | 00:00 | . 4 | 0.009707 | 0.017767 | 0.985280 | 00:00 | . 5 | 0.009688 | 0.017754 | 0.985280 | 00:00 | . 6 | 0.009669 | 0.017742 | 0.985280 | 00:00 | . 7 | 0.009650 | 0.017730 | 0.985280 | 00:00 | . 8 | 0.009631 | 0.017717 | 0.985280 | 00:00 | . 9 | 0.009613 | 0.017706 | 0.985280 | 00:00 | . Adding a non-linearity . So far, we studied a general procedure for optimising the parameters of a function by using a simple linear classifier. To make it a bit more sophisticated, we need to add a non-linearity between two linear classifiers, and this gives us a neural network . def simple_net(xb): s1=xb@w1+b1 res=s1.max(tensor(0.0)) s2=res@w2+b2 return s2 . The little s1.max(tensor(0.0)) is called rectified linear unit (RELU). In Pytorch, it is also available as F.relu . plot_function(F.relu) . The addition of nonlinear function, we can provide what Universal Approximation Theorem says, which can represent any arbitrary function. . We can replace the initialization of the basic neural network by taking advantage of Pytorch: . simple_net = nn.Sequential( nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1) ) . nn.Sequential is a module that calls each of the listed layers or function in turn. . learn=Learner(dls,simple_net,opt_func=SGD,loss_func=mnist_loss,metrics=batch_accuracy) #hide_output learn.fit(40,0.1) . plt.plot(L(learn.recorder.values).itemgot(2)) . [&lt;matplotlib.lines.Line2D at 0x2b1b144271c0&gt;] . For deeper models, we may need to use a lower learning rate and a few more epochs. In practice, we can freely to set many layers as well as the nonlinearity between layers. However, the deeper the model gets, the harder it is to optimize the parameters in practice. So why we would use a deeper model? The reason is the performance. With the deeper model, it turns out that we can use smaller matrices , with more layers, and get better results that we would get with larger matrices and few layers. . Here is what happens when we train an 18-layer model . dls=ImageDataLoaders.from_folder(path) learn=cnn_learner(dls,resnet18,pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(1,0.1) . /home/tmpext4/nd258645/conda-env/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . epoch train_loss valid_loss accuracy time . 0 | 0.156772 | 0.021748 | 0.995584 | 00:32 | .",
            "url": "https://dnlam.github.io/fastblog/2022/03/17/intro_fastai.html",
            "relUrl": "/2022/03/17/intro_fastai.html",
            "date": " • Mar 17, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "The Beauty of Neural Network",
            "content": "The Sample Problem . Consider a system that tries to map groups of 3 inputs to some corresponding output which is a single number. In the following picture, we&#39;ll show each set of 3 inputs as a row of a matrix $X$, and each output as the corresponding row of $Y$: . $$ overbrace{ left[ { begin{array}{ccc} 0 &amp; 0 &amp; 1 0 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 end{array} } right] }^{X} rightarrow overbrace{ left[ { begin{array}{c} 0 0 1 1 end{array} } right] }^Y. $$Even though this system has an exact solution (namely, $Y$ equals the first column of $X$), usually we&#39;ll need to be satisfied with a system that maps our inputs $X$ to some approximate &quot;prediction&quot; $ tilde{Y}$, which we hope to bring closer to the &quot;target&quot; $Y$ by means of successive improvements. . The way we&#39;ll get our prediction $ tilde{Y}$ is by means of a weighted sum of each set of 3 inputs, and some nonlinear function $f$ which we call the &quot;activation function&quot; (or just &quot;activation&quot;). Pictorially, the process looks like the following, for each row $i$ of $X$ and $Y$, (where the columns of $X$ are shown arranged vertically instead of horizonally): . . In terms of matrix multiplication, since X is a 4x3 matrix, and Y is a 4x1 matrix, that implies that our weights should be a 3x1 matrix consisting of (unknown) values $w_0$, $w_1$ and $w_2$. The calculation can be written as: . $$ f left( overbrace{ left[ { begin{array}{ccc} 0 &amp; 0 &amp; 1 0 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 end{array} } right] }^ text{X} overbrace{ left[ { begin{array}{c} w_0 w_1 w_2 end{array} } right] }^{w} right) = overbrace{ left[ { begin{array}{c} 0 0 1 1 end{array} } right] }^{ tilde{Y}} $$Our nonlinear activation function $f$ is taken to operate on each row element one at a time, and we&#39;ll let $f_i$ denote the $i$th row of this completed activation, i.e.: . $$ f_i = f left( sum_j X_{ij}w_j right) = tilde{Y}_i . $$The particular activation function we will use is the &quot;sigmoid&quot;, . $$ f(x) = {1 over{1+e^{-x}}}, $$-- click here to see a plot of this function -- which has the derivative . $$ {df over dx} = {e^{-x} over(1 + e^{-x})^2} $$which can be shown (Hint: exercise for &quot;mathy&quot; students!) to simplify to $$ {df over dx}= f(1-f). $$ . The overall problem then amounts to finding the values of the &quot;weights&quot; $w_0, w_1,$ and $w_2$ so that the $ tilde{Y}$ we calculate is as close to the target $Y$ as possible. . To do this, we will seek to minimize a loss function defined as a sum across all data points we have, i.e. all 4 rows. The loss function $L$ we will choose is the mean square error loss, or MSE, we will use a &#39;better&#39; loss function for this problem): . $$ L = {1 over N} sum_{i=0}^{N-1} left[ tilde{Y}_i - Y_i right]^2, $$or in terms of the activation function $$ L = {1 over N} sum_{i=0}^{N-1} left[ f_i - Y_i right]^2. $$ . Each of the weights $w_j$ ($j=0..2$) will start with random values, and then be updated via gradient descent, i.e. . $$ w_j^{new} = w_j^{old} - alpha{ partial L over partial w_j} $$where $ alpha$ is the learning rate, chosen to be some small parameter. For the MSE loss shown above, the partial derivatives with respect to each of the weights is . $$ { partial L over partial w_j} = {2 over N} sum_{i=0}^{N-1} left[ tilde{Y}_i - Y_i right]{ partial f_i over partial w_j} = {2 over N} sum_{i=0}^{N-1} left[ tilde{Y}_i - Y_i right]f_i(1-f_i)X_{ij}. $$Absorbing the factor of 2/N into our choice of $ alpha$, and writing the summation as a dot product, and noting that $f_i = tilde{Y}_i$, we can write the update for all the weights together as . $$ w = w - alpha X^T cdot left( [ tilde{Y}-Y]* tilde{Y}*(1- tilde{Y}) right) $$where the $ cdot$ denotes a matrix-matrix product (i.e. a dot product for successive rows of $X^T$) and $*$ denotes elementwise multiplication. . To clarify the above expression in terms of matrix dimensions, we can see that $w$, a 3x1 matrix, can be made by multipyting $X^T$ (a 3 x4 matrix) with the term in parentheses, i.e. the product of elementwise terms involving $ tilde{Y}$, which is a 4x1 matrix. In other words, a 3x4 matrix, times a 4x1 matrix, yields a 3x1 matrix. . Actual Code . The full code for all of this is then... . # https://iamtrask.github.io/2015/07/12/basic-python-network/ import numpy as np # sigmoid activation def sigmoid(x,deriv=False): if(deriv==True): return x*(1-x) return 1/(1+np.exp(-x)) # input dataset X = np.array([ [0,0,1], [0,1,1], [1,0,1], [1,1,1] ]) # target output dataset Y = np.array([[0,0,1,1]]).T # seed random numbers to make calculation # deterministic (just a good practice) np.random.seed(1) # initialize weights randomly with mean 0 w = 2*np.random.random((3,1)) - 1 alpha = 1.0 # learning rate loss_history = [] # keep a record of how the loss proceeded, blank for now for iter in range(1000): # forward propagation Y_pred = sigmoid(np.dot(X,w)) # prediction, i.e. tilde{Y} # how much did we miss? diff = Y_pred - Y loss_history.append((diff**2).mean()) # add to the history of the loss # update weights w -= alpha * np.dot( X.T, diff*sigmoid(Y_pred, deriv=True)) print(&quot;Output After Training:&quot;) print(&quot;Y_pred = (should be two 0&#39;s followed by two 1&#39;s) n&quot;,Y_pred) print(&quot;weights = n&quot;,w) . Output After Training: Y_pred = (should be two 0&#39;s followed by two 1&#39;s) [[0.03178421] [0.02576499] [0.97906682] [0.97414645]] weights = [[ 7.26283009] [-0.21614618] [-3.41703015]] . Note that, because of our nonlinear activation, we don&#39;t get the solution $w_0=1, w_1=0, w_2=0$. . Plotting the loss vs. iteration number, we see... . %matplotlib inline import matplotlib.pyplot as plt plt.loglog(loss_history) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.show() . Change the activation function . Another popular choice of activation function is the rectified linear unit or ReLU. The function ReLU(x) is zero for x &lt;= 0, and equal to x (i.e. a straight line at 45 degrees for) x &gt;0. It can be written as max(x,0) or x * (x&gt;0), and its derivative is 1 for positive x, and zero otherwise. . Click here to see a graph of ReLU . Modifying our earlier code to use ReLU activation instead of sigmoid looks like this: . def relu(x,deriv=False): # relu activation if(deriv==True): return 1*(x&gt;0) return x*(x&gt;0) # seed random numbers to make calculation # deterministic (just a good practice) np.random.seed(1) # initialize weights randomly (but only &gt;0 because ReLU clips otherwise) w = np.random.random((3,1)) alpha = 0.3 # learning rate new_loss_history = [] # keep a record of how the error proceeded for iter in range(1000): # forward propagation Y_pred = relu(np.dot(X,w)) # how much did we miss? diff = Y_pred - Y new_loss_history.append((diff**2).mean()) # add to the record of the loss # update weights w -= alpha * np.dot( X.T, diff*relu(Y_pred, deriv=True)) print(&quot;Output After Training:&quot;) print(&quot;Y_pred = (should be two 0&#39;s followed by two 1&#39;s) n&quot;,Y_pred) print(&quot;weights = n&quot;,w) . Output After Training: Y_pred = (should be two 0&#39;s followed by two 1&#39;s) [[-0.] [-0.] [ 1.] [ 1.]] weights = [[ 1.01784368e+00] [ 8.53961786e-17] [-1.78436793e-02]] . print( w[2] - (1-w[0]) ) . [-3.46944695e-17] . Plot old results with new results: . %matplotlib inline import matplotlib.pyplot as plt plt.loglog(loss_history,label=&quot;sigmoid&quot;) plt.loglog(new_loss_history,label=&quot;relu&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.legend() plt.show() . Looks like ReLU may be a better choice than sigmoid for this problem! . Exercise: Read a 7-segment display . A 7-segment display is used for displaying numerical digits 0 through 9, usually by lighting up LEDs or parts of a liquid crystal display (LCD). The segments are labelled $a$ through $g$ according to the following diagram: . . Diagram of the network . The 7 inputs &quot;a&quot; through &quot;g&quot; will be mapped to 10 outputs for the individual digits, and each output can range from 0 (&quot;false&quot; or &quot;no&quot;) to 1 (&quot;true&quot; or &quot;yes&quot;) for that digit. The input and outputs will be connected by a matrix of weights. Pictorially, this looks like the following (Not shown: activation function $f$): . . ...where again, this network operates on a single data point at a time, datapoints which are rows of X and Y. What is shown in the above diagram are the columns of $X$ and $Y$ for a single row (/ single data point). . Create the dataset . Let the input X be the segments $a$ through $g$ are the columns of the input $X$, and are either 1 for on or 0 for off. Let the columns of the target $Y$ be the digits 0-9 themselves arranged in a &quot;one hot&quot; encoding scheme, as follows: . Digit | One-Hot Encoding for $Y$ | . 0 | 1,0,0,0,0,0,0,0,0,0 | . 1 | 0,1,0,0,0,0,0,0,0,0 | . 2 | 0,0,1,0,0,0,0,0,0,0 | . ... | ... | . 9 | 0,0,0,0,0,0,0,0,0,1 | . The values in the columns for $Y$ are essentially true/false &quot;bits&quot; for each digit, answering the question &quot;Is this digit the appropriate output?&quot; with a &quot;yes&quot;(=1) or &quot;no&quot; (=0) response. . The input $X$ will be a 10x7 matrix, and the target $Y$ will be a 10x10 matrix. Each row of $X$ will be the segments to produce the digit for that row. For example, the zeroth row of $X$ should show segments on which make an image of the digit zero, namely segments a, b, c, d, e, and f but not g, so that the zeroth row of X should be [1,1,1,1,1,1,0]. . Define numpy arrays for both $X$ and $Y$ (Hint: for $Y$, check out np.eye()): . # for the 7-segment display. The following is just a &quot;stub&quot; to get you started. X = np.array([ [1,1,1,1,1,1,0], [], [], [] ]) Y = np.array([ [1,0,0,0,0,0,0,0,0,0], [], [] ]) . Initialize the weights . Previously the dimensions of the weight matrix $w$ were 3x1 because we were mapping each row of 3 elements in $X$ to each row of 1 element of $Y$. For this new problem, each row of $X$ has 7 elements, and we want to map those to the 10 elements in each 1-hot-encoded row of $Y$, so what should the dimensions of the weights matrix $w$ be? . Write some numpy code to randomly initialize the weights matrix: . np.random.seed(1) # initial RNG so everybody gets similar results w = np.random.random(( , )) # Students, fill in the array dimensions here . File &#34;&lt;ipython-input-7-20f53a51cded&gt;&#34;, line 1 w = np.random.random(( , )) ^ SyntaxError: invalid syntax . Train the network . Having created an $X$ and its matching $Y$, and initalized the weights $w$ randomly, train a neural network such as the ones above to learn to map a row of X to a row of Y, i.e. train it to recognize digits on 7-segment displays. Do this below. . # Use sigmoid activation, and 1000 iterations, and learning rate of 0.9 # Question: What happens if you use ReLU instead? Try it later. Is ReLU always the best choice? # And then print out your Y_pred &amp; weights matrix, and limit it to 3 significant digits print(&quot;Output After Training:&quot;) np.set_printoptions(formatter={&#39;float&#39;: lambda x: &quot;{0:0.3f}&quot;.format(x)}) # 3 sig figs print(&quot;Y_pred= n&quot;,Y_pred) print(&quot;weights = n&quot;,repr(w)) # the repr() makes it so it can be copied &amp; pasted back into Python code . Final Check: Keras version . Keras is a neural network library that lets us write NN applications very compactly. Try running the following using the X and Y from your 7-segment dataset: . import keras from keras.models import Sequential from keras.layers import Dense, Activation model = Sequential([ Dense(10, input_shape=(7,)), Activation(&#39;sigmoid&#39;) ]) model.compile(optimizer=&#39;adam&#39;, # We&#39;ll talk about optimizer choices and loss choices later loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.fit(X, Y, epochs=200, batch_size=1) print(&quot; nY_tilde = n&quot;, model.predict(X) ) . Follow-up: Remarks . Re-stating what we just did . The original problem (posed at the top of this notebook) involves mapping some points from a 3-dimensional space into points in a 1-dimensional space, i.e. to points on the number line. The mapping is done by the combination of a weighted sum (a linear operation) and a nonlinear &quot;activation function&quot; applied to that sum. The use of an activation function like a sigmoid was originally intended to serve as an analogy of activation of biological neurons. Nonlinear activation functions are source of the &quot;power&quot; of neural networks (essentially we approximate some other function by means of a sum of basis functions in some function space, but don&#39;t worry about that if you&#39;re not math-inclined). The algorithm &#39;learns&#39; to approximate this operation via supervised learning and gradient descent according to some loss function. We used the mean squared error (MSE) for our loss, but lots and lots of different loss functions could be used, a few of which we&#39;ll look at another time. . Question for reflection: Unlike fitting a line $y = mx+b$, the weighted sum in our models in this notebook had no constant &quot;bias&quot; term like $b$. How might we include such a term? . One thing we glossed over: &quot;batch size&quot; . Question: Should we apply the gradient descent &quot;update&quot; to the weights each time we process a single row of $X$ &amp; $Y$, or should we compute the combined loss of all the rows together at the same time, and then do the update? This is essentially asking the same question as &quot;When fitting a line $mx+b$ to a bunch of data points, should we use all the points together to update $m$ and $b,$ or should we do this one point at a time -- compute the gradients of the loss at one point, update the weights, compute gradients at another point, etc.?&quot; . The number of points you use is called the batch size and it is what&#39;s known as a &quot;hyperparameter&quot; -- it is not part of the model per se, but it is a(n important) choice you make when training the model. The batch size affects the learning as follows: Averaging the gradints for many data points (i..e. a large batch size) will produce a smoother loss function and will also usually make the code execute more quickly through the dataset, but updating the weights for every point will cause the algorithm to learn with fewer iterations. . One quick way to observe this is to go up to the Keras code above and change batch_size from 1 to 10, and re-execute the cell. How is the accuracy after 200 iteractions, compared to when batch_size=1? . Terminology: Technically, it&#39;s called &quot;batch training&quot; when you sum the gradients for all the data points before updating the weights, whereas using fewer points is &quot;minibatch training&quot;, and updating for each point (i.e. each row, for us) is Stochastic Gradient Descent* (SGD -- more on these terms here). In practice, there is a tradeoff between smaller vs. larger (mini)batches, which has been the subject of intense scrutiny by researchers over the years. We will have more to say on this later. . For discussion later: In our presentation above, were we using batch training, minibatch training or SGD? . . . *Note: many people will regard SGD as an optimization algorithm per se, and refer to doing SGD even for (mini)batch sizes larger than 1. . Optional: If you want to go really crazy . How about training on this dataset: $$ overbrace{ left[ { begin{array}{cc} 0 &amp; 0 0 &amp; 1 1 &amp; 0 1 &amp; 1 end{array} } right] }^{X} rightarrow overbrace{ left[ { begin{array}{c} 0 1 1 0 end{array} } right] }^Y. $$ Good luck! ;-) (Hint 1: This problem features prominently in the history of Neural Networks, involving Marvin Minsky and &quot;AI Winter.&quot; Hint 2: This whole lesson could instead be entitled &quot;My First Artificial Neuron.&quot;) . Additional Optional Exercise: Binary Math vs. One-Hot Encoding . For the 7-segment display, we used a one-hot encoding for our output, namely a set of true/false &quot;bits&quot; for each digit. One may wonder how effective this ouput-encoding method is, compared to a different bit-setting encoding method, namely binary representations. . Construct the target output matrix $Y$ for binary representations of the numbers 0 through 9. Your target matrix should have 10 rows and 4 columns (i.e, output bits for 1s, 2s, 4s, and 8s). | Using this $Y$ array, train the network as before, and plot the loss as a function of iteration. | Question: Which method works &#39;better&#39;? One-hot encoding or binary encoding? .",
            "url": "https://dnlam.github.io/fastblog/2021/06/04/The_Beauty_Of_Neural_Network.html",
            "relUrl": "/2021/06/04/The_Beauty_Of_Neural_Network.html",
            "date": " • Jun 4, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Optimal Policies with Dynamic Programming",
            "content": "Section: Dynamic Programming . The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies. . In order to shed light on the Chapter 4 of the bible Reinforcement Learning, the objectives of this notebook are: . Policy Evaluation and Policy Improvement | Value and Policy Iteration | Bellman Equations | . Context . We will give an example of how to apply DP into Gridworld City to deal with the City&#39;s Parking problem. The city council has created a Markov decision process (MDP) to model the demand for parking with a reward function that reflects its preferences. . States are nonnegative integers indicating how many parking spaces are occupied. | Actions are nonnegative integers designating the price of street parking. | The reward is a real value describing the city&#39;s preference for the situation. | Time is discretized by hour. | . Preliminaries . The BaseAgent, Environment and RLGlue should follow the our first Notebook on Reinforcement Learning Exploration/Exploitation. . The construction of a virtual ParkingWorld and the plot function are given below: . &lt;Figure size 432x288 with 0 Axes&gt; . Section 1: Policy Evaluation . First, the city council would like you to evaluate the quality of the existing pricing scheme. Policy evaluation works by iteratively applying the Bellman equation for $v_{ pi}$ to a working value function, as an update rule, as shown below. . $$ large v(s) leftarrow sum_a pi(a | s) sum_{s&#39;, r} p(s&#39;, r | s, a)[r + gamma v(s&#39;)]$$ This update can either occur &quot;in-place&quot; (i.e. the update rule is sequentially applied to each state) or with &quot;two-arrays&quot; (i.e. the update rule is simultaneously applied to each state). Both versions converge to $v_{ pi}$ but the in-place version usually converges faster. In this assignment, we will be implementing all update rules in-place, as is done in the pseudocode of chapter 4 of the textbook. . The policy evaluation can be expressed in the code below: . def evaluate_policy(env, V, pi, gamma, theta): delta = float(&#39;inf&#39;) while delta &gt; theta: delta = 0 for s in env.S: v = V[s] bellman_update(env, V, pi, s, gamma) delta = max(delta, abs(v - V[s])) return V . Then, the Bellman update will be: . def bellman_update(env, V, pi, s, gamma): &quot;&quot;&quot;Mutate ``V`` according to the Bellman update equation.&quot;&quot;&quot; v = 0 for a in env.A: transitions = env.transitions(s, a) for s_, (r, p) in enumerate(transitions): v += pi[s][a] * p * (r + gamma * V[s_]) V[s] = v . The observation shows that the value monotonically increases as more parking is used, until there is no parking left, in which case the value is lower. Because of the relatively simple reward function (more reward is accrued when many but not all parking spots are taken and less reward is accrued when few or all parking spots are taken) and the highly stochastic dynamics function (each state has positive probability of being reached each time step) the value functions of most policies will qualitatively resemble this graph. However, depending on the intelligence of the policy, the scale of the graph will differ. In other words, better policies will increase the expected return at every state rather than changing the relative desirability of the states. Intuitively, the value of a less desirable state can be increased by making it less likely to remain in a less desirable state. Similarly, the value of a more desirable state can be increased by making it more likely to remain in a more desirable state. That is to say, good policies are policies that spend more time in desirable states and less time in undesirable states. As we will see in this assignment, such a steady state distribution is achieved by setting the price to be low in low occupancy states (so that the occupancy will increase) and setting the price high when occupancy is high (so that full occupancy will be avoided). . Policy Iteration . Now the city council would like to propose a more efficient policy using policy iteration. Policy iteration works by alternating between evaluating the existing policy and making the policy greedy with respect to the existing value function. . def improve_policy(env, V, pi, gamma): policy_stable = True for s in env.S: old = pi[s].copy() q_greedify_policy(env, V, pi, s, gamma) if not np.array_equal(pi[s], old): policy_stable = False return pi, policy_stable def policy_iteration(env, gamma, theta): V = np.zeros(len(env.S)) pi = np.ones((len(env.S), len(env.A))) / len(env.A) policy_stable = False while not policy_stable: V = evaluate_policy(env, V, pi, gamma, theta) pi, policy_stable = improve_policy(env, V, pi, gamma) return V, pi . def q_greedify_policy(env, V, pi, s, gamma): &quot;&quot;&quot;Mutate ``pi`` to be greedy with respect to the q-values induced by ``V``.&quot;&quot;&quot; G = np.zeros_like(env.A, dtype=float) for a in env.A: transitions = env.transitions(s, a) for s_, (r, p) in enumerate(transitions): G[a] += p * (r + gamma * V[s_]) greed_actions = np.argwhere(G == np.amax(G)) for a in env.A: if a in greed_actions: pi[s, a] = 1 / len(greed_actions) else: pi[s, a] = 0 . Section 3: Value Iteration . Value iteration works by iteratively applying the Bellman optimality equation for $v_{ ast}$ to a working value function, as an update rule, as shown below. . $$ large v(s) leftarrow max_a sum_{s&#39;, r} p(s&#39;, r | s, a)[r + gamma v(s&#39;)]$$ . def value_iteration(env, gamma, theta): V = np.zeros(len(env.S)) while True: delta = 0 for s in env.S: v = V[s] bellman_optimality_update(env, V, s, gamma) delta = max(delta, abs(v - V[s])) if delta &lt; theta: break pi = np.ones((len(env.S), len(env.A))) / len(env.A) for s in env.S: q_greedify_policy(env, V, pi, s, gamma) return V, pi . def bellman_optimality_update(env, V, s, gamma): &quot;&quot;&quot;Mutate ``V`` according to the Bellman optimality update equation.&quot;&quot;&quot; vmax = - float(&#39;inf&#39;) for a in env.A: transitions = env.transitions(s, a) va = 0 for s_, (r, p) in enumerate(transitions): va += p * (r + gamma * V[s_]) vmax = max(va, vmax) V[s] = vmax . In the value iteration algorithm above, a policy is not explicitly maintained until the value function has converged. Below, we have written an identically behaving value iteration algorithm that maintains an updated policy. Writing value iteration in this form makes its relationship to policy iteration more evident. Policy iteration alternates between doing complete greedifications and complete evaluations. On the other hand, value iteration alternates between doing local greedifications and local evaluations. . def value_iteration2(env, gamma, theta): V = np.zeros(len(env.S)) pi = np.ones((len(env.S), len(env.A))) / len(env.A) while True: delta = 0 for s in env.S: v = V[s] q_greedify_policy(env, V, pi, s, gamma) bellman_update(env, V, pi, s, gamma) delta = max(delta, abs(v - V[s])) if delta &lt; theta: break return V, pi .",
            "url": "https://dnlam.github.io/fastblog/2020/06/29/Optimal_Policy_Dynamic_Programming.html",
            "relUrl": "/2020/06/29/Optimal_Policy_Dynamic_Programming.html",
            "date": " • Jun 29, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Bandits and Exploitation/Exploration in Reinforcement Learning",
            "content": "Objective of this notebook are to: . Help you understand the bandit problem | Understand the effects of epsilon on exploration and learn about exploitation/exploration trade-off | Introduce some of essential RL softwares that are useful to build a RL solver. | . Section 0: Preliminaries . Firstly, we will build a BaseAgent for our RL . from __future__ import print_function from abc import ABCMeta, abstractmethod class BaseAgent: &quot;&quot;&quot;Implements the agent for an RL-Glue environment. Note: agent_init, agent_start, agent_step, agent_end, agent_cleanup, and agent_message are required methods. &quot;&quot;&quot; __metaclass__ = ABCMeta def __init__(self): pass @abstractmethod def agent_init(self, agent_info= {}): &quot;&quot;&quot;Setup for the agent called when the experiment first starts.&quot;&quot;&quot; @abstractmethod def agent_start(self, observation): &quot;&quot;&quot;The first method called when the experiment starts, called after the environment starts. Args: observation (Numpy array): the state observation from the environment&#39;s evn_start function. Returns: The first action the agent takes. &quot;&quot;&quot; @abstractmethod def agent_step(self, reward, observation): &quot;&quot;&quot;A step taken by the agent. Args: reward (float): the reward received for taking the last action taken observation (Numpy array): the state observation from the environment&#39;s step based, where the agent ended up after the last step Returns: The action the agent is taking. &quot;&quot;&quot; @abstractmethod def agent_end(self, reward): &quot;&quot;&quot;Run when the agent terminates. Args: reward (float): the reward the agent received for entering the terminal state. &quot;&quot;&quot; @abstractmethod def agent_cleanup(self): &quot;&quot;&quot;Cleanup done after the agent ends.&quot;&quot;&quot; @abstractmethod def agent_message(self, message): &quot;&quot;&quot;A function used to pass information from the agent to the experiment. Args: message: The message passed to the agent. Returns: The response (or answer) to the message. &quot;&quot;&quot; . class Agent(BaseAgent): &quot;&quot;&quot;agent does *no* learning, selects action 0 always&quot;&quot;&quot; def __init__(self): self.last_action = None self.num_actions = None self.q_values = None self.step_size = None self.epsilon = None self.initial_value = 0.0 self.arm_count = [0.0 for _ in range(10)] def agent_init(self, agent_info={}): &quot;&quot;&quot;Setup for the agent called when the experiment first starts.&quot;&quot;&quot; # if &quot;actions&quot; in agent_info: # self.num_actions = agent_info[&quot;actions&quot;] # if &quot;state_array&quot; in agent_info: # self.q_values = agent_info[&quot;state_array&quot;] self.num_actions = agent_info.get(&quot;num_actions&quot;, 2) self.initial_value = agent_info.get(&quot;initial_value&quot;, 0.0) self.q_values = np.ones(agent_info.get(&quot;num_actions&quot;, 2)) * self.initial_value self.step_size = agent_info.get(&quot;step_size&quot;, 0.1) self.epsilon = agent_info.get(&quot;epsilon&quot;, 0.0) self.last_action = 0 def agent_start(self, observation): &quot;&quot;&quot;The first method called when the experiment starts, called after the environment starts. Args: observation (Numpy array): the state observation from the environment&#39;s evn_start function. Returns: The first action the agent takes. &quot;&quot;&quot; self.last_action = np.random.choice(self.num_actions) # set first action to 0 return self.last_action def agent_step(self, reward, observation): &quot;&quot;&quot;A step taken by the agent. Args: reward (float): the reward received for taking the last action taken observation (Numpy array): the state observation from the environment&#39;s step based, where the agent ended up after the last step Returns: The action the agent is taking. &quot;&quot;&quot; # local_action = 0 # choose the action here self.last_action = np.random.choice(self.num_actions) return self.last_action def agent_end(self, reward): &quot;&quot;&quot;Run when the agent terminates. Args: reward (float): the reward the agent received for entering the terminal state. &quot;&quot;&quot; pass def agent_cleanup(self): &quot;&quot;&quot;Cleanup done after the agent ends.&quot;&quot;&quot; pass def agent_message(self, message): &quot;&quot;&quot;A function used to pass information from the agent to the experiment. Args: message: The message passed to the agent. Returns: The response (or answer) to the message. &quot;&quot;&quot; pass . Then, we create an Environment and its abstract for our RL. This is the 10-armed Testbed introduced in section 2.3 of the textbook. We use this throughout this notebook to test our bandit agents. It has 10 arms, which are the actions the agent can take. Pulling an arm generates a stochastic reward from a Gaussian distribution with unit-variance. For each action, the expected value of that action is randomly sampled from a normal distribution, at the start of each run. If you are unfamiliar with the 10-armed Testbed please review it in the textbook before continuing. . &quot;&quot;&quot;Abstract environment base class. &quot;&quot;&quot; from __future__ import print_function from abc import ABCMeta, abstractmethod class BaseEnvironment: &quot;&quot;&quot;Implements the environment for an RLGlue environment Note: env_init, env_start, env_step, env_cleanup, and env_message are required methods. &quot;&quot;&quot; __metaclass__ = ABCMeta def __init__(self): reward = None observation = None termination = None self.reward_obs_term = (reward, observation, termination) @abstractmethod def env_init(self, env_info={}): &quot;&quot;&quot;Setup for the environment called when the experiment first starts. Note: Initialize a tuple with the reward, first state observation, boolean indicating if it&#39;s terminal. &quot;&quot;&quot; @abstractmethod def env_start(self): &quot;&quot;&quot;The first method called when the experiment starts, called before the agent starts. Returns: The first state observation from the environment. &quot;&quot;&quot; @abstractmethod def env_step(self, action): &quot;&quot;&quot;A step taken by the environment. Args: action: The action taken by the agent Returns: (float, state, Boolean): a tuple of the reward, state observation, and boolean indicating if it&#39;s terminal. &quot;&quot;&quot; @abstractmethod def env_cleanup(self): &quot;&quot;&quot;Cleanup done after the environment ends&quot;&quot;&quot; @abstractmethod def env_message(self, message): &quot;&quot;&quot;A message asking the environment for information Args: message: the message passed to the environment Returns: the response (or answer) to the message &quot;&quot;&quot; . class Environment(BaseEnvironment): &quot;&quot;&quot;Implements the environment for an RLGlue environment Note: env_init, env_start, env_step, env_cleanup, and env_message are required methods. &quot;&quot;&quot; actions = [0] def __init__(self): reward = None observation = None termination = None self.reward_obs_term = (reward, observation, termination) self.count = 0 self.arms = [] self.seed = None def env_init(self, env_info={}): &quot;&quot;&quot;Setup for the environment called when the experiment first starts. Note: Initialize a tuple with the reward, first state observation, boolean indicating if it&#39;s terminal. &quot;&quot;&quot; self.arms = np.random.randn(10)#[np.random.normal(0.0, 1.0) for _ in range(10)] local_observation = 0 # An empty NumPy array self.reward_obs_term = (0.0, local_observation, False) def env_start(self): &quot;&quot;&quot;The first method called when the experiment starts, called before the agent starts. Returns: The first state observation from the environment. &quot;&quot;&quot; return self.reward_obs_term[1] def env_step(self, action): &quot;&quot;&quot;A step taken by the environment. Args: action: The action taken by the agent Returns: (float, state, Boolean): a tuple of the reward, state observation, and boolean indicating if it&#39;s terminal. &quot;&quot;&quot; # if action == 0: # if np.random.random() &lt; 0.2: # reward = 14 # else: # reward = 6 # if action == 1: # reward = np.random.choice(range(10,14)) # if action == 2: # if np.random.random() &lt; 0.8: # reward = 174 # else: # reward = 7 # reward = np.random.normal(self.arms[action], 1.0) reward = self.arms[action] + np.random.randn() obs = self.reward_obs_term[1] self.reward_obs_term = (reward, obs, False) return self.reward_obs_term def env_cleanup(self): &quot;&quot;&quot;Cleanup done after the environment ends&quot;&quot;&quot; pass def env_message(self, message): &quot;&quot;&quot;A message asking the environment for information Args: message (string): the message passed to the environment Returns: string: the response (or answer) to the message &quot;&quot;&quot; if message == &quot;what is the current reward?&quot;: return &quot;{}&quot;.format(self.reward_obs_term[0]) # else return &quot;I don&#39;t know how to respond to your message&quot; . Here, we create RLGlue which was originally designed by Adam White, Brian Tanner, and Rich Sutton. This library will give you a solid framework to understand how reinforcement learning experiments work and how to run your own. . class RLGlue: &quot;&quot;&quot;RLGlue class args: env_name (string): the name of the module where the Environment class can be found agent_name (string): the name of the module where the Agent class can be found &quot;&quot;&quot; def __init__(self, env_class, agent_class): self.environment = env_class() self.agent = agent_class() self.total_reward = None self.last_action = None self.num_steps = None self.num_episodes = None def rl_init(self, agent_init_info={}, env_init_info={}): &quot;&quot;&quot;Initial method called when RLGlue experiment is created&quot;&quot;&quot; self.environment.env_init(env_init_info) self.agent.agent_init(agent_init_info) self.total_reward = 0.0 self.num_steps = 0 self.num_episodes = 0 def rl_start(self, agent_start_info={}, env_start_info={}): &quot;&quot;&quot;Starts RLGlue experiment Returns: tuple: (state, action) &quot;&quot;&quot; last_state = self.environment.env_start() self.last_action = self.agent.agent_start(last_state) observation = (last_state, self.last_action) return observation def rl_agent_start(self, observation): &quot;&quot;&quot;Starts the agent. Args: observation: The first observation from the environment Returns: The action taken by the agent. &quot;&quot;&quot; return self.agent.agent_start(observation) def rl_agent_step(self, reward, observation): &quot;&quot;&quot;Step taken by the agent Args: reward (float): the last reward the agent received for taking the last action. observation : the state observation the agent receives from the environment. Returns: The action taken by the agent. &quot;&quot;&quot; return self.agent.agent_step(reward, observation) def rl_agent_end(self, reward): &quot;&quot;&quot;Run when the agent terminates Args: reward (float): the reward the agent received when terminating &quot;&quot;&quot; self.agent.agent_end(reward) def rl_env_start(self): &quot;&quot;&quot;Starts RL-Glue environment. Returns: (float, state, Boolean): reward, state observation, boolean indicating termination &quot;&quot;&quot; self.total_reward = 0.0 self.num_steps = 1 this_observation = self.environment.env_start() return this_observation def rl_env_step(self, action): &quot;&quot;&quot;Step taken by the environment based on action from agent Args: action: Action taken by agent. Returns: (float, state, Boolean): reward, state observation, boolean indicating termination. &quot;&quot;&quot; ro = self.environment.env_step(action) (this_reward, _, terminal) = ro self.total_reward += this_reward if terminal: self.num_episodes += 1 else: self.num_steps += 1 return ro def rl_step(self): &quot;&quot;&quot;Step taken by RLGlue, takes environment step and either step or end by agent. Returns: (float, state, action, Boolean): reward, last state observation, last action, boolean indicating termination &quot;&quot;&quot; (reward, last_state, term) = self.environment.env_step(self.last_action) self.total_reward += reward if term: self.num_episodes += 1 self.agent.agent_end(reward) roat = (reward, last_state, None, term) else: self.num_steps += 1 self.last_action = self.agent.agent_step(reward, last_state) roat = (reward, last_state, self.last_action, term) return roat def rl_cleanup(self): &quot;&quot;&quot;Cleanup done at end of experiment.&quot;&quot;&quot; self.environment.env_cleanup() self.agent.agent_cleanup() def rl_agent_message(self, message): &quot;&quot;&quot;Message passed to communicate with agent during experiment Args: message: the message (or question) to send to the agent Returns: The message back (or answer) from the agent &quot;&quot;&quot; return self.agent.agent_message(message) def rl_env_message(self, message): &quot;&quot;&quot;Message passed to communicate with environment during experiment Args: message: the message (or question) to send to the environment Returns: The message back (or answer) from the environment &quot;&quot;&quot; return self.environment.env_message(message) def rl_episode(self, max_steps_this_episode): &quot;&quot;&quot;Runs an RLGlue episode Args: max_steps_this_episode (Int): the maximum steps for the experiment to run in an episode Returns: Boolean: if the episode should terminate &quot;&quot;&quot; is_terminal = False self.rl_start() while (not is_terminal) and ((max_steps_this_episode == 0) or (self.num_steps &lt; max_steps_this_episode)): rl_step_result = self.rl_step() is_terminal = rl_step_result[3] return is_terminal def rl_return(self): &quot;&quot;&quot;The total reward Returns: float: the total reward &quot;&quot;&quot; return self.total_reward def rl_num_steps(self): &quot;&quot;&quot;The total number of steps taken Returns: Int: the total number of steps taken &quot;&quot;&quot; return self.num_steps def rl_num_episodes(self): &quot;&quot;&quot;The number of episodes Returns Int: the total number of episodes &quot;&quot;&quot; return self.num_episodes . %matplotlib inline import numpy as np import matplotlib.pyplot as plt from tqdm import tqdm import time . Section 1: Greedy Agent . We want to create an agent that will find the action with the highest expected reward. One way an agent could operate is to always choose the action with the highest value based on the agent’s current estimates. This is called a greedy agent as it greedily chooses the action that it thinks has the highest value. Let&#39;s look at what happens in this case. . First we are going to implement the argmax function, which takes in a list of action values and returns an action with the highest value. Why are we implementing our own instead of using the argmax function that numpy uses? Numpy&#39;s argmax function returns the first instance of the highest value. We do not want that to happen as it biases the agent to choose a specific action in the case of ties. Instead we want to break ties between the highest values randomly. So we are going to implement our own argmax function. You may want to look at np.random.choice to randomly select from a list of values. . def argmax(q_values): &quot;&quot;&quot; Takes in a list of q_values and returns the index of the item with the highest value. Breaks ties randomly. returns: int - the index of the highest value in q_values &quot;&quot;&quot; top_value = float(&quot;-inf&quot;) ties = [] for i in range(len(q_values)): # if a value in q_values is greater than the highest value update top and reset ties to zero # if a value is equal to top value add the index to ties # return a random selection from ties. # YOUR CODE HERE if q_values[i] &gt; top_value: top_value = q_values[i] ties = [i] elif q_values[i] == top_value: ties.append(i) return np.random.choice(ties) . Next, we are going to create a GreedyAgent and implement the agent_step method. This method gets called each time the agent takes a step. The method has to return the action selected by the agent. This method also ensures the agent’s estimates are updated based on the signals it gets from the environment. . class GreedyAgent(Agent): def agent_step(self, reward, observation=None): &quot;&quot;&quot; Takes one step for the agent. It takes in a reward and observation and returns the action the agent chooses at that time step. Arguments: reward -- float, the reward the agent recieved from the environment after taking the last action. observation -- float, the observed state the agent is in. Do not worry about this as you will not use it until future lessons Returns: current_action -- int, the action chosen by the agent at the current time step. &quot;&quot;&quot; ### Useful Class Variables ### # self.q_values : An array with what the agent believes each of the values of the arm are. # self.arm_count : An array with a count of the number of times each arm has been pulled. # self.last_action : The action that the agent took on the previous time step ####################### # Update Q values Hint: Look at the algorithm in section 2.4 of the textbook. # increment the counter in self.arm_count for the action from the previous time step # update the step size using self.arm_count # update self.q_values for the action from the previous time step self.arm_count[self.last_action] += 1 self.q_values[self.last_action] += (reward - self.q_values[self.last_action]) / self.arm_count[self.last_action] current_action = argmax(self.q_values) self.last_action = current_action return current_action . Then, let&#39;s visualize the result . num_runs = 200 # The number of times we run the experiment num_steps = 1000 # The number of pulls of each arm the agent takes env = Environment # We set what environment we want to use to test agent = GreedyAgent # We choose what agent we want to use agent_info = {&quot;num_actions&quot;: 10} # We pass the agent the information it needs. Here how many arms there are. env_info = {} # We pass the environment the information it needs. In this case nothing. rewards = np.zeros((num_runs, num_steps)) average_best = 0 for run in tqdm(range(num_runs)): # tqdm is what creates the progress bar below np.random.seed(run) rl_glue = RLGlue(env, agent) # Creates a new RLGlue experiment with the env and agent we chose above rl_glue.rl_init(agent_info, env_info) # We pass RLGlue what it needs to initialize the agent and environment rl_glue.rl_start() # We start the experiment average_best += np.max(rl_glue.environment.arms) for i in range(num_steps): reward, _, action, _ = rl_glue.rl_step() # The environment and agent take a step and return # the reward, and action taken. rewards[run, i] = reward greedy_scores = np.mean(rewards, axis=0) plt.figure(figsize=(15, 5), dpi= 80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.plot([average_best / num_runs for _ in range(num_steps)], linestyle=&quot;--&quot;) plt.plot(greedy_scores) plt.legend([&quot;Best Possible&quot;, &quot;Greedy&quot;]) plt.title(&quot;Average Reward of Greedy Agent&quot;) plt.xlabel(&quot;Steps&quot;) plt.ylabel(&quot;Average reward&quot;) plt.show() . 100%|██████████| 200/200 [00:03&lt;00:00, 63.50it/s] . Section 2: Epsilon-Greedy Agent . We noticed about a trade off between Exploitation and Exploration, where it does not always take the greedy action. Instead, sometimes it takes an exploratory action. It does this so that it can find out what the best action really is. If we always choose what we think is the current best action is, we may miss out on taking the true best action, because we haven&#39;t explored enough times to find that best action. . Implement an epsilon-greedy agent below. Hint: we are implementing the algorithm from section 2.4 of this textbook. You may want to use your greedy code from above and look at np.random.random, as well as np.random.randint, to help you select random actions. . class EpsilonGreedyAgent(Agent): def agent_step(self, reward, observation): &quot;&quot;&quot; Takes one step for the agent. It takes in a reward and observation and returns the action the agent chooses at that time step. Arguments: reward -- float, the reward the agent recieved from the environment after taking the last action. observation -- float, the observed state the agent is in. Do not worry about this as you will not use it until future lessons Returns: current_action -- int, the action chosen by the agent at the current time step. &quot;&quot;&quot; ### Useful Class Variables ### # self.q_values : An array with what the agent believes each of the values of the arm are. # self.arm_count : An array with a count of the number of times each arm has been pulled. # self.last_action : The action that the agent took on the previous time step # self.epsilon : The probability an epsilon greedy agent will explore (ranges between 0 and 1) ####################### # Update Q values - this should be the same update as your greedy agent above self.arm_count[self.last_action] += 1 self.q_values[self.last_action] += (reward - self.q_values[self.last_action]) / self.arm_count[self.last_action] # Choose action using epsilon greedy # Randomly choose a number between 0 and 1 and see if it&#39;s less than self.epsilon if np.random.random() &lt; self.epsilon: current_action = np.random.randint(len(self.q_values)) else: current_action = argmax(self.q_values) self.last_action = current_action return current_action . Now that we have our epsilon greedy agent created. Let&#39;s compare it against the greedy agent with epsilon of 0.1. . num_runs = 200 num_steps = 1000 epsilon = 0.1 agent = EpsilonGreedyAgent env = Environment # ten arms agent_info = {&quot;num_actions&quot;: 10, &quot;epsilon&quot;: epsilon} env_info = {} all_rewards = np.zeros((num_runs, num_steps)) for run in tqdm(range(num_runs)): np.random.seed(run) rl_glue = RLGlue(env, agent) rl_glue.rl_init(agent_info, env_info) rl_glue.rl_start() for i in range(num_steps): reward, _, action, _ = rl_glue.rl_step() # The environment and agent take a step and return # the reward, and action taken. all_rewards[run, i] = reward # take the mean over runs scores = np.mean(all_rewards, axis=0) plt.figure(figsize=(15, 5), dpi= 80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.plot([1.55 for _ in range(num_steps)], linestyle=&quot;--&quot;) plt.plot(greedy_scores) plt.title(&quot;Average Reward of Greedy Agent vs. E-Greedy Agent&quot;) plt.plot(scores) plt.legend((&quot;Best Possible&quot;, &quot;Greedy&quot;, &quot;Epsilon: 0.1&quot;)) plt.xlabel(&quot;Steps&quot;) plt.ylabel(&quot;Average reward&quot;) plt.show() . 100%|██████████| 200/200 [00:03&lt;00:00, 63.69it/s] . Here, we noticed how much better the epsilon-greedy agent did. Because we occasionally choose a random action we were able to find a better long term policy. By acting greedily before our value estimates are accurate, we risk settling on a suboptimal action. . Section 3: Comparing values of epsilon . Can we do better than an epsilon of 0.1? Let&#39;s try several different values for epsilon and see how they perform. We try different settings of key performance parameters to understand how the agent might perform under different conditions. . Below we run an experiment where we sweep over different values for epsilon: . epsilons = [0.0, 0.01, 0.1, 0.4] plt.figure(figsize=(15, 5), dpi= 80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.plot([1.55 for _ in range(num_steps)], linestyle=&quot;--&quot;) n_q_values = [] n_averages = [] n_best_actions = [] num_runs = 200 for epsilon in epsilons: all_averages = [] for run in tqdm(range(num_runs)): agent = EpsilonGreedyAgent agent_info = {&quot;num_actions&quot;: 10, &quot;epsilon&quot;: epsilon} env_info = {&quot;random_seed&quot;: run} rl_glue = RLGlue(env, agent) rl_glue.rl_init(agent_info, env_info) rl_glue.rl_start() best_arm = np.argmax(rl_glue.environment.arms) scores = [0] averages = [] best_action_chosen = [] for i in range(num_steps): reward, state, action, is_terminal = rl_glue.rl_step() scores.append(scores[-1] + reward) averages.append(scores[-1] / (i + 1)) if action == best_arm: best_action_chosen.append(1) else: best_action_chosen.append(0) if epsilon == 0.1 and run == 0: n_q_values.append(np.copy(rl_glue.agent.q_values)) if epsilon == 0.1: n_averages.append(averages) n_best_actions.append(best_action_chosen) all_averages.append(averages) plt.plot(np.mean(all_averages, axis=0)) plt.legend([&quot;Best Possible&quot;] + epsilons) plt.xlabel(&quot;Steps&quot;) plt.ylabel(&quot;Average reward&quot;) plt.show() . 100%|██████████| 200/200 [00:04&lt;00:00, 48.39it/s] 100%|██████████| 200/200 [00:04&lt;00:00, 48.03it/s] 100%|██████████| 200/200 [00:03&lt;00:00, 55.24it/s] 100%|██████████| 200/200 [00:02&lt;00:00, 67.29it/s] . Section 4: The Effect of Step Size . In Section 1, we decayed the step size over time based on action-selection counts. The step-size was 1/N(A), where N(A) is the number of times action A was selected. This is the same as computing a sample average. We could also set the step size to be a constant value, such as 0.1. What would be the effect of doing that? And is it better to use a constant or the sample average method? . To investigate this question, let’s start by creating a new agent that has a constant step size. This will be nearly identical to the agent created above. You will use the same code to select the epsilon-greedy action. You will change the update to have a constant step size instead of using the 1/N(A) update. . class EpsilonGreedyAgentConstantStepsize(Agent): def agent_step(self, reward, observation): &quot;&quot;&quot; Takes one step for the agent. It takes in a reward and observation and returns the action the agent chooses at that time step. Arguments: reward -- float, the reward the agent recieved from the environment after taking the last action. observation -- float, the observed state the agent is in. Do not worry about this as you will not use it until future lessons Returns: current_action -- int, the action chosen by the agent at the current time step. &quot;&quot;&quot; ### Useful Class Variables ### # self.q_values : An array with what the agent believes each of the values of the arm are. # self.arm_count : An array with a count of the number of times each arm has been pulled. # self.last_action : An int of the action that the agent took on the previous time step. # self.step_size : A float which is the current step size for the agent. # self.epsilon : The probability an epsilon greedy agent will explore (ranges between 0 and 1) ####################### # Update q_values for action taken at previous time step # using self.step_size intead of using self.arm_count self.arm_count[self.last_action] += 1 self.q_values[self.last_action] += self.step_size * (reward - self.q_values[self.last_action]) # Choose action using epsilon greedy. This is the same as you implemented above. if np.random.random() &lt; self.epsilon: current_action = np.random.randint(len(self.q_values)) else: current_action = argmax(self.q_values) self.last_action = current_action return current_action . step_sizes = [0.01, 0.1, 0.5, 1.0, &#39;1/N(A)&#39;] epsilon = 0.1 num_steps = 1000 num_runs = 200 fig, ax = plt.subplots(figsize=(15, 5), dpi= 80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) q_values = {step_size: [] for step_size in step_sizes} true_values = {step_size: None for step_size in step_sizes} best_actions = {step_size: [] for step_size in step_sizes} for step_size in step_sizes: all_averages = [] for run in tqdm(range(num_runs)): np.random.seed(run) agent = EpsilonGreedyAgentConstantStepsize if step_size != &#39;1/N(A)&#39; else EpsilonGreedyAgent agent_info = {&quot;num_actions&quot;: 10, &quot;epsilon&quot;: epsilon, &quot;step_size&quot;: step_size, &quot;initial_value&quot;: 0.0} env_info = {} rl_glue = RLGlue(env, agent) rl_glue.rl_init(agent_info, env_info) rl_glue.rl_start() best_arm = np.argmax(rl_glue.environment.arms) if run == 0: true_values[step_size] = np.copy(rl_glue.environment.arms) best_action_chosen = [] for i in range(num_steps): reward, state, action, is_terminal = rl_glue.rl_step() if action == best_arm: best_action_chosen.append(1) else: best_action_chosen.append(0) if run == 0: q_values[step_size].append(np.copy(rl_glue.agent.q_values)) best_actions[step_size].append(best_action_chosen) ax.plot(np.mean(best_actions[step_size], axis=0)) plt.legend(step_sizes) plt.title(&quot;% Best Arm Pulled&quot;) plt.xlabel(&quot;Steps&quot;) plt.ylabel(&quot;% Best Arm Pulled&quot;) vals = ax.get_yticks() ax.set_yticklabels([&#39;{:,.2%}&#39;.format(x) for x in vals]) plt.show() . 100%|██████████| 200/200 [00:03&lt;00:00, 56.86it/s] 100%|██████████| 200/200 [00:03&lt;00:00, 60.23it/s] 100%|██████████| 200/200 [00:03&lt;00:00, 59.37it/s] 100%|██████████| 200/200 [00:03&lt;00:00, 59.35it/s] 100%|██████████| 200/200 [00:03&lt;00:00, 56.94it/s] C: Users ND258645 AppData Local Temp ipykernel_15840 3358721971.py:48: UserWarning: FixedFormatter should only be used together with FixedLocator ax.set_yticklabels([&#39;{:,.2%}&#39;.format(x) for x in vals]) . Notice first that we are now plotting the amount of time that the best action is taken rather than the average reward. To better understand the performance of an agent, it can be useful to measure specific behaviors, beyond just how much reward is accumulated. This measure indicates how close the agent’s behaviour is to optimal. . It seems as though 1/N(A) performed better than the others, in that it reaches a solution where it takes the best action most frequently. Now why might this be? Why did a step size of 0.5 start out better but end up performing worse? Why did a step size of 0.01 perform so poorly? . Let&#39;s dig into this further below. Let’s plot how well each agent tracks the true value, where each agent has a different step size method. You do not have to enter any code here, just follow along. . largest = 0 num_steps = 1000 for step_size in step_sizes: plt.figure(figsize=(15, 5), dpi= 80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) largest = np.argmax(true_values[step_size]) plt.plot([true_values[step_size][largest] for _ in range(num_steps)], linestyle=&quot;--&quot;) plt.title(&quot;Step Size: {}&quot;.format(step_size)) plt.plot(np.array(q_values[step_size])[:, largest]) plt.legend([&quot;True Expected Value&quot;, &quot;Estimated Value&quot;]) plt.xlabel(&quot;Steps&quot;) plt.ylabel(&quot;Value&quot;) plt.show() . These plots help clarify the performance differences between the different step sizes. A step size of 0.01 makes such small updates that the agent’s value estimate of the best action does not get close to the actual value. Step sizes of 0.5 and 1.0 both get close to the true value quickly, but are very susceptible to stochasticity in the rewards. The updates overcorrect too much towards recent rewards, and so oscillate around the true value. This means that on many steps, the action that pulls the best arm may seem worse than it actually is. A step size of 0.1 updates fairly quickly to the true value, and does not oscillate as widely around the true values as 0.5 and 1.0. This is one of the reasons that 0.1 performs quite well. Finally we see why 1/N(A) performed well. Early on while the step size is still reasonably high it moves quickly to the true expected value, but as it gets pulled more its step size is reduced which makes it less susceptible to the stochasticity of the rewards. . Does this mean that 1/N(A) is always the best? When might it not be? One possible setting where it might not be as effective is in non-stationary problems. You learned about non-stationarity in the lessons. Non-stationarity means that the environment may change over time. This could manifest itself as continual change over time of the environment, or a sudden change in the environment. . Let&#39;s look at how a sudden change in the reward distributions affects a step size like 1/N(A). This time we will run the environment for 2000 steps, and after 1000 steps we will randomly change the expected value of all of the arms. We compare two agents, both using epsilon-greedy with epsilon = 0.1. One uses a constant step size of 0.1, the other a step size of 1/N(A) that reduces over time. . epsilon = 0.1 num_steps = 2000 num_runs = 500 step_size = 0.1 plt.figure(figsize=(15, 5), dpi= 80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.plot([1.55 for _ in range(num_steps)], linestyle=&quot;--&quot;) for agent in [EpsilonGreedyAgent, EpsilonGreedyAgentConstantStepsize]: rewards = np.zeros((num_runs, num_steps)) for run in tqdm(range(num_runs)): agent_info = {&quot;num_actions&quot;: 10, &quot;epsilon&quot;: epsilon, &quot;step_size&quot;: step_size} np.random.seed(run) rl_glue = RLGlue(env, agent) rl_glue.rl_init(agent_info, env_info) rl_glue.rl_start() for i in range(num_steps): reward, state, action, is_terminal = rl_glue.rl_step() rewards[run, i] = reward if i == 1000: rl_glue.environment.arms = np.random.randn(10) plt.plot(np.mean(rewards, axis=0)) plt.legend([&quot;Best Possible&quot;, &quot;1/N(A)&quot;, &quot;0.1&quot;]) plt.xlabel(&quot;Steps&quot;) plt.ylabel(&quot;Average reward&quot;) plt.show() . 100%|██████████| 500/500 [00:14&lt;00:00, 34.20it/s] 100%|██████████| 500/500 [00:18&lt;00:00, 27.43it/s] . Now the agent with a step size of 1/N(A) performed better at the start but then performed worse when the environment changed! What happened? . Think about what the step size would be after 1000 steps. Let&#39;s say the best action gets chosen 500 times. That means the step size for that action is 1/500 or 0.002. At each step when we update the value of the action and the value is going to move only 0.002 * the error. That is a very tiny adjustment and it will take a long time for it to get to the true value. . The agent with step size 0.1, however, will always update in 1/10th of the direction of the error. This means that on average it will take ten steps for it to update its value to the sample mean. . These are the types of tradeoffs we have to think about in reinforcement learning. A larger step size moves us more quickly toward the true value, but can make our estimated values oscillate around the expected value. A step size that reduces over time can converge to close to the expected value, without oscillating. On the other hand, such a decaying stepsize is not able to adapt to changes in the environment. Nonstationarityand the related concept of partial observabilityis a common feature of reinforcement learning problems and when learning online. .",
            "url": "https://dnlam.github.io/fastblog/2020/06/22/Bandits_Exploration_Exploitation.html",
            "relUrl": "/2020/06/22/Bandits_Exploration_Exploitation.html",
            "date": " • Jun 22, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Exploitation-Exploration Dilemma",
            "content": "Preliminaries . A policy defines the agent&#39;s behaviours Deterministic policy A = $ pi(S)$ | Stochastic policy: $ pi(A|S) = p(A|S)$ | . | The actual value function is the expected return $$ begin{split} v_ pi(s) = mathbb{E} [G_t | St = s, pi] &amp;= mathbb{E} [R{t+1} + gamma R_{t+2 + ...} | St = s, pi] &amp;= mathbb{E} [R{t+1} + gamma v pi(S{t+1}) | S_t = s, A_t ~= pi(s)] | . end{split} $$ where $ gamma$ is a discount factor . Optimal value is the highest possible value for any policy | . $$ v_*(s) = max_{a} mathbb{E} [R_{t+1} + gamma v_*(S_{t+1}) | S_t = s, A_t=a] $$ The true action value for action a is the expected reward $$ q(a) = mathbb{E} left {R_t | A_t=a right } $$ | . A simple estimate of the action value is the average of the sampled rewards: $$ Q_t(a)= frac{ sum_{n}^{}R_n mathbb{I}(A_n=a)}{ sum_{n}^{} mathbb{I}(A_n=a)} $$ | . where $R_n$ is the reward at time n. . The update of the action values at tume step n+1 $$ Q_{n+1} = Q_n + frac{1}{n} (R_n - Q_n) $$ | . where $ alpha= frac{1}{n}$ is a step size. . The optimal value is $$ v_* = max_{a in A} q(a) $$ . | Regret is the opportunity loss for one step . | . $$ v_* - q(A_t) $$ Action Regret $ Delta_a$ for a fiven action is the difference between optimal value and true value of a $$ Delta_a = v_* -q(a) $$ | . The trade off between exploration and exploitation will be done by minimizing the total regret | . $$ begin{split} L_t &amp;= sum_{i=1}^t (v_* - q(a_i)) &amp;= sum _{a in mathcal{A}} N_t(a)(v_* - q(a_i)) = sum _{a in mathcal{A}} N_t(a) Delta _a end{split} $$ Categorizing Agents . Value Based (Value Function) | Policy Based (Policy) | Actor Critic (Policy and Value Function) | . | Prediction and Control . Prediction: evaluate the future for a given policy | Control: optimize the future (find the best policy) $$ pi_*(s) = argmax_{ pi} v_ pi (s) $$ | . | . Introduction . The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the action taken rather than instructs by giving correct actions. It creates the need for active exploration, for an explicit search of good behaviour. . Evaluative feedback indicates how good the action taken was, but not whether it was the best of worst action possible. On the other hand, instructive feedback indicates the correct action to take which is the basis of supervised learning. . The well known trade-off between exploitation and exploration is essentially the compromise between maximizing performance (exploitation) and increasing the knowledge (exploration). It is the typical problem in online decision making because we are actively collecting our information to make the best overall decisions. . Multi-Armed Bandit . Consider the following learning problem: we are amongs the choice of k different options, after each choice, we receiver a numerical reward which are sampled from a stationary probability distribution that depends on what we selected. The objective is to maximize the expected reward $ sum _i R_i$ over some time period. This is the original form of the k-armed bandit problem. . When we look into the total regrets, we see the differences between the optimal value and the actual action value which are accumulated as time evolves. The objective is to minimise that regrets because the faster it grows, the worst it is. . In principle, the regret can grow unbounded, so the interesting part is to study how fast it grows. For example, greedy policy has linear regret as it grows in the number of step we have taken. . $ epsilon$-greedy algorithm . To explore the new information, the most basic strategy is to apply the common solution $ epsilon$-greedy where a randomness is added to the policy for picking a random action uniformly across all of the actions on each time step with a certain probability. It is considered as the most common exploration strategy in current Reinforcement Learning. By doing this, we can avoid the sticking at the sub optimal action forever as the greedy policy does and it continues to explore forever. . Typically, the performance of $ epsilon$-greedy is better than greedy because we do learn event and we select the optimal action with probability 1-$ epsilon$ which is not guaranteed in greedy case. However, the regret still grows linearly so the question is whether ncessary to explore something that are already bad? . Lower Bound . A decade ago, there was an investigation about the exploration problem to see what is the best possible thing we could hope to get. It is related to the similarities between the optimal action and all the others actions (similar distribution but different means). . Lai and Robbins Theorem: Asymtotic total regret is at least logarithmic in number of steps $$ lim_{t to infty} L_t geq log t sum ... $$ . It shows out that the total expected regrets that we will incur for any algorithm will grow on the logarithm of time. So, it means that the regrets grows unbounded as expected. . However, the grow speed of $ epsilon$-greedy is much slower than the greedy. . In the optimism in the face of uncertainty, practically, if we are more uncertain about the value of an action, maybe it is more important to explore that action but at first we need to be more greedy to try the action with the highest mean. . Upper Confidence Bound . Exploration is needed because there is always uncertainty about the accuracy of the action-value estimates. So it will be better to select amongs the non-greedy actions according to their potential for actually being optimal , taking into account how close their estimates are maximal and the uncertainties in those estimates. . One effective method to do so is upper confidence bound $U_t(a)$ for each action value $q(a)$. We&quot;ll do such as the true value with a high probability is smaller than the current estimate plus the bonus $q(a) leq Q_t(a)+U_t(a)$ . . It signifies that although we do not knwo the true value is but we are pretty certain that it is less than something. We will then design an algorithm that just greedily selects among the actions whose estimates are added a bonus. Normally the bonus will be high if the uncertainties are high. . $$ A_t = argmax_{a} [Q_t(a) + c sqrt{ frac{ln t}{N_t(a)}}] $$ Where $N_t(a)$ represents the number of times action $a$ has been selected prior to time t. Then, the uncertainty about the true value will typically decrease as the square root of the number of times you selected an action. . Recall that we want to minimize $ sum_a N_t(a) Delta_a$, so if the gap $ Delta_a$ is big, we want $N_t(a)$ to be small and vice versa. As $N_t(a)$ grows over the time, so we hope to often select the actions with low gap than the actions with high gap. . Hoeffding&#39;s Inequality: Let $X_1, .., X_n$ be i.i.d random variables in [0,1], and let $ overline{X_t}$ be the mean. Then $$ p( mathbb{E}[X] geq overline{X_n}+u) leq exp{(-2nu^2)} $$ . It means that if we selected a number of points and averaged these, it is an added bonus that we can bound the probability that it is still an underestimate. The bigger u us, the smaller this probability will be. . Now, let&#39;s say that we want to pick certain probability $p$ that the true value exceeds the upper confidence bound. . $$ exp{(-2N_t(a)U_t(a)^2)} = p $$Then, . $$ U_t(a) = sqrt{( frac{- log p}{2N_t(a)})} = sqrt{( frac{ log t}{2N_t(a)})} $$ Then, it leads to our UCB algorithm: . $$ a_t = argmax_{a in mathcal{A}} Q_t(a) + c sqrt{ frac{ log t}{N_t(a)})} $$ Intuitively speaking, if we consider an action that we have not selected in a long long while, it means that the bonus keep growing ultil it is higher than all of the other estimate plus bonuses for all the other actions. At that point, we will select it. . Bayesian Bandits . Besides value-based algorithm, we can come up with model-based approach which predict the reward from reward model for each action. However, we are based on the model, so we can model the distribution of rewards as well and it gives us Bayesian bandit. . Bayesian bandits model parameterized distributions over rewards, $p(q(a) | theta_t)$ which is a Bayesian probability. This is interpreted as our belief that $q(a)=x$ $ forall x in mathcal{R}$ . We are going to update this using probability of the reward under the model. $$ p_t( theta |a) propto p(R_t | theta,a)p_{t-1}( theta|a) $$ . $ theta$ represents the parameters of our parametric model. Then, it allows us to inject rich prior knowledge $p_0( theta|a)$, it means that we could update the parameters each time we see our reward from taken action. .",
            "url": "https://dnlam.github.io/fastblog/2020/06/06/Exploration_Exploitation.html",
            "relUrl": "/2020/06/06/Exploration_Exploitation.html",
            "date": " • Jun 6, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://dnlam.github.io/fastblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there, I am Lam DINH (Ngoc-Lam DINH), I graduated from Ha Noi University of Science and Technology (Vietnam) in 2016 with a degree in Electronics and Telecommunications Engineering. My training mainly focused on several areas: Digital Signal Processing, Wireless Communication and Embedded Programming. Then, I continued my study with a special interest in Signal Theory, Wireless Telecommunications and Optical Networks at Universidad Politecnica de Valencia (Spain) in 2017. . My Master’s degree is jointly awarded by the École Normale Supérieure Paris Saclay (France) and the Universidad Complutense de Madrid (Spain) in 2019. The courses are related to the application of molecular photonics to telecommunications and biosensors. . From 2019, I am conducting a PhD thesis at the Commissariat à l’Énergie Atomique (CEA) in Grenoble. My research addresses ultra-reliable and low-latency communications (URLLC) in 5G systems and beyond. . More Information . Personal Web Page .",
          "url": "https://dnlam.github.io/fastblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://dnlam.github.io/fastblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
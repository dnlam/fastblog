{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process and Dynamic Programming\n",
    "\n",
    "> A notebook that helps us to discover Reinforcement Learning\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- metadata_key1: metadata_value1\n",
    "- metadata_key2: metadata_value2\n",
    "- image: https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQChSoDYsSnyFzrYnjJXEA6d5-kRELW-lzYcA&usqp=CAU\n",
    "- description: Second in a series on understanding Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes (MDP)\n",
    "MDPs are a classical formulations of sequential decision making, where each action influences not only immediate rewards, but also subsequent situations, or states and through those future rewards. Thus MDPs involve delayed reward and the need to trade-off immediate reward and delay reward.\n",
    "\n",
    "In bandit problems, we estimated the value $q_*(a)$ of each action a. In MDP, we estimate the value $q_*(s,a)$ of each action a in each state s, or we estimate the value $v_*(s)$ of each state given optimal action selections. These states dependent quantities are essential to accurately assign credit for long term consequences to individual action selections. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

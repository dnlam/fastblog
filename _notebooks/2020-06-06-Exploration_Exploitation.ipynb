{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation-Exploration Dilemma\n",
    "> A notebook that helps us to discover Reinforcement Learning\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- metadata_key1: metadata_value1\n",
    "- metadata_key2: metadata_value2\n",
    "- image: https://miro.medium.com/max/1400/1*ywOrdJAHgSL5RP-AuxsfJQ.png\n",
    "- description: First in a series on understanding Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "* A ``policy`` defines the agent's behaviours\n",
    "    * Deterministic policy A = $\\pi(S)$\n",
    "    * Stochastic policy: $\\pi(A|S) = p(A|S)$ \n",
    "- The actual value function is the expected return\n",
    "$$\n",
    "\\begin{split}\n",
    "v_\\pi(s) = \\mathbb{E} [G_t | S_t = s, \\pi] &= \\mathbb{E} [R_{t+1} + \\gamma R_{t+2 + ...} | S_t = s, \\pi] \\\\\n",
    "&=\\mathbb{E} [R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t = s, A_t ~= \\pi(s)]\n",
    "\n",
    "\\end{split}\n",
    "$$\n",
    "where $\\gamma$ is a discount factor\n",
    "\n",
    "- Optimal value is the highest possible value for any policy\n",
    "\n",
    "$$\n",
    "v_*(s) = \\max_{a} \\mathbb{E} [R_{t+1} + \\gamma v_*(S_{t+1}) | S_t = s, A_t=a]\n",
    "$$\n",
    "\n",
    "\n",
    "- The true action value for action ``a`` is the expected reward\n",
    "$$\n",
    "q(a) = \\mathbb{E}\\left\\{R_t | A_t=a \\right\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A simple estimate of the action value is the average of the sampled rewards:\n",
    "$$\n",
    "Q_t(a)=\\frac{\\sum_{n}^{}R_n\\mathbb{I}(A_n=a)}{\\sum_{n}^{}\\mathbb{I}(A_n=a)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $R_n$ is the reward at time n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The update of the action values at tume step n+1\n",
    "$$\n",
    "Q_{n+1} = Q_n + \\frac{1}{n} (R_n - Q_n)\n",
    "$$\n",
    "\n",
    "where $\\alpha=\\frac{1}{n}$ is a step size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The optimal value is\n",
    "$$\n",
    "v_* = \\max_{a \\in A}  q(a)\n",
    "$$\n",
    "\n",
    "- Regret is the opportunity loss for one step\n",
    "\n",
    "$$\n",
    "v_* - q(A_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Action Regret $\\Delta_a$ for a fiven action is the difference between optimal value and true value of a\n",
    "$$\n",
    "\\Delta_a = v_* -q(a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The trade off between exploration and exploitation will be done by minimizing the total regret\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "L_t &= \\sum_{i=1}^t (v_* - q(a_i)) \\\\\n",
    "&= \\sum _{a \\in \\mathcal{A}} N_t(a)(v_* - q(a_i)) = \\sum _{a \\in \\mathcal{A}} N_t(a) \\Delta _a\n",
    "\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Categorizing Agents\n",
    "    * Value Based (Value Function)\n",
    "    * Policy Based (Policy)\n",
    "    * Actor Critic (Policy and Value Function)\n",
    "\n",
    "* Prediction and Control\n",
    "    * Prediction: evaluate the future for a given policy\n",
    "    * Control: optimize the future (find the best policy)\n",
    "$$\n",
    "\\pi_*(s) = \\argmax_{\\pi} v_\\pi (s) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the action taken rather than instructs by giving correct actions. It creates the need for active exploration, for an explicit search of good behaviour. \n",
    "\n",
    "Evaluative feedback indicates how good the action taken was, but not whether it was the best of worst action possible. On the other hand, instructive feedback indicates the correct action to take  which is the basis of supervised learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The well known trade-off between exploitation and exploration is essentially the compromise between maximizing performance (exploitation)  and increasing the knowledge (exploration). It is the typical problem in online decision making because we are actively collecting our information to make the best overall decisions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Armed Bandit\n",
    "\n",
    "Consider the following learning problem: we are amongs the choice of k different options, after each choice, we receiver a numerical reward which are sampled from a stationary probability distribution that depends on what we selected. The objective is to maximize the expected reward $\\sum _i R_i$ over some time period. This is the original form of the k-armed bandit problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look into the total regrets, we see the differences between the optimal value and the actual action value which are accumulated as time evolves. The objective is to minimise that regrets because the faster it grows, the worst it is. \n",
    "\n",
    "In principle, the regret can grow unbounded, so the interesting part is to study how fast it grows. For example, greedy policy has linear regret as it grows in the number of step we have taken. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

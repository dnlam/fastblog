{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Free Learning Method\n",
    "\n",
    "> A notebook that helps us to discover Reinforcement Learning\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- metadata_key1: metadata_value1\n",
    "- metadata_key2: metadata_value2\n",
    "- image: https://www.mdpi.com/symmetry/symmetry-12-01685/article_deploy/html/images/symmetry-12-01685-g002.png\n",
    "- description: Third in a series on understanding Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Previously, we talked about using  [Dynamic Programming](https://dnlam.github.io/fastblog/2020/06/14/MDP_DP.html), that have access to the full model, to solve optimal Bellman's Equation using MDPs. Thus, DP can learn directly from the interaction with the environment. \n",
    "\n",
    "In this Notebook, we will study model free learning methods that do not require the full model's knowledge but only experience to attain optimal behaviour.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo (MC) methods are ways of solving the reinforcement learning problem based on averaging sample returns for episodic tasks. Only a completion of an episode, the value estimates and policies will be changed.\n",
    "\n",
    "Here, we talked about an episode which is defined as an trajectory of experience having some natural ending points. \n",
    "\n",
    "MC is a model free algorithm because it does not require any knowledge of MDP but only interaction returns or samples. As a concrete example, multi-armed bandit can be considered as a MC algorithm because the average reward samples (action values) are estimated for each action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the update form, the iterative true value of an action under multi-armed bandit is calculated as below:\n",
    "$$\n",
    "q_{t+1}(A_t) = q_t(A_t) + \\frac{1}{N(t)} (R_{t+1} - q_t(A_t))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extend slightly the above bandit problem with different states. In MAB, the episodes are 1 step long as the bandit case before and it means that the actions do not affect the state transitions (you perform an action and you might see a new state that does not depend on your action.). Now, multiple states may appear and do not depend on the actions taken which means there is no long-term consequences. Then the goal is to estimate the action value that is conditioned on the action and state (contextual bandit).\n",
    "$$\n",
    "q(s,a) = \\mathbb{E} [R_{t+1} | S_t=s, A_t=a]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function Approximation\n",
    "So far, we talked about the computation of values in MDP are based on the look up table. But it comes with a problem when a large MDP is introduced when it will consume a huge amount of memory to store every value entry.\n",
    "\n",
    "A possible solution for those problem is to use function approximation where the value function and action value function can be parameterized. Then, the parameter will be updated to obtain a value function instead of updating a huge entry during the learning process.\n",
    "$$\n",
    "v_w(s) \\approx v_\\pi(s) \\\\\\\\\n",
    "q_w(s,a) \\approx a_\\pi(s,a) \\\\\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In model free learning, parameter $w$ can be updated as MC Algorithm or Temporal Difference Learning to generalize the unseen states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of the environment state is not fully observable, we will use agent state $S_t = u_w(S_{t-1},A_{t-1},O_t)$ parameterized by parameters $w$. Then, the current state is just a function of previous inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make a concrete example of a linear function approximation, where linear function can be represented a fixed feature vectors.\n",
    "\n",
    "$$\n",
    "x(s)=\\begin{pmatrix}\n",
    "x_1(s) \\\\\n",
    "... \\\\\n",
    "x_m(s) \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing that, we can transfer the states into a bunch of vectors with elements.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the linear function approximation approach takes these features and defines our value function to be in the inner product.\n",
    "\n",
    "$$\n",
    "v_w(s) = w^Tx(s) = \\sum_{j=1}^n x_j(s)w_j\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update the our weights $w$, we need some sorts of objective. In our concern, the predictions will be the case so the objective will be the minimization of the loss  between the true value and our estimate.\n",
    "$$\n",
    "L(w) = \\mathbb{E} [(v_\\pi(S) - w^T x(S))^2]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By apply Stochastic Gradient Descent for this objective, it will converge to global optimum of this loss function because the loss function is convex.\n",
    "\n",
    "Then, the update rule is relatively simple because the gradient of the value function with respect to our parameter $w$ is the feature $x$ and the stochastic gradient update if we would have the true value function $v_\\pi$ will be the step size times the error terms of prediction errors and the feature vector.\n",
    "$$\n",
    "\\Delta w = \\alpha (v_\\pi(S_t) - v_w(S_t))x_t \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the table lookup is a special case of linear value function approximation. \n",
    "Let's n states be given by $\\mathcal{S} = \\left\\{s_1,...,s_n \\right\\}$\n",
    "\n",
    "One hot feature represents the zeros on almost of the component except on the component that correspond the the state that we care about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x(s)=\\begin{pmatrix}\n",
    "\\mathcal{I}(x_1(s)) \\\\\n",
    "... \\\\\n",
    "\\mathcal{I}(x_m(s)) \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, parameter $w$ contains just value estimates for each state.\n",
    "\n",
    "$ v(s) = w^Tx(s) = \\sum _j w_j x_j(s) = w_s $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Free Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, q could be a parametric function (neural network) and we could use the following loss\n",
    "$$\n",
    "L(w) = \\frac{1}{2} \\mathbb{E} [(R_{t+1} - q_w(S_t,A_t))^2]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient update will be\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\alpha \\triangledown {w_t} L(w_t) \\\\\\\\\n",
    "= w_t - \\alpha \\triangledown _{w_t} \\frac{1}{2} \\mathbb{E} [(R_{t+1} - q_w(S_t,A_t))^2] \\\\\\\\\n",
    "= w_t + \\alpha \\mathbb{E} [(R_{t+1} - q_w(S_t,A_t)) \\triangledown _{w_t} q_{w_t}(S_t,A_t) ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of a large continuous state space $\\mathcal{S}$ appears, it is just regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the action value function can be represented as a linear function, e.g. $q(s,a) = w^T x(s,a)$, then \n",
    "$$\n",
    "w_{t+1} = w_t + \\alpha (R_{t+1} - q_w(S_t,A_t)) x(s,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Policy Evaluation\n",
    "Now, let's consider sequential decision problems where the objective is to learn $v_\\pi$ from episodes of experience under policy  $\\pi$ under discouted factor $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "S_1, A_1, R_2, ..., S_k \\sim \\pi\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discounted reward will be:\n",
    "$$\n",
    "G_t = R_{t+1} + ... + \\gamma ^{T-t-1} R_T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected return :\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}[G_t | S_t=s, \\pi]\n",
    "$$\n",
    "Here, we could use sample average return instead of expected return as the target of our updates and it is called Monte Carlo policy evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Drawbacks of MC Learning\n",
    "- When episodes are long, learning can be quite slow because we need to wait until the end of the episode before start updating (The return is not well defined before we do.) and the return can have high variance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind about the Bellman's equation relates the value of a state with the value of the next state and it is the definition of the value of a policy. \n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E} [R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t=s, A_t \\sim \\pi(S_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before that, we learned that we can approximate these values by tuning this definition into an update  where $v_\\pi$ is now replaced with the current estimate $v_k$. However, we should notice that the update can not happen at time step t because it is not known yet the return at time step t. \n",
    "This iterative algorithm do learn and the do find the true value for the policy.\n",
    "$$\n",
    "v_{k+1}(s) = \\mathbb{E} [R_{t+1} + \\gamma v_k(S_{t+1}) | S_t=s, A_t \\sim \\pi(S_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It suggested that we can sample this so that $v_{t+1}(S_t) = R_{t+1} + \\gamma v_t(S_{t+1}) $. then, it is better to take a small step with parameter $\\alpha$ than do all the way like that, so\n",
    "$$\n",
    "v_{t+1}(S_t) = v_t(S_t) + \\alpha _t (R_{t+1} + \\gamma v_t(S_{t+1}) - v_t(S_t))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very similar to MC algorithm but instead of updating towards a full return, we are going to update the target $R_{t+1} + \\gamma v_t(S_{t+1})$. So it uses our current value estimates instead of full return. It is called ```temporal difference``` learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in temporal difference learning, the new target unrolls the experience one step and then uses our estimates to replace the rest of the return. \n",
    "The ```temporal difference error ```($\\delta _t$) now is the difference in value from what we currently thing ($v_t(S_t)$) comparing to the one step into the future ($R_{t+1} + \\gamma v_t(S_{t+1})$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

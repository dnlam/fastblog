<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Model Free Learning Method | Lam Dinh</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Model Free Learning Method" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Third in a series on understanding Reinforcement Learning." />
<meta property="og:description" content="Third in a series on understanding Reinforcement Learning." />
<link rel="canonical" href="https://dnlam.github.io/fastblog/2022/04/16/Model_Free_Prediction_Control.html" />
<meta property="og:url" content="https://dnlam.github.io/fastblog/2022/04/16/Model_Free_Prediction_Control.html" />
<meta property="og:site_name" content="Lam Dinh" />
<meta property="og:image" content="https://www.mdpi.com/symmetry/symmetry-12-01685/article_deploy/html/images/symmetry-12-01685-g002.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-04-16T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://www.mdpi.com/symmetry/symmetry-12-01685/article_deploy/html/images/symmetry-12-01685-g002.png" />
<meta property="twitter:title" content="Model Free Learning Method" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-04-16T00:00:00-05:00","datePublished":"2022-04-16T00:00:00-05:00","description":"Third in a series on understanding Reinforcement Learning.","headline":"Model Free Learning Method","image":"https://www.mdpi.com/symmetry/symmetry-12-01685/article_deploy/html/images/symmetry-12-01685-g002.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://dnlam.github.io/fastblog/2022/04/16/Model_Free_Prediction_Control.html"},"url":"https://dnlam.github.io/fastblog/2022/04/16/Model_Free_Prediction_Control.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/fastblog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://dnlam.github.io/fastblog/feed.xml" title="Lam Dinh" /><link rel="shortcut icon" type="image/x-icon" href="/fastblog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/fastblog/">Lam Dinh</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/fastblog/about/">About Me</a><a class="page-link" href="/fastblog/search/">Search</a><a class="page-link" href="/fastblog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Model Free Learning Method</h1><p class="page-description">Third in a series on understanding Reinforcement Learning.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-04-16T00:00:00-05:00" itemprop="datePublished">
        Apr 16, 2022
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/dnlam/fastblog/tree/master/_notebooks/2020-21-06-Model_Free_Prediction_Control.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/fastblog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/dnlam/fastblog/master?filepath=_notebooks%2F2020-21-06-Model_Free_Prediction_Control.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/fastblog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/dnlam/fastblog/blob/master/_notebooks/2020-21-06-Model_Free_Prediction_Control.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/fastblog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fdnlam%2Ffastblog%2Fblob%2Fmaster%2F_notebooks%2F2020-21-06-Model_Free_Prediction_Control.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/fastblog/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#Introduction">Introduction </a></li>
<li class="toc-entry toc-h1"><a href="#Monte-Carlo-Algorithm">Monte Carlo Algorithm </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Value-Function-Approximation">Value Function Approximation </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Linear-Function-Approximation">Linear Function Approximation </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Model-Free-Prediction">Model-Free Prediction </a></li>
<li class="toc-entry toc-h2"><a href="#MC-Policy-Evaluation">MC Policy Evaluation </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-21-06-Model_Free_Prediction_Control.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction<a class="anchor-link" href="#Introduction"> </a>
</h1>
<p>Previously, we talked about using  <a href="https://dnlam.github.io/fastblog/2020/06/14/MDP_DP.html">Dynamic Programming</a>, that have access to the full model, to solve optimal Bellman's Equation using MDPs. Thus, DP can learn directly from the interaction with the environment.</p>
<p>In this Notebook, we will study model free learning methods that do not require the full model's knowledge but only experience to attain optimal behaviour.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Monte-Carlo-Algorithm">
<a class="anchor" href="#Monte-Carlo-Algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>Monte Carlo Algorithm<a class="anchor-link" href="#Monte-Carlo-Algorithm"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Monte Carlo (MC) methods are ways of solving the reinforcement learning problem based on averaging sample returns for episodic tasks. Only a completion of an episode, the value estimates and policies will be changed.</p>
<p>Here, we talked about an episode which is defined as an trajectory of experience having some natural ending points.</p>
<p>MC is a model free algorithm because it does not require any knowledge of MDP but only interaction returns or samples. As a concrete example, multi-armed bandit can be considered as a MC algorithm because the average reward samples (action values) are estimated for each action.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the update form, the iterative true value of an action under multi-armed bandit is calculated as below:
$$
q_{t+1}(A_t) = q_t(A_t) + \frac{1}{N(t)} (R_{t+1} - q_t(A_t))
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's extend slightly the above bandit problem with different states. In MAB, the episodes are 1 step long as the bandit case before and it means that the actions do not affect the state transitions (you perform an action and you might see a new state that does not depend on your action.). Now, multiple states may appear and do not depend on the actions taken which means there is no long-term consequences. Then the goal is to estimate the action value that is conditioned on the action and state (contextual bandit).
$$
q(s,a) = \mathbb{E} [R_{t+1} | S_t=s, A_t=a]
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Value-Function-Approximation">
<a class="anchor" href="#Value-Function-Approximation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Value Function Approximation<a class="anchor-link" href="#Value-Function-Approximation"> </a>
</h2>
<p>So far, we talked about the computation of values in MDP are based on the look up table. But it comes with a problem when a large MDP is introduced when it will consume a huge amount of memory to store every value entry.</p>
<p>A possible solution for those problem is to use function approximation where the value function and action value function can be parameterized. Then, the parameter will be updated to obtain a value function instead of updating a huge entry during the learning process.
$$
v_w(s) \approx v_\pi(s) \\\\
q_w(s,a) \approx a_\pi(s,a) \\\\
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In model free learning, parameter $w$ can be updated as MC Algorithm or Temporal Difference Learning to generalize the unseen states.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In case of the environment state is not fully observable, we will use agent state $S_t = u_w(S_{t-1},A_{t-1},O_t)$ parameterized by parameters $w$. Then, the current state is just a function of previous inputs.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Linear-Function-Approximation">
<a class="anchor" href="#Linear-Function-Approximation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear Function Approximation<a class="anchor-link" href="#Linear-Function-Approximation"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will make a concrete example of a linear function approximation, where linear function can be represented a fixed feature vectors.</p>
$$
x(s)=\begin{pmatrix}
x_1(s) \\
... \\
x_m(s) \\
\end{pmatrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By doing that, we can transfer the states into a bunch of vectors with elements.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then, the linear function approximation approach takes these features and defines our value function to be in the inner product.</p>
$$
v_w(s) = w^Tx(s) = \sum_{j=1}^n x_j(s)w_j
$$<p></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To update the our weights $w$, we need some sorts of objective. In our concern, the predictions will be the case so the objective will be the minimization of the loss  between the true value and our estimate.
$$
L(w) = \mathbb{E} [(v_\pi(S) - w^T x(S))^2]
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By apply Stochastic Gradient Descent for this objective, it will converge to global optimum of this loss function because the loss function is convex.</p>
<p>Then, the update rule is relatively simple because the gradient of the value function with respect to our parameter $w$ is the feature $x$ and the stochastic gradient update if we would have the true value function $v_\pi$ will be the step size times the error terms of prediction errors and the feature vector.
$$
\Delta w = \alpha (v_\pi(S_t) - v_w(S_t))x_t 
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Basically, the table lookup is a special case of linear value function approximation. 
Let's n states be given by $\mathcal{S} = \left\{s_1,...,s_n \right\}$</p>
<p>One hot feature represents the zeros on almost of the component except on the component that correspond the the state that we care about.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
x(s)=\begin{pmatrix}
\mathcal{I}(x_1(s)) \\
... \\
\mathcal{I}(x_m(s)) \\
\end{pmatrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then, parameter $w$ contains just value estimates for each state.</p>
<p>$ v(s) = w^Tx(s) = \sum _j w_j x_j(s) = w_s $</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Model-Free-Prediction">
<a class="anchor" href="#Model-Free-Prediction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model-Free Prediction<a class="anchor-link" href="#Model-Free-Prediction"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Basically, q could be a parametric function (neural network) and we could use the following loss
$$
L(w) = \frac{1}{2} \mathbb{E} [(R_{t+1} - q_w(S_t,A_t))^2]
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The gradient update will be</p>
$$
w_{t+1} = w_t - \alpha \triangledown {w_t} L(w_t) \\\\
= w_t - \alpha \triangledown _{w_t} \frac{1}{2} \mathbb{E} [(R_{t+1} - q_w(S_t,A_t))^2] \\\\
= w_t + \alpha \mathbb{E} [(R_{t+1} - q_w(S_t,A_t)) \triangledown _{w_t} q_{w_t}(S_t,A_t) ]
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In case of a large continuous state space $\mathcal{S}$ appears, it is just regression.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If the action value function can be represented as a linear function, e.g. $q(s,a) = w^T x(s,a)$, then 
$$
w_{t+1} = w_t + \alpha (R_{t+1} - q_w(S_t,A_t)) x(s,a)
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="MC-Policy-Evaluation">
<a class="anchor" href="#MC-Policy-Evaluation" aria-hidden="true"><span class="octicon octicon-link"></span></a>MC Policy Evaluation<a class="anchor-link" href="#MC-Policy-Evaluation"> </a>
</h2>
<p>Now, let's consider sequential decision problems where the objective is to learn $v_\pi$ from episodes of experience under policy  $\pi$ under discouted factor $\gamma$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
S_1, A_1, R_2, ..., S_k \sim \pi
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The discounted reward will be:
$$
G_t = R_{t+1} + ... + \gamma ^{T-t-1} R_T
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The expected return :
$$
v_\pi(s) = \mathbb{E}[G_t | S_t=s, \pi]
$$
Here, we could use sample average return instead of expected return as the target of our updates and it is called Monte Carlo policy evaluation.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="dnlam/fastblog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/fastblog/2022/04/16/Model_Free_Prediction_Control.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/fastblog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/fastblog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/fastblog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My blog about code and ideas.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/dnlam" target="_blank" title="dnlam"><svg class="svg-icon grey"><use xlink:href="/fastblog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/lam-dinh-34a66a160" target="_blank" title="lam-dinh-34a66a160"><svg class="svg-icon grey"><use xlink:href="/fastblog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>

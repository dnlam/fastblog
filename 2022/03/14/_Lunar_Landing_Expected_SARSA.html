<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Assignment 2 - Implement your agent | Lam Dinh</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Assignment 2 - Implement your agent" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My blog about code and ideas." />
<meta property="og:description" content="My blog about code and ideas." />
<link rel="canonical" href="https://dnlam.github.io/fastblog/2022/03/14/_Lunar_Landing_Expected_SARSA.html" />
<meta property="og:url" content="https://dnlam.github.io/fastblog/2022/03/14/_Lunar_Landing_Expected_SARSA.html" />
<meta property="og:site_name" content="Lam Dinh" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-03-14T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Assignment 2 - Implement your agent" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-03-14T00:00:00-05:00","datePublished":"2022-03-14T00:00:00-05:00","description":"My blog about code and ideas.","headline":"Assignment 2 - Implement your agent","mainEntityOfPage":{"@type":"WebPage","@id":"https://dnlam.github.io/fastblog/2022/03/14/_Lunar_Landing_Expected_SARSA.html"},"url":"https://dnlam.github.io/fastblog/2022/03/14/_Lunar_Landing_Expected_SARSA.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/fastblog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://dnlam.github.io/fastblog/feed.xml" title="Lam Dinh" /><link rel="shortcut icon" type="image/x-icon" href="/fastblog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/fastblog/">Lam Dinh</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/fastblog/about/">About Me</a><a class="page-link" href="/fastblog/search/">Search</a><a class="page-link" href="/fastblog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Assignment 2 - Implement your agent</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-03-14T00:00:00-05:00" itemprop="datePublished">
        Mar 14, 2022
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      36 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/dnlam/fastblog/tree/master/_notebooks/2022-03-15_Lunar_Landing_Expected_SARSA.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/fastblog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/dnlam/fastblog/master?filepath=_notebooks%2F2022-03-15_Lunar_Landing_Expected_SARSA.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/fastblog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/dnlam/fastblog/blob/master/_notebooks/2022-03-15_Lunar_Landing_Expected_SARSA.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/fastblog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fdnlam%2Ffastblog%2Fblob%2Fmaster%2F_notebooks%2F2022-03-15_Lunar_Landing_Expected_SARSA.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/fastblog/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-03-15_Lunar_Landing_Expected_SARSA.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Packages">Packages<a class="anchor-link" href="#Packages"> </a></h2><ul>
<li><a href="www.numpy.org">numpy</a> : Fundamental package for scientific computing with Python.</li>
<li><a href="http://matplotlib.org">matplotlib</a> : Library for plotting graphs in Python.</li>
<li><a href="http://www.jmlr.org/papers/v10/tanner09a.html">RL-Glue</a>, BaseEnvironment, BaseAgent : Library and abstract classes to inherit from  for reinforcement learning experiments.</li>
<li><a href="https://gym.openai.com/envs/LunarLander-v2/">LunarLanderEnvironment</a> : An RLGlue environment that wraps a LundarLander environment implementation from OpenAI Gym.</li>
<li><a href="https://docs.python.org/3/library/collections.html#collections.deque">collections.deque</a>: a double-ended queue implementation. We use deque to implement the experience replay buffer.</li>
<li><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy">copy.deepcopy</a>: As objects are not passed by value in python, we often need to make copies of mutable objects. copy.deepcopy allows us to make a new object with the same contents as another object. (Take a look at this link if you are interested to learn more: <a href="https://robertheaton.com/2014/02/09/pythons-pass-by-object-reference-as-explained-by-philip-k-dick/">https://robertheaton.com/2014/02/09/pythons-pass-by-object-reference-as-explained-by-philip-k-dick/</a>)</li>
<li><a href="https://github.com/tqdm/tqdm">tqdm</a> : A package to display progress bar when running experiments</li>
<li><a href="https://docs.python.org/3/library/os.html">os</a>: Package used to interface with the operating system. Here we use it for creating a results folder when it does not exist.</li>
<li><a href="https://docs.python.org/3/library/shutil.html">shutil</a>: Package used to operate on files and folders. Here we use it for creating a zip file of the results folder.</li>
<li>plot_script: Used for plotting learning curves using matplotlib.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Import necessary libraries</span>
<span class="c1"># DO NOT IMPORT OTHER LIBRARIES - This will break the autograder.</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">from</span> <span class="nn">rl_glue</span> <span class="kn">import</span> <span class="n">RLGlue</span>
<span class="kn">from</span> <span class="nn">environment</span> <span class="kn">import</span> <span class="n">BaseEnvironment</span>
<span class="kn">from</span> <span class="nn">lunar_lander</span> <span class="kn">import</span> <span class="n">LunarLanderEnvironment</span>
<span class="kn">from</span> <span class="nn">agent</span> <span class="kn">import</span> <span class="n">BaseAgent</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">os</span> 
<span class="kn">import</span> <span class="nn">shutil</span>
<span class="kn">from</span> <span class="nn">plot_script</span> <span class="kn">import</span> <span class="n">plot_result</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Section-1:-Action-Value-Network">Section 1: Action-Value Network<a class="anchor-link" href="#Section-1:-Action-Value-Network"> </a></h2><p>This section includes the function approximator that we use in our agent, a neural network. In Course 3 Assignment 2, we used a neural network as the function approximator for a policy evaluation problem. In this assignment, we will use a neural network for approximating the action-value function in a control problem. The main difference between approximating a state-value function and an action-value function using a neural network is that in the former the output layer only includes one unit whereas in the latter the output layer includes as many units as the number of actions.</p>
<p>In the cell below, you will specify the architecture of the action-value neural network. More specifically, you will specify <code>self.layer_sizes</code> in the <code>__init__()</code> function.</p>
<p>We have already provided <code>get_action_values()</code> and <code>get_TD_update()</code> methods. The former computes the action-value function by doing a forward pass and the latter computes the gradient of the action-value function with respect to the weights times the TD error. These <code>get_action_values()</code> and <code>get_TD_update()</code> methods are similar to the <code>get_value()</code> and <code>get_gradient()</code> methods that you implemented in Course 3 Assignment 2. The main difference is that in this notebook, they are designed to be applied to batches of states instead of one state. You will later use these functions for implementing the agent.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Graded Cell</span>
<span class="c1"># -----------</span>

<span class="c1"># Work Required: Yes. Fill in the code for layer_sizes in __init__ (~1 Line). </span>
<span class="c1"># Also go through the rest of the code to ensure your understanding is correct.</span>
<span class="k">class</span> <span class="nc">ActionValueNetwork</span><span class="p">:</span>
    <span class="c1"># Work Required: Yes. Fill in the layer_sizes member variable (~1 Line).</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network_config</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span> <span class="o">=</span> <span class="n">network_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;state_dim&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_units</span> <span class="o">=</span> <span class="n">network_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;num_hidden_units&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span> <span class="o">=</span> <span class="n">network_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;num_actions&quot;</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">rand_generator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">network_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;seed&quot;</span><span class="p">))</span>
        
        <span class="c1"># Specify self.layer_sizes which shows the number of nodes in each layer</span>
        <span class="c1"># your code here</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_units</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span><span class="p">]</span>
        <span class="c1"># Initialize the weights of the neural network</span>
        <span class="c1"># self.weights is an array of dictionaries with each dictionary corresponding to </span>
        <span class="c1"># the weights from one layer to the next. Each dictionary includes W and b</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="nb">dict</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;W&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_saxe</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;b&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>
    
    <span class="c1"># Work Required: No.</span>
    <span class="k">def</span> <span class="nf">get_action_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            s (Numpy array): The state.</span>
<span class="sd">        Returns:</span>
<span class="sd">            The action-values (Numpy array) calculated using the network&#39;s weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="n">W0</span><span class="p">,</span> <span class="n">b0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;W&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
        <span class="n">psi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">W0</span><span class="p">)</span> <span class="o">+</span> <span class="n">b0</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">psi</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        
        <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;W&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
        <span class="n">q_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>

        <span class="k">return</span> <span class="n">q_vals</span>
    
    <span class="c1"># Work Required: No.</span>
    <span class="k">def</span> <span class="nf">get_TD_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">delta_mat</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            s (Numpy array): The state.</span>
<span class="sd">            delta_mat (Numpy array): A 2D array of shape (batch_size, num_actions). Each row of delta_mat  </span>
<span class="sd">            correspond to one state in the batch. Each row has only one non-zero element </span>
<span class="sd">            which is the TD-error corresponding to the action taken.</span>
<span class="sd">        Returns:</span>
<span class="sd">            The TD update (Array of dictionaries with gradient times TD errors) for the network&#39;s weights</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">W0</span><span class="p">,</span> <span class="n">b0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;W&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
        <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;W&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
        
        <span class="n">psi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">W0</span><span class="p">)</span> <span class="o">+</span> <span class="n">b0</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">psi</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="n">psi</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

        <span class="c1"># td_update has the same structure as self.weights, that is an array of dictionaries.</span>
        <span class="c1"># td_update[0][&quot;W&quot;], td_update[0][&quot;b&quot;], td_update[1][&quot;W&quot;], and td_update[1][&quot;b&quot;] have the same shape as </span>
        <span class="c1"># self.weights[0][&quot;W&quot;], self.weights[0][&quot;b&quot;], self.weights[1][&quot;W&quot;], and self.weights[1][&quot;b&quot;] respectively</span>
        <span class="n">td_update</span> <span class="o">=</span> <span class="p">[</span><span class="nb">dict</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">))]</span>
         
        <span class="n">v</span> <span class="o">=</span> <span class="n">delta_mat</span>
        <span class="n">td_update</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;W&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">td_update</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;b&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">W1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">dx</span>
        <span class="n">td_update</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;W&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">td_update</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;b&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                
        <span class="k">return</span> <span class="n">td_update</span>
    
    <span class="c1"># Work Required: No. You may wish to read the relevant paper for more information on this weight initialization</span>
    <span class="c1"># (Exact solutions to the nonlinear dynamics of learning in deep linear neural networks by Saxe, A et al., 2013)</span>
    <span class="k">def</span> <span class="nf">init_saxe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            rows (int): number of input units for layer.</span>
<span class="sd">            cols (int): number of output units for layer.</span>
<span class="sd">        Returns:</span>
<span class="sd">            NumPy Array consisting of weights for the layer based on the initialization in Saxe et al.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rand_generator</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">rows</span> <span class="o">&lt;</span> <span class="n">cols</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">T</span>
        <span class="n">tensor</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">ph</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
        <span class="n">tensor</span> <span class="o">*=</span> <span class="n">ph</span>

        <span class="k">if</span> <span class="n">rows</span> <span class="o">&lt;</span> <span class="n">cols</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">T</span>
        <span class="k">return</span> <span class="n">tensor</span>
    
    <span class="c1"># Work Required: No.</span>
    <span class="k">def</span> <span class="nf">get_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns: </span>
<span class="sd">            A copy of the current weights of this network.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>
    
    <span class="c1"># Work Required: No.</span>
    <span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args: </span>
<span class="sd">            weights (list of dictionaries): Consists of weights that this network will set as its own weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Run the cell below to test your implementation of the <code>__init__()</code> function for ActionValueNetwork:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Debugging Cell</span>
<span class="c1"># --------------</span>
<span class="c1"># Feel free to make any changes to this cell to debug your code</span>

<span class="n">network_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;state_dim&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="s2">&quot;num_hidden_units&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s2">&quot;num_actions&quot;</span><span class="p">:</span> <span class="mi">3</span>
<span class="p">}</span>

<span class="n">test_network</span> <span class="o">=</span> <span class="n">ActionValueNetwork</span><span class="p">(</span><span class="n">network_config</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;layer_sizes:&quot;</span><span class="p">,</span> <span class="n">test_network</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">)</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">test_network</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">])))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>layer_sizes: [5, 20, 3]
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Tested Cell</span>
<span class="c1"># -----------</span>
<span class="c1"># The contents of the cell will be tested by the autograder.</span>
<span class="c1"># If they do not pass here, they will not pass there.</span>

<span class="n">rand_generator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">network_config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;state_dim&quot;</span><span class="p">:</span> <span class="n">rand_generator</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
        <span class="s2">&quot;num_hidden_units&quot;</span><span class="p">:</span> <span class="n">rand_generator</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
        <span class="s2">&quot;num_actions&quot;</span><span class="p">:</span> <span class="n">rand_generator</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="n">test_network</span> <span class="o">=</span> <span class="n">ActionValueNetwork</span><span class="p">(</span><span class="n">network_config</span><span class="p">)</span>

    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">test_network</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">network_config</span><span class="p">[</span><span class="s2">&quot;state_dim&quot;</span><span class="p">],</span> 
                                                           <span class="n">network_config</span><span class="p">[</span><span class="s2">&quot;num_hidden_units&quot;</span><span class="p">],</span> 
                                                           <span class="n">network_config</span><span class="p">[</span><span class="s2">&quot;num_actions&quot;</span><span class="p">]])))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Expected output:</strong> (assuming no changes to the debugging cell)</p>

<pre><code>layer_sizes: [ 5 20  3]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Section-2:-Adam-Optimizer">Section 2: Adam Optimizer<a class="anchor-link" href="#Section-2:-Adam-Optimizer"> </a></h2><p>In this assignment, you will use the Adam algorithm for updating the weights of your action-value network. As you may remember from Course 3 Assignment 2, the Adam algorithm is a more advanced variant of stochastic gradient descent (SGD). The Adam algorithm improves the SGD update with two concepts: adaptive vector stepsizes and momentum. It keeps running estimates of the mean and second moment of the updates, denoted by $\mathbf{m}$ and $\mathbf{v}$ respectively:
$$\mathbf{m_t} = \beta_m \mathbf{m_{t-1}} + (1 - \beta_m)g_t \\
\mathbf{v_t} = \beta_v \mathbf{v_{t-1}} + (1 - \beta_v)g^2_t
$$</p>
<p>Here, $\beta_m$ and $\beta_v$ are fixed parameters controlling the linear combinations above and $g_t$ is the update at time $t$ (generally the gradients, but here the TD error times the gradients).</p>
<p>Given that $\mathbf{m}$ and $\mathbf{v}$ are initialized to zero, they are biased toward zero. To get unbiased estimates of the mean and second moment, Adam defines $\mathbf{\hat{m}}$ and $\mathbf{\hat{v}}$ as:
$$ \mathbf{\hat{m}_t} = \frac{\mathbf{m_t}}{1 - \beta_m^t} \\
\mathbf{\hat{v}_t} = \frac{\mathbf{v_t}}{1 - \beta_v^t}
$$</p>
<p>The weights are then updated as follows:
$$ \mathbf{w_t} = \mathbf{w_{t-1}} + \frac{\alpha}{\sqrt{\mathbf{\hat{v}_t}}+\epsilon} \mathbf{\hat{m}_t}
$$</p>
<p>Here, $\alpha$ is the step size parameter and $\epsilon$ is another small parameter to keep the denominator from being zero.</p>
<p>In the cell below, you will implement the <code>__init__()</code> and <code>update_weights()</code> methods for the Adam algorithm. In <code>__init__()</code>, you will initialize <code>self.m</code> and <code>self.v</code>. In <code>update_weights()</code>, you will compute new weights given the input weights and an update $g$ (here <code>td_errors_times_gradients</code>) according to the equations above.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Adam</span><span class="p">():</span>
    <span class="c1"># Work Required: Yes. Fill in the initialization for self.m and self.v (~4 Lines).</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_sizes</span><span class="p">,</span> 
                 <span class="n">optimizer_info</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_sizes</span> <span class="o">=</span> <span class="n">layer_sizes</span>

        <span class="c1"># Specify Adam algorithm&#39;s hyper parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_size</span> <span class="o">=</span> <span class="n">optimizer_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;step_size&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta_m</span> <span class="o">=</span> <span class="n">optimizer_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;beta_m&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta_v</span> <span class="o">=</span> <span class="n">optimizer_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;beta_v&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">optimizer_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">)</span>
        
        <span class="c1"># Initialize Adam algorithm&#39;s m and v</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="p">[</span><span class="nb">dict</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">))]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="p">[</span><span class="nb">dict</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">))]</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># Hint: The initialization for m and v should look very much like the initializations of the weights</span>
            <span class="c1"># except for the fact that initialization here is to zeroes (see description above.)</span>
            <span class="c1"># Replace the None in each following line</span>
            
            <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
            
            <span class="c1"># your code here</span>
            
            
        <span class="c1"># Notice that to calculate m_hat and v_hat, we use powers of beta_m and beta_v to </span>
        <span class="c1"># the time step t. We can calculate these powers using an incremental product. At initialization then, </span>
        <span class="c1"># beta_m_product and beta_v_product should be ...? (Note that timesteps start at 1 and if we were to </span>
        <span class="c1"># start from 0, the denominator would be 0.)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta_m_product</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_m</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta_v_product</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_v</span>
    
    <span class="c1"># Work Required: Yes. Fill in the weight updates (~5-7 lines).</span>
    <span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">td_errors_times_gradients</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            weights (Array of dictionaries): The weights of the neural network.</span>
<span class="sd">            td_errors_times_gradients (Array of dictionaries): The gradient of the </span>
<span class="sd">            action-values with respect to the network&#39;s weights times the TD-error</span>
<span class="sd">        Returns:</span>
<span class="sd">            The updated weights (Array of dictionaries).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)):</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="c1"># Hint: Follow the equations above. First, you should update m and v and then compute </span>
                <span class="c1"># m_hat and v_hat. Finally, compute how much the weights should be incremented by.</span>
                <span class="c1"># self.m[i][param] = None</span>
                <span class="c1"># self.v[i][param] = None</span>
                <span class="c1"># m_hat = None</span>
                <span class="c1"># v_hat = None</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">param</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_m</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">param</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">beta_m</span><span class="p">)</span><span class="o">*</span><span class="n">td_errors_times_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">param</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">param</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_v</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">param</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">beta_v</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">td_errors_times_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">param</span><span class="p">]</span> <span class="o">*</span> <span class="n">td_errors_times_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">param</span><span class="p">])</span>
                
                <span class="n">m_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">param</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_m_product</span><span class="p">)</span>
                <span class="n">v_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">param</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_v_product</span><span class="p">)</span>
                
                <span class="n">weight_update</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step_size</span> <span class="o">*</span> <span class="n">m_hat</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v_hat</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
                
                <span class="c1"># your code here</span>
                
                
                <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">param</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">param</span><span class="p">]</span> <span class="o">+</span> <span class="n">weight_update</span>
        <span class="c1"># Notice that to calculate m_hat and v_hat, we use powers of beta_m and beta_v to </span>
        <span class="c1">### update self.beta_m_product and self.beta_v_product</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta_m_product</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_m</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta_v_product</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_v</span>
        
        <span class="k">return</span> <span class="n">weights</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Run the following code to test your implementation of the <code>__init__()</code> function:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Debugging Cell</span>
<span class="c1"># --------------</span>
<span class="c1"># Feel free to make any changes to this cell to debug your code</span>

<span class="n">network_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;state_dim&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
                  <span class="s2">&quot;num_hidden_units&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
                  <span class="s2">&quot;num_actions&quot;</span><span class="p">:</span> <span class="mi">3</span>
                 <span class="p">}</span>

<span class="n">optimizer_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;step_size&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
                  <span class="s2">&quot;beta_m&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
                  <span class="s2">&quot;beta_v&quot;</span><span class="p">:</span> <span class="mf">0.999</span><span class="p">,</span>
                  <span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="mf">0.0001</span>
                 <span class="p">}</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">ActionValueNetwork</span><span class="p">(</span><span class="n">network_config</span><span class="p">)</span>
<span class="n">test_adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">,</span> <span class="n">optimizer_info</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;m[0][</span><span class="se">\&quot;</span><span class="s2">W</span><span class="se">\&quot;</span><span class="s2">] shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;m[0][</span><span class="se">\&quot;</span><span class="s2">b</span><span class="se">\&quot;</span><span class="s2">] shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;m[1][</span><span class="se">\&quot;</span><span class="s2">W</span><span class="se">\&quot;</span><span class="s2">] shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;m[1][</span><span class="se">\&quot;</span><span class="s2">b</span><span class="se">\&quot;</span><span class="s2">] shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">])))</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])))</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])))</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;v[0][</span><span class="se">\&quot;</span><span class="s2">W</span><span class="se">\&quot;</span><span class="s2">] shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;v[0][</span><span class="se">\&quot;</span><span class="s2">b</span><span class="se">\&quot;</span><span class="s2">] shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;v[1][</span><span class="se">\&quot;</span><span class="s2">W</span><span class="se">\&quot;</span><span class="s2">] shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;v[1][</span><span class="se">\&quot;</span><span class="s2">b</span><span class="se">\&quot;</span><span class="s2">] shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">])))</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])))</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])))</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])))</span>

<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span>

<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>m[0][&#34;W&#34;] shape: (5, 2)
m[0][&#34;b&#34;] shape: (1, 2)
m[1][&#34;W&#34;] shape: (2, 3)
m[1][&#34;b&#34;] shape: (1, 3) 

v[0][&#34;W&#34;] shape: (5, 2)
v[0][&#34;b&#34;] shape: (1, 2)
v[1][&#34;W&#34;] shape: (2, 3)
v[1][&#34;b&#34;] shape: (1, 3) 

</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Tested Cell</span>
<span class="c1"># -----------</span>
<span class="c1"># The contents of the cell will be tested by the autograder.</span>
<span class="c1"># If they do not pass here, they will not pass there.</span>



<span class="c1"># import our implementation of Adam</span>
<span class="c1"># while you can go look at this for the answer, try to solve the programming challenge yourself first</span>
<span class="kn">from</span> <span class="nn">tests</span> <span class="kn">import</span> <span class="n">TrueAdam</span>

<span class="n">rand_generator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">network_config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;state_dim&quot;</span><span class="p">:</span> <span class="n">rand_generator</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
        <span class="s2">&quot;num_hidden_units&quot;</span><span class="p">:</span> <span class="n">rand_generator</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
        <span class="s2">&quot;num_actions&quot;</span><span class="p">:</span> <span class="n">rand_generator</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="p">}</span>
    
    <span class="n">optimizer_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;step_size&quot;</span><span class="p">:</span> <span class="n">rand_generator</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">5</span><span class="p">)),</span>
                  <span class="s2">&quot;beta_m&quot;</span><span class="p">:</span> <span class="n">rand_generator</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mf">0.9999</span><span class="p">,</span> <span class="mf">0.99999</span><span class="p">]),</span>
                  <span class="s2">&quot;beta_v&quot;</span><span class="p">:</span> <span class="n">rand_generator</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mf">0.9999</span><span class="p">,</span> <span class="mf">0.99999</span><span class="p">]),</span>
                  <span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">rand_generator</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
                 <span class="p">}</span>

    <span class="n">test_network</span> <span class="o">=</span> <span class="n">ActionValueNetwork</span><span class="p">(</span><span class="n">network_config</span><span class="p">)</span>
    <span class="n">test_adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">test_network</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">,</span> <span class="n">optimizer_info</span><span class="p">)</span>
    <span class="n">true_adam</span> <span class="o">=</span> <span class="n">TrueAdam</span><span class="p">(</span><span class="n">test_network</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">,</span> <span class="n">optimizer_info</span><span class="p">)</span>
    
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">true_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">true_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">true_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">true_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">true_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">true_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">true_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">true_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span>

    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span>
    
    <span class="k">assert</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">beta_m_product</span> <span class="o">==</span> <span class="n">optimizer_info</span><span class="p">[</span><span class="s2">&quot;beta_m&quot;</span><span class="p">])</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">test_adam</span><span class="o">.</span><span class="n">beta_v_product</span> <span class="o">==</span> <span class="n">optimizer_info</span><span class="p">[</span><span class="s2">&quot;beta_v&quot;</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Expected output:</strong></p>

<pre><code>m[0]["W"] shape: (5, 2)
m[0]["b"] shape: (1, 2)
m[1]["W"] shape: (2, 3)
m[1]["b"] shape: (1, 3) 

v[0]["W"] shape: (5, 2)
v[0]["b"] shape: (1, 2)
v[1]["W"] shape: (2, 3)
v[1]["b"] shape: (1, 3) </code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Section-3:-Experience-Replay-Buffers">Section 3: Experience Replay Buffers<a class="anchor-link" href="#Section-3:-Experience-Replay-Buffers"> </a></h2><p>In Course 3, you implemented agents that update value functions once for each sample. We can use a more efficient approach for updating value functions. You have seen an example of an efficient approach in Course 2 when implementing Dyna. The idea behind Dyna is to learn a model using sampled experience, obtain simulated experience from the model, and improve the value function using the simulated experience.</p>
<p>Experience replay is a simple method that can get some of the advantages of Dyna by saving a buffer of experience and using the data stored in the buffer as a model. This view of prior data as a model works because the data represents actual transitions from the underlying MDP. Furthermore, as a side note, this kind of model that is not learned and simply a collection of experience can be called non-parametric as it can be ever-growing as opposed to a parametric model where the transitions are learned to be represented with a fixed set of parameters or weights.</p>
<p>We have provided the implementation of the experience replay buffer in the cell below. ReplayBuffer includes two main functions: <code>append()</code> and <code>sample()</code>. <code>append()</code> adds an experience transition to the buffer as an array that includes the state, action, reward, terminal flag (indicating termination of the episode), and next_state. <code>sample()</code> gets a batch of experiences from the buffer with size <code>minibatch_size</code>.</p>
<p>You will use the <code>append()</code> and <code>sample()</code> functions when implementing the agent.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Discussion Cell</span>
<span class="c1"># ---------------</span>

<span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">minibatch_size</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            size (integer): The size of the replay buffer.              </span>
<span class="sd">            minibatch_size (integer): The sample size.</span>
<span class="sd">            seed (integer): The seed for the random number generator. </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span> <span class="o">=</span> <span class="n">minibatch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rand_generator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span> <span class="o">=</span> <span class="n">size</span>

    <span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminal</span><span class="p">,</span> <span class="n">next_state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            state (Numpy array): The state.              </span>
<span class="sd">            action (integer): The action.</span>
<span class="sd">            reward (float): The reward.</span>
<span class="sd">            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.</span>
<span class="sd">            next_state (Numpy array): The next state.           </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span><span class="p">:</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminal</span><span class="p">,</span> <span class="n">next_state</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns:</span>
<span class="sd">            A list of transition tuples including state, action, reward, terinal, and next_state</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">idxs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rand_generator</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)),</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Section-4:-Softmax-Policy">Section 4: Softmax Policy<a class="anchor-link" href="#Section-4:-Softmax-Policy"> </a></h2><p>In this assignment, you will use a softmax policy. One advantage of a softmax policy is that it explores according to the action-values, meaning that an action with a moderate value has a higher chance of getting selected compared to an action with a lower value. Contrast this with an $\epsilon$-greedy policy which does not consider the individual action values when choosing an exploratory action in a state and instead chooses randomly when doing so.</p>
<p>The probability of selecting each action according to the softmax policy is shown below:

$$Pr{(A_t=a | S_t=s)} \hspace{0.1cm} \dot{=} \hspace{0.1cm} \frac{e^{Q(s, a)/\tau}}{\sum_{b \in A}e^{Q(s, b)/\tau}}$$

where $\tau$ is the temperature parameter which controls how much the agent focuses on the highest valued actions. The smaller the temperature, the more the agent selects the greedy action. Conversely, when the temperature is high, the agent selects among actions more uniformly random.</p>
<p>Given that a softmax policy exponentiates action values, if those values are large, exponentiating them could get very large. To implement the softmax policy in a numerically stable way, we often subtract the maximum action-value from the action-values. If we do so, the probability of selecting each action looks as follows:</p>
<p>
$$Pr{(A_t=a | S_t=s)} \hspace{0.1cm} \dot{=} \hspace{0.1cm} \frac{e^{Q(s, a)/\tau - max_{c}Q(s, c)/\tau}}{\sum_{b \in A}e^{Q(s, b)/\tau - max_{c}Q(s, c)/\tau}}$$
</p>
<p>In the cell below, you will implement the <code>softmax()</code> function. In order to do so, you could break the above computation into smaller steps:</p>
<ul>
<li>compute the preference, $H(a)$, for taking each action by dividing the action-values by the temperature parameter $\tau$,</li>
<li>subtract the maximum preference across the actions from the preferences to avoid overflow, and,</li>
<li>compute the probability of taking each action.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Graded Cell</span>
<span class="c1"># -----------</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">action_values</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        action_values (Numpy array): A 2D array of shape (batch_size, num_actions). </span>
<span class="sd">                       The action-values computed by an action-value network.              </span>
<span class="sd">        tau (float): The temperature parameter scalar.</span>
<span class="sd">    Returns:</span>
<span class="sd">        A 2D array of shape (batch_size, num_actions). Where each column is a probability distribution over</span>
<span class="sd">        the actions representing the policy.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Compute the preferences by dividing the action-values by the temperature parameter tau</span>
    <span class="n">preferences</span> <span class="o">=</span> <span class="n">action_values</span> <span class="o">/</span> <span class="n">tau</span>
    <span class="c1"># Compute the maximum preference across the actions</span>
    <span class="n">max_preference</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">preferences</span><span class="p">,</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># your code here</span>
    
    
    
    <span class="c1"># Reshape max_preference array which has shape [Batch,] to [Batch, 1]. This allows NumPy broadcasting </span>
    <span class="c1"># when subtracting the maximum preference from the preference of each action.</span>
    <span class="n">reshaped_max_preference</span> <span class="o">=</span> <span class="n">max_preference</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="c1"># Compute the numerator, i.e., the exponential of the preference - the max preference.</span>
    <span class="n">exp_preferences</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">preferences</span> <span class="o">-</span> <span class="n">reshaped_max_preference</span><span class="p">)</span>
    <span class="c1"># Compute the denominator, i.e., the sum over the numerator along the actions axis.</span>
    <span class="n">sum_of_exp_preferences</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_preferences</span><span class="p">,</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># your code here</span>
    
    
    
    <span class="c1"># Reshape sum_of_exp_preferences array which has shape [Batch,] to [Batch, 1] to  allow for NumPy broadcasting </span>
    <span class="c1"># when dividing the numerator by the denominator.</span>
    <span class="n">reshaped_sum_of_exp_preferences</span> <span class="o">=</span> <span class="n">sum_of_exp_preferences</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="c1"># Compute the action probabilities according to the equation in the previous cell.</span>
    <span class="n">action_probs</span> <span class="o">=</span> <span class="n">exp_preferences</span> <span class="o">/</span> <span class="n">reshaped_sum_of_exp_preferences</span>
    
    <span class="c1"># your code here</span>
    
    
    
    <span class="c1"># squeeze() removes any singleton dimensions. It is used here because this function is used in the </span>
    <span class="c1"># agent policy when selecting an action (for which the batch dimension is 1.) As np.random.choice is used in </span>
    <span class="c1"># the agent policy and it expects 1D arrays, we need to remove this singleton batch dimension.</span>
    <span class="n">action_probs</span> <span class="o">=</span> <span class="n">action_probs</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">action_probs</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Run the cell below to test your implementation of the <code>softmax()</code> function:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Debugging Cell</span>
<span class="c1"># --------------</span>
<span class="c1"># Feel free to make any changes to this cell to debug your code</span>

<span class="n">rand_generator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">action_values</span> <span class="o">=</span> <span class="n">rand_generator</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">tau</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="n">action_probs</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">action_values</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;action_probs&quot;</span><span class="p">,</span> <span class="n">action_probs</span><span class="p">)</span>

<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">action_probs</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.25849645</span><span class="p">,</span> <span class="mf">0.01689625</span><span class="p">,</span> <span class="mf">0.05374514</span><span class="p">,</span> <span class="mf">0.67086216</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.84699852</span><span class="p">,</span> <span class="mf">0.00286345</span><span class="p">,</span> <span class="mf">0.13520063</span><span class="p">,</span> <span class="mf">0.01493741</span><span class="p">]</span>
<span class="p">])))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>action_probs [[0.25849645 0.01689625 0.05374514 0.67086216]
 [0.84699852 0.00286345 0.13520063 0.01493741]]
Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Tested Cell</span>
<span class="c1"># -----------</span>
<span class="c1"># The contents of the cell will be tested by the autograder.</span>
<span class="c1"># If they do not pass here, they will not pass there.</span>

<span class="kn">from</span> <span class="nn">tests</span> <span class="kn">import</span> <span class="n">__true__softmax</span>

<span class="n">rand_generator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">action_values</span> <span class="o">=</span> <span class="n">rand_generator</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">rand_generator</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">rand_generator</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">softmax</span><span class="p">(</span><span class="n">action_values</span><span class="p">,</span> <span class="n">tau</span><span class="p">),</span> <span class="n">__true__softmax</span><span class="p">(</span><span class="n">action_values</span><span class="p">,</span> <span class="n">tau</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Expected output:</strong></p>

<pre><code>action_probs [[0.25849645 0.01689625 0.05374514 0.67086216]
 [0.84699852 0.00286345 0.13520063 0.01493741]]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Section-5:-Putting-the-pieces-together">Section 5: Putting the pieces together<a class="anchor-link" href="#Section-5:-Putting-the-pieces-together"> </a></h2><p>In this section, you will combine components from the previous sections to write up an RL-Glue Agent. The main component that you will implement is the action-value network updates with experience sampled from the experience replay buffer.</p>
<p>At time $t$, we have an action-value function represented as a neural network, say $Q_t$. We want to update our action-value function and get a new one we can use at the next timestep. We will get this $Q_{t+1}$ using multiple replay steps that each result in an intermediate action-value function $Q_{t+1}^{i}$ where $i$ indexes which replay step we are at.</p>
<p>In each replay step, we sample a batch of experiences from the replay buffer and compute a minibatch Expected-SARSA update. Across these N replay steps, we will use the current "un-updated" action-value network at time $t$, $Q_t$, for computing the action-values of the next-states. This contrasts using the most recent action-values from the last replay step $Q_{t+1}^{i}$. We make this choice to have targets that are stable across replay steps. Here is the pseudocode for performing the updates:</p>
$$
\begin{align}
&amp; Q_t \leftarrow \text{action-value network at timestep t (current action-value network)}\\
&amp; \text{Initialize } Q_{t+1}^1 \leftarrow Q_t\\
&amp; \text{For } i \text{ in } [1, ..., N] \text{ (i.e. N} \text{  replay steps)}:\\
&amp; \hspace{1cm} s, a, r, t, s'
\leftarrow \text{Sample batch of experiences from experience replay buffer} \\
&amp; \hspace{1cm} \text{Do Expected Sarsa update with } Q_t: Q_{t+1}^{i+1}(s, a) \leftarrow Q_{t+1}^{i}(s, a) + \alpha \cdot \left[r + \gamma \left(\sum_{b} \pi(b | s') Q_t(s', b)\right) - Q_{t+1}^{i}(s, a)\right]\\
&amp; \hspace{1.5cm} \text{ making sure to add the } \gamma \left(\sum_{b} \pi(b | s') Q_t(s', b)\right) \text{ for non-terminal transitions only.} \\
&amp; \text{After N replay steps, we set } Q_{t+1}^{N} \text{ as } Q_{t+1} \text{ and have a new } Q_{t+1} \text{for time step } t + 1 \text{ that we will fix in the next set of updates. }
\end{align}
$$<p>As you can see in the pseudocode, after sampling a batch of experiences, we do many computations. The basic idea however is that we are looking to compute a form of a TD error. In order to so, we can take the following steps:</p>
<ul>
<li>compute the action-values for the next states using the action-value network $Q_{t}$,</li>
<li>compute the policy $\pi(b | s')$ induced by the action-values $Q_{t}$ (using the softmax function you implemented before),</li>
<li>compute the Expected sarsa targets $r + \gamma \left(\sum_{b} \pi(b | s') Q_t(s', b)\right)$,</li>
<li>compute the action-values for the current states using the latest $Q_{t + 1}$, and,</li>
<li>compute the TD-errors with the Expected Sarsa targets.</li>
</ul>
<p>For the third step above, you can start by computing $\pi(b | s') Q_t(s', b)$ followed by summation to get $\hat{v}_\pi(s') = \left(\sum_{b} \pi(b | s') Q_t(s', b)\right)$. $\hat{v}_\pi(s')$ is an estimate of the value of the next state. Note for terminal next states, $\hat{v}_\pi(s') = 0$. Finally, we add the rewards to the discount times $\hat{v}_\pi(s')$.</p>
<p>You will implement these steps in the <code>get_td_error()</code> function below which given a batch of experiences (including states, next_states, actions, rewards, terminals), fixed action-value network (current_q), and action-value network (network), computes the TD error in the form of a 1D array of size batch_size.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_td_error</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">discount</span><span class="p">,</span> <span class="n">terminals</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">current_q</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        states (Numpy array): The batch of states with the shape (batch_size, state_dim).</span>
<span class="sd">        next_states (Numpy array): The batch of next states with the shape (batch_size, state_dim).</span>
<span class="sd">        actions (Numpy array): The batch of actions with the shape (batch_size,).</span>
<span class="sd">        rewards (Numpy array): The batch of rewards with the shape (batch_size,).</span>
<span class="sd">        discount (float): The discount factor.</span>
<span class="sd">        terminals (Numpy array): The batch of terminals with the shape (batch_size,).</span>
<span class="sd">        network (ActionValueNetwork): The latest state of the network that is getting replay updates.</span>
<span class="sd">        current_q (ActionValueNetwork): The fixed network used for computing the targets, </span>
<span class="sd">                                        and particularly, the action-values at the next-states.</span>
<span class="sd">    Returns:</span>
<span class="sd">        The TD errors (Numpy array) for actions taken, of shape (batch_size,)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Note: Here network is the latest state of the network that is getting replay updates. In other words, </span>
    <span class="c1"># the network represents Q_{t+1}^{i} whereas current_q represents Q_t, the fixed network used for computing the </span>
    <span class="c1"># targets, and particularly, the action-values at the next-states.</span>
    
    <span class="c1"># Compute action values at next states using current_q network</span>
    <span class="c1"># Note that q_next_mat is a 2D array of shape (batch_size, num_actions)</span>
    
    <span class="c1">### START CODE HERE (~1 Line)</span>
    <span class="n">q_next_mat</span> <span class="o">=</span> <span class="n">current_q</span><span class="o">.</span><span class="n">get_action_values</span><span class="p">(</span><span class="n">next_states</span><span class="p">)</span>
    <span class="c1">### END CODE HERE</span>
    <span class="c1"># your code here</span>
    
    
    <span class="c1"># Compute policy at next state by passing the action-values in q_next_mat to softmax()</span>
    <span class="c1"># Note that probs_mat is a 2D array of shape (batch_size, num_actions)</span>
    
    <span class="c1">### START CODE HERE (~1 Line)</span>
    <span class="n">probs_mat</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">q_next_mat</span><span class="p">,</span><span class="n">tau</span><span class="p">)</span>
    <span class="c1">### END CODE HERE</span>
    <span class="c1"># your code here</span>
    
    
    <span class="c1"># Compute the estimate of the next state value, v_next_vec.</span>
    <span class="c1"># Hint: sum the action-values for the next_states weighted by the policy, probs_mat. Then, multiply by</span>
    <span class="c1"># (1 - terminals) to make sure v_next_vec is zero for terminal next states.</span>
    <span class="c1"># Note that v_next_vec is a 1D array of shape (batch_size,)</span>
    
    <span class="c1">### START CODE HERE (~3 Lines)</span>
    <span class="n">v_next_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">q_next_mat</span> <span class="o">*</span> <span class="n">probs_mat</span> <span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">terminals</span><span class="p">)</span>
    <span class="c1">### END CODE HERE</span>
    <span class="c1"># your code here</span>
    
    
    <span class="c1"># Compute Expected Sarsa target</span>
    <span class="c1"># Note that target_vec is a 1D array of shape (batch_size,)</span>
    
    <span class="c1">### START CODE HERE (~1 Line)</span>
    <span class="n">target_vec</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">+</span> <span class="n">discount</span> <span class="o">*</span> <span class="n">v_next_vec</span>
    <span class="c1">### END CODE HERE</span>
    <span class="c1"># your code here</span>
    
    
    <span class="c1"># Compute action values at the current states for all actions using network</span>
    <span class="c1"># Note that q_mat is a 2D array of shape (batch_size, num_actions)</span>
    
    <span class="c1">### START CODE HERE (~1 Line)</span>
    <span class="n">q_mat</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">get_action_values</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
    <span class="c1">### END CODE HERE</span>
    <span class="c1"># your code here</span>
    
    
    <span class="c1"># Batch Indices is an array from 0 to the batch size - 1. </span>
    <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">q_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># Compute q_vec by selecting q(s, a) from q_mat for taken actions</span>
    <span class="c1"># Use batch_indices as the index for the first dimension of q_mat</span>
    <span class="c1"># Note that q_vec is a 1D array of shape (batch_size)</span>
    
    <span class="c1">### START CODE HERE (~1 Line)</span>
    <span class="n">q_vec</span> <span class="o">=</span> <span class="n">q_mat</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">,</span><span class="n">actions</span><span class="p">]</span>
    <span class="c1">### END CODE HERE</span>
    <span class="c1"># your code here</span>
    
    
    <span class="c1"># Compute TD errors for actions taken</span>
    <span class="c1"># Note that delta_vec is a 1D array of shape (batch_size)</span>
    
    <span class="c1">### START CODE HERE (~1 Line)</span>
    <span class="n">delta_vec</span> <span class="o">=</span> <span class="n">target_vec</span> <span class="o">-</span> <span class="n">q_vec</span>
    <span class="c1">### END CODE HERE</span>
    <span class="c1"># your code here</span>
    
    
    <span class="k">return</span> <span class="n">delta_vec</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Run the following code to test your implementation of the <code>get_td_error()</code> function:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Debugging Cell</span>
<span class="c1"># --------------</span>
<span class="c1"># Feel free to make any changes to this cell to debug your code</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;asserts/get_td_error_1.npz&quot;</span><span class="p">,</span> <span class="n">allow_pickle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">states</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;states&quot;</span><span class="p">]</span>
<span class="n">next_states</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;next_states&quot;</span><span class="p">]</span>
<span class="n">actions</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;actions&quot;</span><span class="p">]</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">]</span>
<span class="n">discount</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;discount&quot;</span><span class="p">]</span>
<span class="n">terminals</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;terminals&quot;</span><span class="p">]</span>
<span class="n">tau</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">network_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;state_dim&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
                  <span class="s2">&quot;num_hidden_units&quot;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
                  <span class="s2">&quot;num_actions&quot;</span><span class="p">:</span> <span class="mi">4</span>
                  <span class="p">}</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">ActionValueNetwork</span><span class="p">(</span><span class="n">network_config</span><span class="p">)</span>
<span class="n">network</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;network_weights&quot;</span><span class="p">])</span>

<span class="n">current_q</span> <span class="o">=</span> <span class="n">ActionValueNetwork</span><span class="p">(</span><span class="n">network_config</span><span class="p">)</span>
<span class="n">current_q</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;current_q_weights&quot;</span><span class="p">])</span>

<span class="n">delta_vec</span> <span class="o">=</span> <span class="n">get_td_error</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">discount</span><span class="p">,</span> <span class="n">terminals</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">current_q</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
<span class="n">answer_delta_vec</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;delta_vec&quot;</span><span class="p">]</span>

<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">delta_vec</span><span class="p">,</span> <span class="n">answer_delta_vec</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Tested Cell</span>
<span class="c1"># -----------</span>
<span class="c1"># The contents of the cell will be tested by the autograder.</span>
<span class="c1"># If they do not pass here, they will not pass there.</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;asserts/get_td_error_1.npz&quot;</span><span class="p">,</span> <span class="n">allow_pickle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">states</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;states&quot;</span><span class="p">]</span>
<span class="n">next_states</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;next_states&quot;</span><span class="p">]</span>
<span class="n">actions</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;actions&quot;</span><span class="p">]</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">]</span>
<span class="n">discount</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;discount&quot;</span><span class="p">]</span>
<span class="n">terminals</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;terminals&quot;</span><span class="p">]</span>
<span class="n">tau</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">network_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;state_dim&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
                  <span class="s2">&quot;num_hidden_units&quot;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
                  <span class="s2">&quot;num_actions&quot;</span><span class="p">:</span> <span class="mi">4</span>
                  <span class="p">}</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">ActionValueNetwork</span><span class="p">(</span><span class="n">network_config</span><span class="p">)</span>
<span class="n">network</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;network_weights&quot;</span><span class="p">])</span>

<span class="n">current_q</span> <span class="o">=</span> <span class="n">ActionValueNetwork</span><span class="p">(</span><span class="n">network_config</span><span class="p">)</span>
<span class="n">current_q</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;current_q_weights&quot;</span><span class="p">])</span>

<span class="n">delta_vec</span> <span class="o">=</span> <span class="n">get_td_error</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">discount</span><span class="p">,</span> <span class="n">terminals</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">current_q</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
<span class="n">answer_delta_vec</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;delta_vec&quot;</span><span class="p">]</span>

<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">delta_vec</span><span class="p">,</span> <span class="n">answer_delta_vec</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that you implemented the <code>get_td_error()</code> function, you can use it to implement the <code>optimize_network()</code> function. In this function, you will:</p>
<ul>
<li>get the TD-errors vector from <code>get_td_error()</code>,</li>
<li>make the TD-errors into a matrix using zeroes for actions not taken in the transitions,</li>
<li>pass the TD-errors matrix to the <code>get_TD_update()</code> function of network to calculate the gradients times TD errors, and,</li>
<li>perform an ADAM optimizer step.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Graded Cell</span>
<span class="c1"># -----------</span>

<span class="c1">### Work Required: Yes. Fill in code in optimize_network (~2 Lines).</span>
<span class="k">def</span> <span class="nf">optimize_network</span><span class="p">(</span><span class="n">experiences</span><span class="p">,</span> <span class="n">discount</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">current_q</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        experiences (Numpy array): The batch of experiences including the states, actions, </span>
<span class="sd">                                   rewards, terminals, and next_states.</span>
<span class="sd">        discount (float): The discount factor.</span>
<span class="sd">        network (ActionValueNetwork): The latest state of the network that is getting replay updates.</span>
<span class="sd">        current_q (ActionValueNetwork): The fixed network used for computing the targets, </span>
<span class="sd">                                        and particularly, the action-values at the next-states.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Get states, action, rewards, terminals, and next_states from experiences</span>
    <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">terminals</span><span class="p">,</span> <span class="n">next_states</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">experiences</span><span class="p">))</span>
    <span class="n">states</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
    <span class="n">next_states</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">next_states</span><span class="p">)</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
    <span class="n">terminals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">terminals</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Compute TD error using the get_td_error function</span>
    <span class="c1"># Note that q_vec is a 1D array of shape (batch_size)</span>
    <span class="n">delta_vec</span> <span class="o">=</span> <span class="n">get_td_error</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">discount</span><span class="p">,</span> <span class="n">terminals</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">current_q</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>

    <span class="c1"># Batch Indices is an array from 0 to the batch_size - 1. </span>
    <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>

    <span class="c1"># Make a td error matrix of shape (batch_size, num_actions)</span>
    <span class="c1"># delta_mat has non-zero value only for actions taken</span>
    <span class="n">delta_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">network</span><span class="o">.</span><span class="n">num_actions</span><span class="p">))</span>
    <span class="n">delta_mat</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">,</span> <span class="n">actions</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_vec</span>

    <span class="c1"># Pass delta_mat to compute the TD errors times the gradients of the network&#39;s weights from back-propagation</span>
    
    <span class="c1">### START CODE HERE</span>
    <span class="n">td_update</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">get_TD_update</span><span class="p">(</span><span class="n">states</span><span class="p">,</span><span class="n">delta_mat</span><span class="p">)</span>
    <span class="c1">### END CODE HERE</span>
    <span class="c1"># your code here</span>
    
    
    <span class="c1"># Pass network.get_weights and the td_update to the optimizer to get updated weights</span>
    <span class="c1">### START CODE HERE</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">update_weights</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">get_weights</span><span class="p">(),</span> <span class="n">td_update</span><span class="p">)</span>
    <span class="c1">### END CODE HERE</span>
    <span class="c1"># your code here</span>
    
    
    <span class="n">network</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Run the following code to test your implementation of the <code>optimize_network()</code> function:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Tested Cell</span>
<span class="c1"># -----------</span>
<span class="c1"># The contents of the cell will be tested by the autograder.</span>
<span class="c1"># If they do not pass here, they will not pass there.</span>

<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;asserts/optimize_network_input_1.npz&quot;</span><span class="p">,</span> <span class="n">allow_pickle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">experiences</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;experiences&quot;</span><span class="p">])</span>
<span class="n">discount</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;discount&quot;</span><span class="p">]</span>
<span class="n">tau</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">network_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;state_dim&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
                  <span class="s2">&quot;num_hidden_units&quot;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
                  <span class="s2">&quot;num_actions&quot;</span><span class="p">:</span> <span class="mi">4</span>
                  <span class="p">}</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">ActionValueNetwork</span><span class="p">(</span><span class="n">network_config</span><span class="p">)</span>
<span class="n">network</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;network_weights&quot;</span><span class="p">])</span>

<span class="n">current_q</span> <span class="o">=</span> <span class="n">ActionValueNetwork</span><span class="p">(</span><span class="n">network_config</span><span class="p">)</span>
<span class="n">current_q</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;current_q_weights&quot;</span><span class="p">])</span>

<span class="n">optimizer_config</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;step_size&#39;</span><span class="p">:</span> <span class="mf">3e-5</span><span class="p">,</span> 
                    <span class="s1">&#39;beta_m&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span> 
                    <span class="s1">&#39;beta_v&#39;</span><span class="p">:</span> <span class="mf">0.999</span><span class="p">,</span>
                    <span class="s1">&#39;epsilon&#39;</span><span class="p">:</span> <span class="mf">1e-8</span>
                   <span class="p">}</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">,</span> <span class="n">optimizer_config</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;optimizer_m&quot;</span><span class="p">]</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;optimizer_v&quot;</span><span class="p">]</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">beta_m_product</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;optimizer_beta_m_product&quot;</span><span class="p">]</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">beta_v_product</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;optimizer_beta_v_product&quot;</span><span class="p">]</span>

<span class="n">optimize_network</span><span class="p">(</span><span class="n">experiences</span><span class="p">,</span> <span class="n">discount</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">current_q</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
<span class="n">updated_weights</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>

<span class="n">output_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;asserts/optimize_network_output_1.npz&quot;</span><span class="p">,</span> <span class="n">allow_pickle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">answer_updated_weights</span> <span class="o">=</span> <span class="n">output_data</span><span class="p">[</span><span class="s2">&quot;updated_weights&quot;</span><span class="p">]</span>

<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">updated_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">],</span> <span class="n">answer_updated_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]))</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">updated_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">],</span> <span class="n">answer_updated_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]))</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">updated_weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">],</span> <span class="n">answer_updated_weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]))</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">updated_weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">],</span> <span class="n">answer_updated_weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that you implemented the <code>optimize_network()</code> function, you can implement the agent. In the cell below, you will fill the <code>agent_step()</code> and <code>agent_end()</code> functions. You should:</p>
<ul>
<li>select an action (only in <code>agent_step()</code>),</li>
<li>add transitions (consisting of the state, action, reward, terminal, and next state) to the replay buffer, and,</li>
<li>update the weights of the neural network by doing multiple replay steps and calling the <code>optimize_network()</code> function that you implemented above.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Graded Cell</span>
<span class="c1"># -----------</span>

<span class="c1">### Work Required: Yes. Fill in code in agent_step and agent_end (~7 Lines).</span>
<span class="k">class</span> <span class="nc">Agent</span><span class="p">(</span><span class="n">BaseAgent</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;expected_sarsa_agent&quot;</span>
        
    <span class="c1"># Work Required: No.</span>
    <span class="k">def</span> <span class="nf">agent_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">agent_config</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Setup for the agent called when the experiment first starts.</span>

<span class="sd">        Set parameters needed to setup the agent.</span>

<span class="sd">        Assume agent_config dict contains:</span>
<span class="sd">        {</span>
<span class="sd">            network_config: dictionary,</span>
<span class="sd">            optimizer_config: dictionary,</span>
<span class="sd">            replay_buffer_size: integer,</span>
<span class="sd">            minibatch_sz: integer, </span>
<span class="sd">            num_replay_updates_per_step: float</span>
<span class="sd">            discount_factor: float,</span>
<span class="sd">        }</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span><span class="n">agent_config</span><span class="p">[</span><span class="s1">&#39;replay_buffer_size&#39;</span><span class="p">],</span> 
                                          <span class="n">agent_config</span><span class="p">[</span><span class="s1">&#39;minibatch_sz&#39;</span><span class="p">],</span> <span class="n">agent_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;seed&quot;</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">ActionValueNetwork</span><span class="p">(</span><span class="n">agent_config</span><span class="p">[</span><span class="s1">&#39;network_config&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">layer_sizes</span><span class="p">,</span> <span class="n">agent_config</span><span class="p">[</span><span class="s2">&quot;optimizer_config&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span> <span class="o">=</span> <span class="n">agent_config</span><span class="p">[</span><span class="s1">&#39;network_config&#39;</span><span class="p">][</span><span class="s1">&#39;num_actions&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_replay</span> <span class="o">=</span> <span class="n">agent_config</span><span class="p">[</span><span class="s1">&#39;num_replay_updates_per_step&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discount</span> <span class="o">=</span> <span class="n">agent_config</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">agent_config</span><span class="p">[</span><span class="s1">&#39;tau&#39;</span><span class="p">]</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">rand_generator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">agent_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;seed&quot;</span><span class="p">))</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">last_state</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_action</span> <span class="o">=</span> <span class="kc">None</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">sum_rewards</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">episode_steps</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Work Required: No.</span>
    <span class="k">def</span> <span class="nf">policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            state (Numpy array): the state.</span>
<span class="sd">        Returns:</span>
<span class="sd">            the action. </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">action_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">get_action_values</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">probs_batch</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">action_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rand_generator</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">probs_batch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">action</span>

    <span class="c1"># Work Required: No.</span>
    <span class="k">def</span> <span class="nf">agent_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The first method called when the experiment starts, called after</span>
<span class="sd">        the environment starts.</span>
<span class="sd">        Args:</span>
<span class="sd">            state (Numpy array): the state from the</span>
<span class="sd">                environment&#39;s evn_start function.</span>
<span class="sd">        Returns:</span>
<span class="sd">            The first action the agent takes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sum_rewards</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">episode_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">state</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_state</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_action</span>

    <span class="c1"># Work Required: Yes. Fill in the action selection, replay-buffer update, </span>
    <span class="c1"># weights update using optimize_network, and updating last_state and last_action (~5 lines).</span>
    <span class="k">def</span> <span class="nf">agent_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;A step taken by the agent.</span>
<span class="sd">        Args:</span>
<span class="sd">            reward (float): the reward received for taking the last action taken</span>
<span class="sd">            state (Numpy array): the state from the</span>
<span class="sd">                environment&#39;s step based, where the agent ended up after the</span>
<span class="sd">                last step</span>
<span class="sd">        Returns:</span>
<span class="sd">            The action the agent is taking.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">sum_rewards</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">episode_steps</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Make state an array of shape (1, state_dim) to add a batch dimension and</span>
        <span class="c1"># to later match the get_action_values() and get_TD_update() functions</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">state</span><span class="p">])</span>

        <span class="c1"># Select action</span>
        <span class="c1"># your code here</span>
        <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        
        <span class="c1"># Append new experience to replay buffer</span>
        <span class="c1"># Note: look at the replay_buffer append function for the order of arguments</span>

        <span class="c1"># your code here</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_state</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">last_action</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">state</span><span class="p">)</span>
        
        <span class="c1"># Perform replay steps:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">minibatch_size</span><span class="p">:</span>
            <span class="n">current_q</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_replay</span><span class="p">):</span>
                
                <span class="c1"># Get sample experiences from the replay buffer</span>
                <span class="n">experiences</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
                
                <span class="c1"># Call optimize_network to update the weights of the network (~1 Line)</span>
                <span class="c1"># your code here</span>
                <span class="n">optimize_network</span><span class="p">(</span><span class="n">experiences</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">discount</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span><span class="n">current_q</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">)</span>
                
        <span class="c1"># Update the last state and last action.</span>
        <span class="c1">### START CODE HERE (~2 Lines)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_state</span> <span class="o">=</span> <span class="n">state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_action</span> <span class="o">=</span> <span class="n">action</span>
        <span class="c1">### END CODE HERE</span>
        <span class="c1"># your code here</span>
        
        
        <span class="k">return</span> <span class="n">action</span>

    <span class="c1"># Work Required: Yes. Fill in the replay-buffer update and</span>
    <span class="c1"># update of the weights using optimize_network (~2 lines).</span>
    <span class="k">def</span> <span class="nf">agent_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run when the agent terminates.</span>
<span class="sd">        Args:</span>
<span class="sd">            reward (float): the reward the agent received for entering the</span>
<span class="sd">                terminal state.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sum_rewards</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">episode_steps</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="c1"># Set terminal state to an array of zeros</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_state</span><span class="p">)</span>

        <span class="c1"># Append new experience to replay buffer</span>
        <span class="c1"># Note: look at the replay_buffer append function for the order of arguments</span>
        
        <span class="c1"># your code here</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_state</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">last_action</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">state</span><span class="p">)</span>
        
        <span class="c1"># Perform replay steps:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">minibatch_size</span><span class="p">:</span>
            <span class="n">current_q</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_replay</span><span class="p">):</span>
                
                <span class="c1"># Get sample experiences from the replay buffer</span>
                <span class="n">experiences</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
                
                <span class="c1"># Call optimize_network to update the weights of the network</span>
                <span class="c1"># your code here</span>
                <span class="n">optimize_network</span><span class="p">(</span><span class="n">experiences</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">discount</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span><span class="n">current_q</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">)</span>
                
                
        
    <span class="k">def</span> <span class="nf">agent_message</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">message</span> <span class="o">==</span> <span class="s2">&quot;get_sum_reward&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sum_rewards</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Unrecognized Message!&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Run the following code to test your implementation of the <code>agent_step()</code> function:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Tested Cell</span>
<span class="c1"># -----------</span>
<span class="c1"># The contents of the cell will be tested by the autograder.</span>
<span class="c1"># If they do not pass here, they will not pass there.</span>

<span class="n">agent_info</span> <span class="o">=</span> <span class="p">{</span>
             <span class="s1">&#39;network_config&#39;</span><span class="p">:</span> <span class="p">{</span>
                 <span class="s1">&#39;state_dim&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
                 <span class="s1">&#39;num_hidden_units&#39;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
                 <span class="s1">&#39;num_hidden_layers&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="s1">&#39;num_actions&#39;</span><span class="p">:</span> <span class="mi">4</span>
             <span class="p">},</span>
             <span class="s1">&#39;optimizer_config&#39;</span><span class="p">:</span> <span class="p">{</span>
                 <span class="s1">&#39;step_size&#39;</span><span class="p">:</span> <span class="mf">3e-5</span><span class="p">,</span> 
                 <span class="s1">&#39;beta_m&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span> 
                 <span class="s1">&#39;beta_v&#39;</span><span class="p">:</span> <span class="mf">0.999</span><span class="p">,</span>
                 <span class="s1">&#39;epsilon&#39;</span><span class="p">:</span> <span class="mf">1e-8</span>
             <span class="p">},</span>
             <span class="s1">&#39;replay_buffer_size&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
             <span class="s1">&#39;minibatch_sz&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
             <span class="s1">&#39;num_replay_updates_per_step&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
             <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
             <span class="s1">&#39;tau&#39;</span><span class="p">:</span> <span class="mf">1000.0</span><span class="p">,</span>
             <span class="s1">&#39;seed&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>

<span class="c1"># Initialize agent</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">()</span>
<span class="n">agent</span><span class="o">.</span><span class="n">agent_init</span><span class="p">(</span><span class="n">agent_info</span><span class="p">)</span>

<span class="c1"># load agent network, optimizer, replay_buffer from the agent_input_1.npz file</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;asserts/agent_input_1.npz&quot;</span><span class="p">,</span> <span class="n">allow_pickle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">agent</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;network_weights&quot;</span><span class="p">])</span>
<span class="n">agent</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;optimizer_m&quot;</span><span class="p">]</span>
<span class="n">agent</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;optimizer_v&quot;</span><span class="p">]</span>
<span class="n">agent</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">beta_m_product</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;optimizer_beta_m_product&quot;</span><span class="p">]</span>
<span class="n">agent</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">beta_v_product</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;optimizer_beta_v_product&quot;</span><span class="p">]</span>
<span class="n">agent</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">rand_generator</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;replay_buffer_seed&quot;</span><span class="p">]))</span>
<span class="k">for</span> <span class="n">experience</span> <span class="ow">in</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;replay_buffer&quot;</span><span class="p">]:</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span>

<span class="c1"># Perform agent_step multiple times</span>
<span class="n">last_state_array</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;last_state_array&quot;</span><span class="p">]</span>
<span class="n">last_action_array</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;last_action_array&quot;</span><span class="p">]</span>
<span class="n">state_array</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;state_array&quot;</span><span class="p">]</span>
<span class="n">reward_array</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;reward_array&quot;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">last_state</span> <span class="o">=</span> <span class="n">last_state_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">last_action</span> <span class="o">=</span> <span class="n">last_action_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">state_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">reward_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    
    <span class="n">agent</span><span class="o">.</span><span class="n">agent_step</span><span class="p">(</span><span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
    
    <span class="c1"># Load expected values for last_state, last_action, weights, and replay_buffer </span>
    <span class="n">output_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;asserts/agent_step_output_</span><span class="si">{}</span><span class="s2">.npz&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">allow_pickle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">answer_last_state</span> <span class="o">=</span> <span class="n">output_data</span><span class="p">[</span><span class="s2">&quot;last_state&quot;</span><span class="p">]</span>
    <span class="n">answer_last_action</span> <span class="o">=</span> <span class="n">output_data</span><span class="p">[</span><span class="s2">&quot;last_action&quot;</span><span class="p">]</span>
    <span class="n">answer_updated_weights</span> <span class="o">=</span> <span class="n">output_data</span><span class="p">[</span><span class="s2">&quot;updated_weights&quot;</span><span class="p">]</span>
    <span class="n">answer_replay_buffer</span> <span class="o">=</span> <span class="n">output_data</span><span class="p">[</span><span class="s2">&quot;replay_buffer&quot;</span><span class="p">]</span>

    <span class="c1"># Asserts for last_state and last_action</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">answer_last_state</span><span class="p">,</span> <span class="n">agent</span><span class="o">.</span><span class="n">last_state</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">answer_last_action</span><span class="p">,</span> <span class="n">agent</span><span class="o">.</span><span class="n">last_action</span><span class="p">))</span>

    <span class="c1"># Asserts for replay_buffer </span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">answer_replay_buffer</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">answer_replay_buffer</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">buffer</span><span class="p">)[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">answer_replay_buffer</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]))</span>

    <span class="c1"># Asserts for network.weights</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">],</span> <span class="n">answer_updated_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">],</span> <span class="n">answer_updated_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">],</span> <span class="n">answer_updated_weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">],</span> <span class="n">answer_updated_weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Run the following code to test your implementation of the <code>agent_end()</code> function:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Tested Cell</span>
<span class="c1"># -----------</span>
<span class="c1"># The contents of the cell will be tested by the autograder.</span>
<span class="c1"># If they do not pass here, they will not pass there.</span>

<span class="n">agent_info</span> <span class="o">=</span> <span class="p">{</span>
             <span class="s1">&#39;network_config&#39;</span><span class="p">:</span> <span class="p">{</span>
                 <span class="s1">&#39;state_dim&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
                 <span class="s1">&#39;num_hidden_units&#39;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
                 <span class="s1">&#39;num_hidden_layers&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="s1">&#39;num_actions&#39;</span><span class="p">:</span> <span class="mi">4</span>
             <span class="p">},</span>
             <span class="s1">&#39;optimizer_config&#39;</span><span class="p">:</span> <span class="p">{</span>
                 <span class="s1">&#39;step_size&#39;</span><span class="p">:</span> <span class="mf">3e-5</span><span class="p">,</span> 
                 <span class="s1">&#39;beta_m&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span> 
                 <span class="s1">&#39;beta_v&#39;</span><span class="p">:</span> <span class="mf">0.999</span><span class="p">,</span>
                 <span class="s1">&#39;epsilon&#39;</span><span class="p">:</span> <span class="mf">1e-8</span>
             <span class="p">},</span>
             <span class="s1">&#39;replay_buffer_size&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
             <span class="s1">&#39;minibatch_sz&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
             <span class="s1">&#39;num_replay_updates_per_step&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
             <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
             <span class="s1">&#39;tau&#39;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
             <span class="s1">&#39;seed&#39;</span><span class="p">:</span> <span class="mi">0</span>
             <span class="p">}</span>

<span class="c1"># Initialize agent</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">()</span>
<span class="n">agent</span><span class="o">.</span><span class="n">agent_init</span><span class="p">(</span><span class="n">agent_info</span><span class="p">)</span>

<span class="c1"># load agent network, optimizer, replay_buffer from the agent_input_1.npz file</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;asserts/agent_input_1.npz&quot;</span><span class="p">,</span> <span class="n">allow_pickle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">agent</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;network_weights&quot;</span><span class="p">])</span>
<span class="n">agent</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;optimizer_m&quot;</span><span class="p">]</span>
<span class="n">agent</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;optimizer_v&quot;</span><span class="p">]</span>
<span class="n">agent</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">beta_m_product</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;optimizer_beta_m_product&quot;</span><span class="p">]</span>
<span class="n">agent</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">beta_v_product</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;optimizer_beta_v_product&quot;</span><span class="p">]</span>
<span class="n">agent</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">rand_generator</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;replay_buffer_seed&quot;</span><span class="p">]))</span>
<span class="k">for</span> <span class="n">experience</span> <span class="ow">in</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;replay_buffer&quot;</span><span class="p">]:</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span>

<span class="c1"># Perform agent_step multiple times</span>
<span class="n">last_state_array</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;last_state_array&quot;</span><span class="p">]</span>
<span class="n">last_action_array</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;last_action_array&quot;</span><span class="p">]</span>
<span class="n">state_array</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;state_array&quot;</span><span class="p">]</span>
<span class="n">reward_array</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="s2">&quot;reward_array&quot;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">last_state</span> <span class="o">=</span> <span class="n">last_state_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">last_action</span> <span class="o">=</span> <span class="n">last_action_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">reward_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    
    <span class="n">agent</span><span class="o">.</span><span class="n">agent_end</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

    <span class="c1"># Load expected values for last_state, last_action, weights, and replay_buffer </span>
    <span class="n">output_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;asserts/agent_end_output_</span><span class="si">{}</span><span class="s2">.npz&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">allow_pickle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">answer_updated_weights</span> <span class="o">=</span> <span class="n">output_data</span><span class="p">[</span><span class="s2">&quot;updated_weights&quot;</span><span class="p">]</span>
    <span class="n">answer_replay_buffer</span> <span class="o">=</span> <span class="n">output_data</span><span class="p">[</span><span class="s2">&quot;replay_buffer&quot;</span><span class="p">]</span>

    <span class="c1"># Asserts for replay_buffer </span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">answer_replay_buffer</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">answer_replay_buffer</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">buffer</span><span class="p">)[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">answer_replay_buffer</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]))</span>

    <span class="c1"># Asserts for network.weights</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">],</span> <span class="n">answer_updated_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">],</span> <span class="n">answer_updated_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">],</span> <span class="n">answer_updated_weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;W&quot;</span><span class="p">]))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">],</span> <span class="n">answer_updated_weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Section-6:-Run-Experiment">Section 6: Run Experiment<a class="anchor-link" href="#Section-6:-Run-Experiment"> </a></h2><p>Now that you implemented the agent, we can use it to run an experiment on the Lunar Lander problem. We will plot the learning curve of the agent to visualize learning progress. To plot the learning curve, we use the sum of rewards in an episode as the performance measure. We have provided for you the experiment/plot code in the cell below which you can go ahead and run. Note that running the cell below has taken approximately 10 minutes in prior testing.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Discussion Cell</span>
<span class="c1"># ---------------</span>

<span class="k">def</span> <span class="nf">run_experiment</span><span class="p">(</span><span class="n">environment</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">environment_parameters</span><span class="p">,</span> <span class="n">agent_parameters</span><span class="p">,</span> <span class="n">experiment_parameters</span><span class="p">):</span>
    
    <span class="n">rl_glue</span> <span class="o">=</span> <span class="n">RLGlue</span><span class="p">(</span><span class="n">environment</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
        
    <span class="c1"># save sum of reward at the end of each episode</span>
    <span class="n">agent_sum_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">experiment_parameters</span><span class="p">[</span><span class="s2">&quot;num_runs&quot;</span><span class="p">],</span> 
                                 <span class="n">experiment_parameters</span><span class="p">[</span><span class="s2">&quot;num_episodes&quot;</span><span class="p">]))</span>

    <span class="n">env_info</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">agent_info</span> <span class="o">=</span> <span class="n">agent_parameters</span>

    <span class="c1"># one agent setting</span>
    <span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">experiment_parameters</span><span class="p">[</span><span class="s2">&quot;num_runs&quot;</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">agent_info</span><span class="p">[</span><span class="s2">&quot;seed&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">run</span>
        <span class="n">agent_info</span><span class="p">[</span><span class="s2">&quot;network_config&quot;</span><span class="p">][</span><span class="s2">&quot;seed&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">run</span>
        <span class="n">env_info</span><span class="p">[</span><span class="s2">&quot;seed&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">run</span>

        <span class="n">rl_glue</span><span class="o">.</span><span class="n">rl_init</span><span class="p">(</span><span class="n">agent_info</span><span class="p">,</span> <span class="n">env_info</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">experiment_parameters</span><span class="p">[</span><span class="s2">&quot;num_episodes&quot;</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">)):</span>
            <span class="c1"># run episode</span>
            <span class="n">rl_glue</span><span class="o">.</span><span class="n">rl_episode</span><span class="p">(</span><span class="n">experiment_parameters</span><span class="p">[</span><span class="s2">&quot;timeout&quot;</span><span class="p">])</span>
            
            <span class="n">episode_reward</span> <span class="o">=</span> <span class="n">rl_glue</span><span class="o">.</span><span class="n">rl_agent_message</span><span class="p">(</span><span class="s2">&quot;get_sum_reward&quot;</span><span class="p">)</span>
            <span class="n">agent_sum_reward</span><span class="p">[</span><span class="n">run</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">episode</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">episode_reward</span>
    <span class="n">save_name</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rl_glue</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;results&#39;</span><span class="p">):</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s1">&#39;results&#39;</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;results/sum_reward_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">save_name</span><span class="p">),</span> <span class="n">agent_sum_reward</span><span class="p">)</span>
    <span class="n">shutil</span><span class="o">.</span><span class="n">make_archive</span><span class="p">(</span><span class="s1">&#39;results&#39;</span><span class="p">,</span> <span class="s1">&#39;zip&#39;</span><span class="p">,</span> <span class="s1">&#39;results&#39;</span><span class="p">)</span>

<span class="c1"># Run Experiment</span>

<span class="c1"># Experiment parameters</span>
<span class="n">experiment_parameters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;num_runs&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;num_episodes&quot;</span> <span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="c1"># OpenAI Gym environments allow for a timestep limit timeout, causing episodes to end after </span>
    <span class="c1"># some number of timesteps. Here we use the default of 500.</span>
    <span class="s2">&quot;timeout&quot;</span> <span class="p">:</span> <span class="mi">500</span>
<span class="p">}</span>

<span class="c1"># Environment parameters</span>
<span class="n">environment_parameters</span> <span class="o">=</span> <span class="p">{}</span>

<span class="n">current_env</span> <span class="o">=</span> <span class="n">LunarLanderEnvironment</span>

<span class="c1"># Agent parameters</span>
<span class="n">agent_parameters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;network_config&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;state_dim&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
        <span class="s1">&#39;num_hidden_units&#39;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
        <span class="s1">&#39;num_actions&#39;</span><span class="p">:</span> <span class="mi">4</span>
    <span class="p">},</span>
    <span class="s1">&#39;optimizer_config&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;step_size&#39;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span>
        <span class="s1">&#39;beta_m&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span> 
        <span class="s1">&#39;beta_v&#39;</span><span class="p">:</span> <span class="mf">0.999</span><span class="p">,</span>
        <span class="s1">&#39;epsilon&#39;</span><span class="p">:</span> <span class="mf">1e-8</span>
    <span class="p">},</span>
    <span class="s1">&#39;replay_buffer_size&#39;</span><span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>
    <span class="s1">&#39;minibatch_sz&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="s1">&#39;num_replay_updates_per_step&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
    <span class="s1">&#39;tau&#39;</span><span class="p">:</span> <span class="mf">0.001</span>
<span class="p">}</span>
<span class="n">current_agent</span> <span class="o">=</span> <span class="n">Agent</span>

<span class="c1"># run experiment</span>
<span class="n">run_experiment</span><span class="p">(</span><span class="n">current_env</span><span class="p">,</span> <span class="n">current_agent</span><span class="p">,</span> <span class="n">environment_parameters</span><span class="p">,</span> <span class="n">agent_parameters</span><span class="p">,</span> <span class="n">experiment_parameters</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [07:05&lt;00:00,  1.42s/it]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Run the cell below to see the comparison between the agent that you implemented and a random agent for the one run and 300 episodes. Note that the <code>plot_result()</code> function smoothes the learning curve by applying a sliding window on the performance measure.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plot_result</span><span class="p">([</span><span class="s2">&quot;expected_sarsa_agent&quot;</span><span class="p">,</span> <span class="s2">&quot;random_agent&quot;</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the following cell you can visualize the performance of the agent with a correct implementation. As you can see, the agent initially crashes quite quickly (Episode 0). Then, the agent learns to avoid crashing by expending fuel and staying far above the ground. Finally however, it learns to land smoothly within the landing zone demarcated by the two flags (Episode 275).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%HTML</span>
<span class="p">&lt;</span><span class="nt">div</span> <span class="na">align</span><span class="o">=</span><span class="s">&quot;middle&quot;</span><span class="p">&gt;</span>
<span class="p">&lt;</span><span class="nt">video</span> <span class="na">width</span><span class="o">=</span><span class="s">&quot;80%&quot;</span> <span class="na">controls</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">source</span> <span class="na">src</span><span class="o">=</span><span class="s">&quot;ImplementYourAgent.mp4&quot;</span> <span class="na">type</span><span class="o">=</span><span class="s">&quot;video/mp4&quot;</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">video</span><span class="p">&gt;&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div align="middle">
<video width="80%" controls="">
      <source src="ImplementYourAgent.mp4" type="video/mp4" />
</video></div>

</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the learning curve above, you can see that sum of reward over episode has quite a high-variance at the beginning. However, the performance seems to be improving. The experiment that you ran was for 300 episodes and 1 run. To understand how the agent performs in the long run, we provide below the learning curve for the agent trained for 3000 episodes with performance averaged over 30 runs.
<figure>
  
    <img class="docimage" src="/fastblog/images/copied_from_nb/3000_episodes.png" alt="Drawing" />
    
    
</figure>

You can see that the agent learns a reasonably good policy within 3000 episodes, gaining sum of reward bigger than 200. Note that because of the high-variance in the agent performance, we also smoothed the learning curve.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Wrapping-up!">Wrapping up!<a class="anchor-link" href="#Wrapping-up!"> </a></h3><p>You have successfully implemented Course 4 Programming Assignment 2.</p>
<p>You have implemented an <strong>Expected Sarsa agent with a neural network and the Adam optimizer</strong> and used it for solving the Lunar Lander problem! You implemented different components of the agent including:</p>
<ul>
<li>a neural network for function approximation,</li>
<li>the Adam algorithm for optimizing the weights of the neural network,</li>
<li>a Softmax policy,</li>
<li>the replay steps for updating the action-value function using the experiences sampled from a replay buffer</li>
</ul>
<p>You tested the agent for a single parameter setting. In the next assignment, you will perform a parameter study on the step-size parameter to gain insight about the effect of step-size on the performance of your agent.</p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/fastblog/2022/03/14/_Lunar_Landing_Expected_SARSA.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/fastblog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/fastblog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/fastblog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My blog about code and ideas.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/dnlam" target="_blank" title="dnlam"><svg class="svg-icon grey"><use xlink:href="/fastblog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/lam-dinh-34a66a160" target="_blank" title="lam-dinh-34a66a160"><svg class="svg-icon grey"><use xlink:href="/fastblog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
